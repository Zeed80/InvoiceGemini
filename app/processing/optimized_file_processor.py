#!/usr/bin/env python3
"""
Optimized File Processing System for InvoiceGemini
–í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
"""

import os
import time
import asyncio
import threading
from typing import Dict, List, Optional, Any, Union, Callable
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from dataclasses import dataclass
from datetime import datetime
import multiprocessing as mp
import logging

from PyQt6.QtCore import QObject, pyqtSignal, QThread, QMutex, QTimer
import torch

from ..settings_manager import settings_manager
from ..core.cache_manager import get_cache_manager
from .. import utils


@dataclass
class ProcessingTask:
    """–ó–∞–¥–∞—á–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
    file_path: str
    task_id: str
    priority: int = 1
    model_type: str = "gemini"
    custom_prompt: str = ""
    ocr_lang: str = "rus+eng"
    
    def __post_init__(self):
        self.created_at = datetime.now()
        self.status = "pending"
        self.result = None
        self.error = None
        self.processing_time = 0.0


@dataclass
class ProcessingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
    task_id: str
    file_path: str
    success: bool
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    processing_time: float = 0.0
    model_type: str = "unknown"
    from_cache: bool = False


class GPUResourceManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä GPU —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —É—Ç–∏–ª–∏–∑–∞—Ü–∏–∏"""
    
    def __init__(self):
        self.gpu_available = torch.cuda.is_available()
        self.device_count = torch.cuda.device_count() if self.gpu_available else 0
        self.memory_usage = {}
        self.device_locks = [threading.Lock() for _ in range(self.device_count)]
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏
        if self.gpu_available:
            self._monitor_gpu_memory()
    
    def _monitor_gpu_memory(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏"""
        for device_id in range(self.device_count):
            try:
                memory_info = torch.cuda.mem_get_info(device_id)
                free_memory = memory_info[0] / 1024**3  # GB
                total_memory = memory_info[1] / 1024**3  # GB
                used_memory = total_memory - free_memory
                
                self.memory_usage[device_id] = {
                    'free': free_memory,
                    'used': used_memory,
                    'total': total_memory,
                    'utilization': used_memory / total_memory
                }
            except Exception as e:
                logging.warning(f"Failed to get GPU {device_id} memory info: {e}")
    
    def get_optimal_device(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if not self.gpu_available:
            return -1  # CPU
            
        self._monitor_gpu_memory()
        
        # –í—ã–±–∏—Ä–∞–µ–º GPU —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
        best_device = 0
        min_utilization = float('inf')
        
        for device_id, memory_info in self.memory_usage.items():
            if memory_info['utilization'] < min_utilization:
                min_utilization = memory_info['utilization']
                best_device = device_id
        
        return best_device
    
    def acquire_device(self, device_id: int) -> bool:
        """–ó–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if device_id == -1 or device_id >= len(self.device_locks):
            return True  # CPU –∏–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π ID
            
        return self.device_locks[device_id].acquire(blocking=False)
    
    def release_device(self, device_id: int):
        """–û—Å–≤–æ–±–æ–¥–∏—Ç—å GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ"""
        if device_id == -1 or device_id >= len(self.device_locks):
            return
            
        try:
            self.device_locks[device_id].release()
        except (IndexError, RuntimeError, AttributeError, Exception) as e:
            # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —É–∂–µ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ
            pass


class SmartCache:
    """–£–º–Ω—ã–π –∫—ç—à —Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π"""
    
    def __init__(self):
        self.cache_manager = get_cache_manager() if hasattr(get_cache_manager, '__call__') else None
        self.prediction_cache = {}
        self.access_patterns = {}
        
    def get_cached_result(self, file_path: str, model_type: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞"""
        if not self.cache_manager:
            return None
            
        try:
            file_hash = self.cache_manager.calculate_file_hash(file_path)
            result = self.cache_manager.get_cached_result(file_hash, model_type)
            
            if result:
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ—Å—Ç—É–ø–∞
                self._update_access_pattern(file_path, model_type)
                
            return result
        except Exception as e:
            logging.error(f"Cache get error: {e}")
            return None
    
    def cache_result(self, file_path: str, model_type: str, result: Dict):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
        if not self.cache_manager or not result:
            return
            
        try:
            file_hash = self.cache_manager.calculate_file_hash(file_path)
            self.cache_manager.cache_result(file_hash, model_type, result, file_path)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ—Å—Ç—É–ø–∞
            self._update_access_pattern(file_path, model_type)
            
        except Exception as e:
            logging.error(f"Cache save error: {e}")
    
    def _update_access_pattern(self, file_path: str, model_type: str):
        """–û–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ—Å—Ç—É–ø–∞ –¥–ª—è –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        key = f"{model_type}:{Path(file_path).suffix.lower()}"
        if key not in self.access_patterns:
            self.access_patterns[key] = 0
        self.access_patterns[key] += 1
    
    def predict_next_files(self, current_file: str, file_list: List[str]) -> List[str]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏"""
        current_path = Path(current_file)
        current_dir = current_path.parent
        current_suffix = current_path.suffix.lower()
        
        # –§–∞–π–ª—ã –∏–∑ —Ç–æ–π –∂–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ç–µ–º –∂–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º
        candidates = []
        for file_path in file_list:
            file_path_obj = Path(file_path)
            if (file_path_obj.parent == current_dir and 
                file_path_obj.suffix.lower() == current_suffix):
                candidates.append(file_path)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏
        candidates.sort()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ 3 —Ñ–∞–π–ª–∞
        try:
            current_index = candidates.index(current_file)
            return candidates[current_index + 1:current_index + 4]
        except ValueError:
            return candidates[:3]


class ParallelFileProcessor(QObject):
    """
    –í—ã—Å–æ–∫–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ñ–∞–π–ª–æ–≤ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    
    –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏:
    - –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ CPU –∏ GPU
    - –£–º–Ω—ã–π –∫—ç—à —Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π
    - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞–º–∏
    - –ü—Ä–∏–æ—Ä–∏—Ç–µ–∑–∞—Ü–∏—è –∑–∞–¥–∞—á
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    # –°–∏–≥–Ω–∞–ª—ã –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    task_started = pyqtSignal(str)  # task_id
    task_completed = pyqtSignal(str, dict)  # task_id, result
    task_failed = pyqtSignal(str, str)  # task_id, error
    progress_updated = pyqtSignal(int, int)  # current, total
    batch_completed = pyqtSignal(list)  # all results
    
    def __init__(self, parent=None):
        super().__init__(parent)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.max_workers = self._calculate_optimal_workers()
        self.max_gpu_tasks = min(torch.cuda.device_count(), 2) if torch.cuda.is_available() else 0
        self.batch_size = 4  # –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        # –ú–µ–Ω–µ–¥–∂–µ—Ä—ã —Ä–µ—Å—É—Ä—Å–æ–≤
        self.gpu_manager = GPUResourceManager()
        self.smart_cache = SmartCache()
        
        # –û—á–µ—Ä–µ–¥–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.task_queue: List[ProcessingTask] = []
        self.processing_tasks: Dict[str, ProcessingTask] = {}
        self.results: Dict[str, ProcessingResult] = {}
        
        # Thread pools
        self.cpu_executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.gpu_executor = ThreadPoolExecutor(max_workers=self.max_gpu_tasks) if self.max_gpu_tasks > 0 else None
        
        # –°—Ç–∞—Ç—É—Å
        self.is_processing = False
        self.stop_requested = False
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'cache_hits': 0,
            'processing_time': 0.0,
            'throughput': 0.0  # —Ñ–∞–π–ª–æ–≤/—Å–µ–∫
        }
        
        logging.info(f"üöÄ ParallelFileProcessor initialized:")
        logging.info(f"   CPU workers: {self.max_workers}")
        logging.info(f"   GPU workers: {self.max_gpu_tasks}")
        logging.info(f"   Batch size: {self.batch_size}")
    
    def _calculate_optimal_workers(self) -> int:
        """–í—ã—á–∏—Å–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö –ø–æ—Ç–æ–∫–æ–≤"""
        cpu_count = mp.cpu_count()
        
        # –î–ª—è I/O-bound –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤
        # –î–ª—è CPU-bound –∑–∞–¥–∞—á –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        if torch.cuda.is_available():
            # –ï—Å–ª–∏ –µ—Å—Ç—å GPU, CPU –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ–Ω—å—à–µ
            return min(cpu_count, 4)
        else:
            # –¢–æ–ª—å–∫–æ CPU - –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤ –Ω–æ –Ω–µ –≤—Å–µ
            return min(cpu_count * 2, 8)
    
    def process_files(self, file_paths: List[str], model_type: str = "gemini", 
                     custom_prompt: str = "", ocr_lang: str = "rus+eng",
                     priority_callback: Callable[[str], int] = None) -> bool:
        """
        –ù–∞—á–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            custom_prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            ocr_lang: –Ø–∑—ã–∫ OCR
            priority_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ñ–∞–π–ª–∞
        """
        if self.is_processing:
            logging.warning("Processing already in progress")
            return False
        
        try:
            # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏
            self.task_queue.clear()
            self.processing_tasks.clear()
            self.results.clear()
            
            for i, file_path in enumerate(file_paths):
                priority = priority_callback(file_path) if priority_callback else 1
                
                task = ProcessingTask(
                    file_path=file_path,
                    task_id=f"task_{i}_{int(time.time())}",
                    priority=priority,
                    model_type=model_type,
                    custom_prompt=custom_prompt,
                    ocr_lang=ocr_lang
                )
                self.task_queue.append(task)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
            self.task_queue.sort(key=lambda t: t.priority, reverse=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats['total_tasks'] = len(self.task_queue)
            self.stats['completed_tasks'] = 0
            self.stats['failed_tasks'] = 0
            self.stats['cache_hits'] = 0
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
            self.is_processing = True
            self.stop_requested = False
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            processing_thread = threading.Thread(target=self._process_queue)
            processing_thread.daemon = True
            processing_thread.start()
            
            return True
            
        except Exception as e:
            logging.error(f"Failed to start file processing: {e}")
            return False
    
    def _process_queue(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—á–µ—Ä–µ–¥–∏"""
        start_time = time.time()
        
        try:
            while self.task_queue and not self.stop_requested:
                # –ü–æ–ª—É—á–∞–µ–º –ø–∞–∫–µ—Ç –∑–∞–¥–∞—á
                batch = self._get_next_batch()
                if not batch:
                    break
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–∫–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
                self._process_batch(batch)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                completed = self.stats['completed_tasks'] + self.stats['failed_tasks']
                self.progress_updated.emit(completed, self.stats['total_tasks'])
            
            # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            self.is_processing = False
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_time = time.time() - start_time
            self.stats['processing_time'] = total_time
            self.stats['throughput'] = self.stats['completed_tasks'] / total_time if total_time > 0 else 0
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_results = list(self.results.values())
            self.batch_completed.emit(all_results)
            
            logging.info(f"‚úÖ Batch processing completed:")
            logging.info(f"   Total: {self.stats['total_tasks']}")
            logging.info(f"   Completed: {self.stats['completed_tasks']}")
            logging.info(f"   Failed: {self.stats['failed_tasks']}")
            logging.info(f"   Cache hits: {self.stats['cache_hits']}")
            logging.info(f"   Throughput: {self.stats['throughput']:.2f} files/sec")
            
        except Exception as e:
            logging.error(f"Error in processing queue: {e}")
            self.is_processing = False
    
    def _get_next_batch(self) -> List[ProcessingTask]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –ø–∞–∫–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        batch = []
        
        while len(batch) < self.batch_size and self.task_queue:
            task = self.task_queue.pop(0)
            batch.append(task)
            self.processing_tasks[task.task_id] = task
        
        return batch
    
    def _process_batch(self, batch: List[ProcessingTask]):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞–∫–µ—Ç –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        # –†–∞–∑–¥–µ–ª—è–µ–º –∑–∞–¥–∞—á–∏ –Ω–∞ CPU –∏ GPU
        cpu_tasks = []
        gpu_tasks = []
        
        for task in batch:
            if (self.gpu_executor and 
                self.max_gpu_tasks > 0 and 
                self._should_use_gpu(task)):
                gpu_tasks.append(task)
            else:
                cpu_tasks.append(task)
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        futures = []
        
        # CPU –∑–∞–¥–∞—á–∏
        for task in cpu_tasks:
            future = self.cpu_executor.submit(self._process_single_task, task, use_gpu=False)
            futures.append(future)
        
        # GPU –∑–∞–¥–∞—á–∏
        if self.gpu_executor:
            for task in gpu_tasks:
                future = self.gpu_executor.submit(self._process_single_task, task, use_gpu=True)
                futures.append(future)
        
        # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á –≤ –ø–∞–∫–µ—Ç–µ
        for future in as_completed(futures):
            try:
                result = future.result()
                self._handle_task_result(result)
            except Exception as e:
                logging.error(f"Task execution error: {e}")
    
    def _should_use_gpu(self, task: ProcessingTask) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å, —Å—Ç–æ–∏—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU –¥–ª—è –∑–∞–¥–∞—á–∏"""
        # GPU —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª–µ–π
        gpu_models = ['layoutlm', 'donut']
        return task.model_type.lower() in gpu_models
    
    def _process_single_task(self, task: ProcessingTask, use_gpu: bool = False) -> ProcessingResult:
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–Ω—É –∑–∞–¥–∞—á—É"""
        start_time = time.time()
        device_id = -1
        
        try:
            self.task_started.emit(task.task_id)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached_result = self.smart_cache.get_cached_result(task.file_path, task.model_type)
            if cached_result:
                self.stats['cache_hits'] += 1
                return ProcessingResult(
                    task_id=task.task_id,
                    file_path=task.file_path,
                    success=True,
                    result=cached_result,
                    processing_time=time.time() - start_time,
                    model_type=task.model_type,
                    from_cache=True
                )
            
            # –ü–æ–ª—É—á–∞–µ–º GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if use_gpu:
                device_id = self.gpu_manager.get_optimal_device()
                if not self.gpu_manager.acquire_device(device_id):
                    # –ï—Å–ª–∏ GPU –∑–∞–Ω—è—Ç–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
                    use_gpu = False
                    device_id = -1
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –º–æ–¥–µ–ª–∏
            processor = self._get_model_processor(task.model_type, device_id)
            if not processor:
                raise RuntimeError(f"Model processor not available: {task.model_type}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
            result = processor.process_image(
                task.file_path, 
                task.ocr_lang, 
                custom_prompt=task.custom_prompt
            )
            
            if result:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                self.smart_cache.cache_result(task.file_path, task.model_type, result)
                
                return ProcessingResult(
                    task_id=task.task_id,
                    file_path=task.file_path,
                    success=True,
                    result=result,
                    processing_time=time.time() - start_time,
                    model_type=task.model_type,
                    from_cache=False
                )
            else:
                raise RuntimeError("Processing returned empty result")
                
        except Exception as e:
            return ProcessingResult(
                task_id=task.task_id,
                file_path=task.file_path,
                success=False,
                error=str(e),
                processing_time=time.time() - start_time,
                model_type=task.model_type
            )
        finally:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU
            if device_id != -1:
                self.gpu_manager.release_device(device_id)
    
    def _get_model_processor(self, model_type: str, device_id: int = -1):
        """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –º–æ–¥–µ–ª–∏"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ModelManager
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        return None
    
    def _handle_task_result(self, result: ProcessingResult):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–¥–∞—á–∏"""
        self.results[result.task_id] = result
        
        if result.success:
            self.stats['completed_tasks'] += 1
            self.task_completed.emit(result.task_id, result.result or {})
        else:
            self.stats['failed_tasks'] += 1
            self.task_failed.emit(result.task_id, result.error or "Unknown error")
        
        # –£–¥–∞–ª—è–µ–º –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
        if result.task_id in self.processing_tasks:
            del self.processing_tasks[result.task_id]
    
    def stop_processing(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"""
        self.stop_requested = True
        
        # –û—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å
        self.task_queue.clear()
        
        logging.info("‚èπÔ∏è Processing stop requested")
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        return self.stats.copy()
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.stop_processing()
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º thread pools
        if self.cpu_executor:
            self.cpu_executor.shutdown(wait=True)
        if self.gpu_executor:
            self.gpu_executor.shutdown(wait=True)
        
        logging.info("üßπ ParallelFileProcessor cleaned up")


class OptimizedProcessingThread(QThread):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å UI
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ParallelFileProcessor –¥–ª—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    
    # –°–∏–≥–Ω–∞–ª—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
    progress_signal = pyqtSignal(int)
    finished_signal = pyqtSignal(object)
    error_signal = pyqtSignal(str)
    partial_result_signal = pyqtSignal(dict)
    
    def __init__(self, file_paths: Union[str, List[str]], model_type: str, 
                 ocr_lang: str = "rus+eng", is_folder: bool = False, 
                 model_manager=None, parent=None):
        super().__init__(parent)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if isinstance(file_paths, str):
            if is_folder:
                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏
                self.file_paths = self._get_supported_files(file_paths)
            else:
                self.file_paths = [file_paths]
        else:
            self.file_paths = file_paths
            
        self.model_type = model_type
        self.ocr_lang = ocr_lang
        self.is_folder = is_folder
        self.model_manager = model_manager
        
        # –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä
        self.processor = ParallelFileProcessor()
        
        # –ü–æ–¥–∫–ª—é—á–∞–µ–º —Å–∏–≥–Ω–∞–ª—ã
        self.processor.task_completed.connect(self._on_task_completed)
        self.processor.task_failed.connect(self._on_task_failed)
        self.processor.progress_updated.connect(self._on_progress_updated)
        self.processor.batch_completed.connect(self._on_batch_completed)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.results = []
        self.errors = []
    
    def _get_supported_files(self, folder_path: str) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏"""
        supported_files = []
        
        try:
            for filename in os.listdir(folder_path):
                file_path = os.path.join(folder_path, filename)
                if os.path.isfile(file_path) and utils.is_supported_format(file_path):
                    supported_files.append(file_path)
        except OSError as e:
            logging.error(f"Error reading directory {folder_path}: {e}")
        
        return sorted(supported_files)  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç–∏
    
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"""
        try:
            if not self.file_paths:
                self.finished_signal.emit(None)
                return
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            custom_prompt = settings_manager.get_string(
                'Prompts', 
                f'{self.model_type}_prompt', 
                ''
            )
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            success = self.processor.process_files(
                file_paths=self.file_paths,
                model_type=self.model_type,
                custom_prompt=custom_prompt,
                ocr_lang=self.ocr_lang
            )
            
            if not success:
                self.error_signal.emit("Failed to start parallel processing")
                return
            
            # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (processor —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–∞—Ö)
            while self.processor.is_processing and not self.isInterruptionRequested():
                self.msleep(100)
            
        except Exception as e:
            logging.error(f"Error in OptimizedProcessingThread: {e}")
            self.error_signal.emit(str(e))
    
    def _on_task_completed(self, task_id: str, result: dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        self.results.append(result)
        
        if self.is_folder:
            # –î–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            self.partial_result_signal.emit(result)
        
    def _on_task_failed(self, task_id: str, error: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –∑–∞–¥–∞—á–∏"""
        self.errors.append(error)
        logging.error(f"Task {task_id} failed: {error}")
    
    def _on_progress_updated(self, current: int, total: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        if total > 0:
            progress = int((current / total) * 100)
            self.progress_signal.emit(progress)
    
    def _on_batch_completed(self, all_results: List):
        """–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        if self.is_folder:
            # –î–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã)
            self.finished_signal.emit(None)
        else:
            # –î–ª—è –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if all_results and all_results[0].success:
                self.finished_signal.emit(all_results[0].result)
            else:
                self.finished_signal.emit(None)
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É"""
        self.requestInterruption()
        self.processor.stop_processing()
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.processor.cleanup()


# –ü—Å–µ–≤–¥–æ–Ω–∏–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
ProcessingThread = OptimizedProcessingThread 