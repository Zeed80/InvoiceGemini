"""
–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""
import asyncio
import logging
import os
import time
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
from pathlib import Path
import torch
from PyQt6.QtCore import QObject, pyqtSignal, QTimer, QThread

logger = logging.getLogger(__name__)


@dataclass
class ModelUsageStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    model_id: str
    usage_count: int
    last_used: float
    avg_load_time: float
    memory_usage_mb: int
    success_rate: float
    user_preference_score: float = 0.0


@dataclass
class PredictiveLoadTask:
    """–ó–∞–¥–∞—á–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
    model_id: str
    priority: int
    file_types: Set[str]
    estimated_load_time: float
    confidence: float


class SmartModelLoader(QObject):
    """
    –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π —Å:
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    - –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é
    - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π
    """
    
    # –°–∏–≥–Ω–∞–ª—ã
    model_loading_started = pyqtSignal(str)  # model_id
    model_loading_progress = pyqtSignal(str, int)  # model_id, progress
    model_loaded = pyqtSignal(str, float)  # model_id, load_time
    model_unloaded = pyqtSignal(str)  # model_id
    memory_warning = pyqtSignal(int)  # free_memory_mb
    
    def __init__(self, parent=None):
        super().__init__(parent)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.max_concurrent_loads = 2
        self.memory_threshold_mb = 1024  # 1GB —Ä–µ–∑–µ—Ä–≤
        self.prediction_window_files = 3  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö 3 —Ñ–∞–π–ª–æ–≤
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
        self.usage_stats: Dict[str, ModelUsageStats] = {}
        self.load_history: List[Tuple[str, float, bool]] = []  # model_id, load_time, success
        self.current_file_queue: List[str] = []
        self.last_model_sequence: List[str] = []
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.loaded_models: Dict[str, object] = {}
        self.loading_tasks: Dict[str, QThread] = {}
        self.load_executor = ThreadPoolExecutor(max_workers=self.max_concurrent_loads)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫—ç—à
        self.prediction_cache: List[PredictiveLoadTask] = []
        self.prediction_timer = QTimer()
        self.prediction_timer.timeout.connect(self._update_predictions)
        self.prediction_timer.start(5000)  # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤
        self.memory_monitor_timer = QTimer()
        self.memory_monitor_timer.timeout.connect(self._monitor_memory)
        self.memory_monitor_timer.start(10000)  # –ö–∞–∂–¥—ã–µ 10 —Å–µ–∫
        
        logger.info("ü§ñ SmartModelLoader –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def analyze_file_queue(self, file_paths: List[str]) -> None:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—á–µ—Ä–µ–¥—å —Ñ–∞–π–ª–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏"""
        self.current_file_queue = file_paths[:self.prediction_window_files]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
        file_types = set()
        for file_path in self.current_file_queue:
            ext = Path(file_path).suffix.lower()
            file_types.add(ext)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω—É–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏
        predicted_models = self._predict_needed_models(file_types)
        
        # –ü–ª–∞–Ω–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        self._schedule_predictive_loading(predicted_models)
    
    def _predict_needed_models(self, file_types: Set[str]) -> List[PredictiveLoadTask]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–æ–≤ —Ñ–∞–π–ª–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        predictions = []
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        model_preferences = {
            '.pdf': ['gemini', 'layoutlm', 'donut'],
            '.png': ['gemini', 'donut', 'layoutlm'],
            '.jpg': ['gemini', 'donut', 'layoutlm'],
            '.jpeg': ['gemini', 'donut', 'layoutlm']
        }
        
        model_scores = {}
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        for file_type in file_types:
            if file_type in model_preferences:
                for i, model_id in enumerate(model_preferences[file_type]):
                    base_score = 100 - (i * 20)  # –£–±—ã–≤–∞—é—â–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    model_scores[model_id] = model_scores.get(model_id, 0) + base_score
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        for model_id, score in model_scores.items():
            if model_id in self.usage_stats:
                stats = self.usage_stats[model_id]
                # –ë–æ–Ω—É—Å—ã –∑–∞ —á–∞—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏ —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
                usage_bonus = min(stats.usage_count * 5, 50)
                success_bonus = stats.success_rate * 30
                preference_bonus = stats.user_preference_score * 20
                
                final_score = score + usage_bonus + success_bonus + preference_bonus
                
                predictions.append(PredictiveLoadTask(
                    model_id=model_id,
                    priority=int(final_score),
                    file_types=file_types,
                    estimated_load_time=stats.avg_load_time,
                    confidence=min(stats.usage_count / 10, 1.0)
                ))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        predictions.sort(key=lambda x: x.priority, reverse=True)
        return predictions[:3]  # –¢–æ–ø-3 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    
    def _schedule_predictive_loading(self, predictions: List[PredictiveLoadTask]):
        """–ü–ª–∞–Ω–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π"""
        self.prediction_cache = predictions
        
        for task in predictions:
            if task.model_id not in self.loaded_models and task.model_id not in self.loading_tasks:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å
                if self._has_sufficient_memory(task.model_id):
                    logger.info(f"üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞: {task.model_id} "
                              f"(–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {task.priority}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {task.confidence:.1f})")
                    self._start_background_loading(task.model_id)
    
    def _start_background_loading(self, model_id: str):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏"""
        if model_id in self.loading_tasks:
            return
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Ç–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏
        loading_thread = ModelLoadingThread(model_id, self)
        loading_thread.loading_progress.connect(
            lambda progress: self.model_loading_progress.emit(model_id, progress)
        )
        loading_thread.loading_completed.connect(self._on_model_loaded)
        loading_thread.loading_failed.connect(self._on_model_load_failed)
        
        self.loading_tasks[model_id] = loading_thread
        self.model_loading_started.emit(model_id)
        
        loading_thread.start()
    
    def _has_sufficient_memory(self, model_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        from .memory_manager import get_memory_manager
        
        memory_manager = get_memory_manager()
        memory_info = memory_manager.get_memory_info()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–∂–∏–¥–∞–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏
        expected_size = memory_manager.get_model_size(model_id, 'base')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º RAM
        required_memory = expected_size * 1.5 + self.memory_threshold_mb  # –ó–∞–ø–∞—Å
        has_ram = memory_info.available_mb >= required_memory
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU –ø–∞–º—è—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        has_gpu = True
        if model_id in ['layoutlm', 'donut'] and torch.cuda.is_available():
            gpu_required = expected_size * 1.2  # GPU —Ç—Ä–µ–±—É–µ—Ç –º–µ–Ω—å—à–µ –∑–∞–ø–∞—Å–∞
            has_gpu = (memory_info.gpu_available_mb or 0) >= gpu_required
        
        return has_ram and has_gpu
    
    def _on_model_loaded(self, model_id: str, model_instance: object, load_time: float):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.loaded_models[model_id] = model_instance
        
        # –£–±–∏—Ä–∞–µ–º –∏–∑ –∑–∞–≥—Ä—É–∂–∞—é—â–∏—Ö—Å—è
        if model_id in self.loading_tasks:
            self.loading_tasks[model_id].deleteLater()
            del self.loading_tasks[model_id]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._update_usage_stats(model_id, load_time, True)
        
        # –≠–º–∏—Ç–∏—Ä—É–µ–º —Å–∏–≥–Ω–∞–ª
        self.model_loaded.emit(model_id, load_time)
        
        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_id} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.1f}—Å")
    
    def _on_model_load_failed(self, model_id: str, error: str):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ—É–¥–∞—á–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        # –£–±–∏—Ä–∞–µ–º –∏–∑ –∑–∞–≥—Ä—É–∂–∞—é—â–∏—Ö—Å—è
        if model_id in self.loading_tasks:
            self.loading_tasks[model_id].deleteLater()
            del self.loading_tasks[model_id]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._update_usage_stats(model_id, 0, False)
        
        logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_id}: {error}")
    
    def _update_usage_stats(self, model_id: str, load_time: float, success: bool):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        current_time = time.time()
        
        if model_id not in self.usage_stats:
            self.usage_stats[model_id] = ModelUsageStats(
                model_id=model_id,
                usage_count=0,
                last_used=current_time,
                avg_load_time=0.0,
                memory_usage_mb=0,
                success_rate=0.0
            )
        
        stats = self.usage_stats[model_id]
        stats.usage_count += 1
        stats.last_used = current_time
        
        if success and load_time > 0:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏
            if stats.avg_load_time == 0:
                stats.avg_load_time = load_time
            else:
                stats.avg_load_time = (stats.avg_load_time + load_time) / 2
        
        # –û–±–Ω–æ–≤–ª—è–µ–º rate —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        total_attempts = len([h for h in self.load_history if h[0] == model_id])
        successful_attempts = len([h for h in self.load_history if h[0] == model_id and h[2]])
        if total_attempts > 0:
            stats.success_rate = successful_attempts / total_attempts
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.load_history.append((model_id, load_time, success))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.load_history) > 1000:
            self.load_history = self.load_history[-500:]
    
    def _update_predictions(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        if self.current_file_queue:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é –æ—á–µ—Ä–µ–¥—å —Ñ–∞–π–ª–æ–≤
            self.analyze_file_queue(self.current_file_queue)
    
    def _monitor_memory(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –∏ –≤—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        from .memory_manager import get_memory_manager
        
        memory_manager = get_memory_manager()
        memory_info = memory_manager.get_memory_info()
        
        if memory_info.available_mb < self.memory_threshold_mb:
            # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –ø–∞–º—è—Ç–∏ - –≤—ã–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
            self._emergency_unload_models()
            self.memory_warning.emit(int(memory_info.available_mb))
    
    def _emergency_unload_models(self):
        """–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        models_by_usage = []
        for model_id in self.loaded_models:
            if model_id in self.usage_stats:
                last_used = self.usage_stats[model_id].last_used
                models_by_usage.append((model_id, last_used))
        
        models_by_usage.sort(key=lambda x: x[1])  # –°–Ω–∞—á–∞–ª–∞ —Å—Ç–∞—Ä—ã–µ
        
        # –í—ã–≥—Ä—É–∂–∞–µ–º –¥–æ 50% –º–æ–¥–µ–ª–µ–π
        models_to_unload = models_by_usage[:len(models_by_usage)//2 + 1]
        
        for model_id, _ in models_to_unload:
            self.unload_model(model_id)
            logger.info(f"üßπ –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_id}")
    
    def get_model(self, model_id: str, blocking: bool = True) -> Optional[object]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å (—Å –∑–∞–≥—Ä—É–∑–∫–æ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        
        Args:
            model_id: ID –º–æ–¥–µ–ª–∏
            blocking: –ñ–¥–∞—Ç—å –ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
            
        Returns:
            –≠–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ –∏–ª–∏ None
        """
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
        if model_id in self.loaded_models:
            return self.loaded_models[model_id]
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
        if model_id in self.loading_tasks:
            if blocking:
                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏
                thread = self.loading_tasks[model_id]
                thread.wait()
                return self.loaded_models.get(model_id)
            else:
                return None
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
        if blocking:
            self._start_background_loading(model_id)
            if model_id in self.loading_tasks:
                thread = self.loading_tasks[model_id]
                thread.wait()
                return self.loaded_models.get(model_id)
        else:
            self._start_background_loading(model_id)
            return None
    
    def unload_model(self, model_id: str):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏"""
        if model_id in self.loaded_models:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã –º–æ–¥–µ–ª–∏
            model = self.loaded_models[model_id]
            if hasattr(model, 'cleanup'):
                model.cleanup()
            
            del self.loaded_models[model_id]
            self.model_unloaded.emit(model_id)
            
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            import gc
            gc.collect()
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
    
    def get_statistics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã"""
        return {
            'loaded_models': list(self.loaded_models.keys()),
            'loading_models': list(self.loading_tasks.keys()),
            'usage_stats': {k: {
                'usage_count': v.usage_count,
                'avg_load_time': v.avg_load_time,
                'success_rate': v.success_rate,
                'last_used': v.last_used
            } for k, v in self.usage_stats.items()},
            'prediction_cache': [
                {
                    'model_id': task.model_id,
                    'priority': task.priority,
                    'confidence': task.confidence
                } for task in self.prediction_cache
            ],
            'total_loads': len(self.load_history),
            'memory_warnings': 0  # TODO: –¥–æ–±–∞–≤–∏—Ç—å —Å—á–µ—Ç—á–∏–∫
        }
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä—ã
        self.prediction_timer.stop()
        self.memory_monitor_timer.stop()
        
        # –í—ã–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        for model_id in list(self.loaded_models.keys()):
            self.unload_model(model_id)
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫–∏
        for thread in self.loading_tasks.values():
            thread.quit()
            thread.wait(5000)
        
        self.load_executor.shutdown(wait=True)
        
        logger.info("üßπ SmartModelLoader –æ—á–∏—â–µ–Ω")


class ModelLoadingThread(QThread):
    """–ü–æ—Ç–æ–∫ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ–Ω–µ"""
    
    loading_progress = pyqtSignal(int)  # progress 0-100
    loading_completed = pyqtSignal(str, object, float)  # model_id, model, load_time
    loading_failed = pyqtSignal(str, str)  # model_id, error
    
    def __init__(self, model_id: str, loader: SmartModelLoader):
        super().__init__()
        self.model_id = model_id
        self.loader = loader
        self._should_stop = False
    
    def run(self):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏"""
        start_time = time.time()
        
        try:
            self.loading_progress.emit(10)
            
            if self._should_stop:
                return
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–∞–ø—Ä—è–º—É—é, –∏–∑–±–µ–≥–∞—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            model = self._create_model_processor(self.model_id)
            
            if self._should_stop:
                return
            
            self.loading_progress.emit(80)
            
            if model:
                load_time = time.time() - start_time
                self.loading_progress.emit(100)
                self.loading_completed.emit(self.model_id, model, load_time)
            else:
                self.loading_failed.emit(self.model_id, "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å")
                
        except Exception as e:
            self.loading_failed.emit(self.model_id, str(e))
    
    def _create_model_processor(self, model_id: str):
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞–ø—Ä—è–º—É—é"""
        from ..settings_manager import settings_manager
        from .. import config as app_config
        
        self.loading_progress.emit(30)
        
        if model_id == 'layoutlm':
            from ..processing_engine import LayoutLMProcessor, OCRProcessor
            ocr_processor = OCRProcessor()
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ LayoutLM
            active_type = settings_manager.get_active_layoutlm_model_type()
            if active_type == 'custom':
                custom_model_name = settings_manager.get_string('Models', 'custom_layoutlm_model_name', app_config.DEFAULT_CUSTOM_LAYOUTLM_MODEL_NAME)
                model_path = os.path.join(app_config.TRAINED_MODELS_PATH, custom_model_name)
                processor = LayoutLMProcessor(ocr_processor, model_path, True)
            else:
                model_id_hf = settings_manager.get_string('Models', 'layoutlm_id', app_config.LAYOUTLM_MODEL_ID)
                processor = LayoutLMProcessor(ocr_processor, model_id_hf, False)
            
            self.loading_progress.emit(60)
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            if processor.load_model():
                return processor
            else:
                return None
                
        elif model_id == 'donut':
            from ..processing_engine import DonutProcessorImpl
            donut_model_id = settings_manager.get_string('Models', 'donut_id', app_config.DONUT_MODEL_ID)
            processor = DonutProcessorImpl(donut_model_id)
            
            self.loading_progress.emit(60)
            if processor.load_model():
                return processor
            else:
                return None
                
        elif model_id == 'gemini':
            from ..gemini_processor import GeminiProcessor
            self.loading_progress.emit(60)
            processor = GeminiProcessor()
            return processor  # GeminiProcessor –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            
        elif model_id == 'trocr':
            from ..trocr_processor import TrOCRProcessor
            model_identifier = settings_manager.get_string('Models', 'trocr_model_id', 'microsoft/trocr-base-printed')
            self.loading_progress.emit(60)
            processor = TrOCRProcessor(model_name=model_identifier)
            return processor
            
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_id}")
            
        return None
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É"""
        self._should_stop = True


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_smart_loader: Optional[SmartModelLoader] = None


def get_smart_model_loader() -> SmartModelLoader:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É–º–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    global _smart_loader
    if _smart_loader is None:
        _smart_loader = SmartModelLoader()
    return _smart_loader 