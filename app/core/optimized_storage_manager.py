"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å –ø—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
"""

import sqlite3
import json
import asyncio
import logging
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
from threading import Lock, RLock
from concurrent.futures import ThreadPoolExecutor
import time
from queue import Queue, Empty
import threading

from PyQt6.QtCore import QObject, pyqtSignal, QTimer

logger = logging.getLogger(__name__)


@dataclass
class QueryStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    query_type: str
    execution_time: float
    rows_affected: int
    timestamp: float
    query_hash: str


class ConnectionPool:
    """–ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å SQLite"""
    
    def __init__(self, db_path: str, pool_size: int = 5):
        self.db_path = Path(db_path)
        self.pool_size = pool_size
        self.connections: Queue = Queue(maxsize=pool_size)
        self.lock = Lock()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ SQLite –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self._sqlite_optimizations = {
            'PRAGMA journal_mode': 'WAL',  # Write-Ahead Logging
            'PRAGMA synchronous': 'NORMAL',  # –ë–∞–ª–∞–Ω—Å–∏—Ä—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å/—Å–∫–æ—Ä–æ—Å—Ç—å
            'PRAGMA cache_size': '10000',  # 10MB –∫—ç—à
            'PRAGMA temp_store': 'memory',  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–º—è—Ç–∏
            'PRAGMA mmap_size': '268435456',  # 256MB memory mapping
            'PRAGMA optimize': None  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        }
        
        # –°–æ–∑–¥–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        self._create_connections()
    
    def _create_connections(self):
        """–°–æ–∑–¥–∞–µ—Ç –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        for _ in range(self.pool_size):
            conn = sqlite3.connect(
                str(self.db_path), 
                check_same_thread=False,
                timeout=30.0
            )
            conn.row_factory = sqlite3.Row  # –î–æ—Å—Ç—É–ø –ø–æ –∏–º–µ–Ω–∏ –∫–æ–ª–æ–Ω–∫–∏
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            for pragma, value in self._sqlite_optimizations.items():
                if value is not None:
                    conn.execute(f"{pragma} = {value}")
                else:
                    conn.execute(pragma)
            
            conn.commit()
            self.connections.put(conn)
    
    def get_connection(self, timeout: float = 5.0) -> sqlite3.Connection:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑ –ø—É–ª–∞"""
        try:
            return self.connections.get(timeout=timeout)
        except Empty:
            raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑ –ø—É–ª–∞")
    
    def return_connection(self, conn: sqlite3.Connection):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –≤ –ø—É–ª"""
        try:
            self.connections.put_nowait(conn)
        except:
            # –ü—É–ª –ø–æ–ª–Ω—ã–π, –∑–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            conn.close()
    
    def close_all(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        while not self.connections.empty():
            try:
                conn = self.connections.get_nowait()
                conn.close()
            except Empty:
                break


class QueryCache:
    """–ö—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤ —Å TTL"""
    
    def __init__(self, max_size: int = 1000, default_ttl: float = 300):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.cache: Dict[str, Tuple[Any, float]] = {}  # query_hash -> (result, expire_time)
        self.access_times: Dict[str, float] = {}
        self.lock = RLock()
    
    def get(self, query_hash: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞"""
        with self.lock:
            if query_hash in self.cache:
                result, expire_time = self.cache[query_hash]
                
                if time.time() < expire_time:
                    self.access_times[query_hash] = time.time()
                    return result
                else:
                    # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∑–∞–ø–∏—Å—å
                    del self.cache[query_hash]
                    if query_hash in self.access_times:
                        del self.access_times[query_hash]
            
            return None
    
    def put(self, query_hash: str, result: Any, ttl: Optional[float] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
        if ttl is None:
            ttl = self.default_ttl
            
        expire_time = time.time() + ttl
        
        with self.lock:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
            if len(self.cache) >= self.max_size:
                self._evict_lru()
            
            self.cache[query_hash] = (result, expire_time)
            self.access_times[query_hash] = time.time()
    
    def _evict_lru(self):
        """–í—ã—Ç–µ—Å–Ω—è–µ—Ç –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∑–∞–ø–∏—Å–∏"""
        if not self.access_times:
            return
            
        # –ù–∞—Ö–æ–¥–∏–º 10% —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        sorted_items = sorted(self.access_times.items(), key=lambda x: x[1])
        to_remove = sorted_items[:len(sorted_items) // 10 + 1]
        
        for query_hash, _ in to_remove:
            if query_hash in self.cache:
                del self.cache[query_hash]
            if query_hash in self.access_times:
                del self.access_times[query_hash]
    
    def clear(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à"""
        with self.lock:
            self.cache.clear()
            self.access_times.clear()


class OptimizedStorageManager(QObject):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å:
    - –ü—É–ª–æ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    - –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∑–∞–ø—Ä–æ—Å–æ–≤
    - –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏
    - –ë–∞—Ç—á–µ–≤—ã–º–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    """
    
    query_executed = pyqtSignal(str, float)  # query_type, execution_time
    cache_hit = pyqtSignal(str)  # query_hash
    cache_miss = pyqtSignal(str)  # query_hash
    
    def __init__(self, db_path: str = "data/invoices.db", 
                 pool_size: int = 5, cache_size: int = 1000):
        super().__init__()
        
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        self.connection_pool = ConnectionPool(str(self.db_path), pool_size)
        
        # –ö—ç—à –∑–∞–ø—Ä–æ—Å–æ–≤
        self.query_cache = QueryCache(max_size=cache_size)
        
        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π executor
        self.executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="storage")
        
        # –ë–∞—Ç—á–µ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
        self.batch_queue: Queue = Queue()
        self.batch_size = 100
        self.batch_timeout = 5.0  # —Å–µ–∫—É–Ω–¥—ã
        self.batch_timer = QTimer()
        self.batch_timer.timeout.connect(self._process_batch)
        self.batch_timer.start(int(self.batch_timeout * 1000))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.query_stats: List[QueryStats] = []
        self.stats_lock = Lock()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ö–µ–º—ã
        self._initialize_schema()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é
        self.optimization_timer = QTimer()
        self.optimization_timer.timeout.connect(self._periodic_optimization)
        self.optimization_timer.start(3600000)  # –ö–∞–∂–¥—ã–π —á–∞—Å
        
        logger.info(f"üóÑÔ∏è OptimizedStorageManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        logger.info(f"   Database: {self.db_path}")
        logger.info(f"   Pool size: {pool_size}")
        logger.info(f"   Cache size: {cache_size}")
    
    def _initialize_schema(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ö–µ–º—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        schema_sql = """
        -- –¢–∞–±–ª–∏—Ü–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        CREATE TABLE IF NOT EXISTS settings (
            key TEXT PRIMARY KEY,
            value TEXT NOT NULL,
            category TEXT DEFAULT 'general',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
        CREATE INDEX IF NOT EXISTS idx_settings_category ON settings(category);
        CREATE INDEX IF NOT EXISTS idx_settings_updated ON settings(updated_at);
        
        -- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        CREATE TABLE IF NOT EXISTS processing_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_hash TEXT NOT NULL,
            file_path TEXT NOT NULL,
            model_type TEXT NOT NULL,
            result_data TEXT NOT NULL,
            processing_time REAL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(file_hash, model_type)
        );
        
        -- –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        CREATE INDEX IF NOT EXISTS idx_results_hash ON processing_results(file_hash);
        CREATE INDEX IF NOT EXISTS idx_results_model ON processing_results(model_type);
        CREATE INDEX IF NOT EXISTS idx_results_created ON processing_results(created_at);
        
        -- –¢–∞–±–ª–∏—Ü–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        CREATE TABLE IF NOT EXISTS usage_stats (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            event_type TEXT NOT NULL,
            event_data TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        -- –ò–Ω–¥–µ–∫—Å –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        CREATE INDEX IF NOT EXISTS idx_stats_type ON usage_stats(event_type);
        CREATE INDEX IF NOT EXISTS idx_stats_timestamp ON usage_stats(timestamp);
        
        -- –¢—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è updated_at
        CREATE TRIGGER IF NOT EXISTS update_settings_timestamp 
        AFTER UPDATE ON settings 
        BEGIN 
            UPDATE settings SET updated_at = CURRENT_TIMESTAMP WHERE key = NEW.key;
        END;
        """
        
        conn = self.connection_pool.get_connection()
        try:
            conn.executescript(schema_sql)
            conn.commit()
        finally:
            self.connection_pool.return_connection(conn)
    
    def get_setting(self, key: str, default: Any = None, 
                   category: str = 'general', use_cache: bool = True) -> Any:
        """–ü–æ–ª—É—á–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        query_hash = f"setting_{key}_{category}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache:
            cached_result = self.query_cache.get(query_hash)
            if cached_result is not None:
                self.cache_hit.emit(query_hash)
                return cached_result
        
        self.cache_miss.emit(query_hash)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        start_time = time.time()
        conn = self.connection_pool.get_connection()
        
        try:
            cursor = conn.execute(
                "SELECT value FROM settings WHERE key = ? AND category = ?",
                (key, category)
            )
            row = cursor.fetchone()
            
            if row:
                try:
                    result = json.loads(row['value'])
                except json.JSONDecodeError:
                    result = row['value']
            else:
                result = default
            
            # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if use_cache:
                self.query_cache.put(query_hash, result, ttl=600)  # 10 –º–∏–Ω—É—Ç
            
            execution_time = time.time() - start_time
            self._record_query_stats('SELECT', execution_time, 1, query_hash)
            
            return result
            
        finally:
            self.connection_pool.return_connection(conn)
    
    def set_setting(self, key: str, value: Any, category: str = 'general', 
                   batch: bool = False) -> bool:
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É"""
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ
        if isinstance(value, (dict, list, tuple)):
            serialized_value = json.dumps(value, ensure_ascii=False)
        else:
            serialized_value = str(value)
        
        query_data = {
            'type': 'upsert_setting',
            'key': key,
            'value': serialized_value,
            'category': category
        }
        
        if batch:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞—Ç—á
            self.batch_queue.put(query_data)
            return True
        else:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ
            return self._execute_setting_update(query_data)
    
    def _execute_setting_update(self, query_data: Dict) -> bool:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏"""
        start_time = time.time()
        conn = self.connection_pool.get_connection()
        
        try:
            conn.execute("""
                INSERT OR REPLACE INTO settings (key, value, category) 
                VALUES (?, ?, ?)
            """, (query_data['key'], query_data['value'], query_data['category']))
            
            conn.commit()
            
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
            query_hash = f"setting_{query_data['key']}_{query_data['category']}"
            self.query_cache.cache.pop(query_hash, None)
            
            execution_time = time.time() - start_time
            self._record_query_stats('INSERT/UPDATE', execution_time, 1, query_hash)
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ {query_data['key']}: {e}")
            return False
        finally:
            self.connection_pool.return_connection(conn)
    
    def get_processing_result(self, file_hash: str, model_type: str, 
                             use_cache: bool = True) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        query_hash = f"result_{file_hash}_{model_type}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache:
            cached_result = self.query_cache.get(query_hash)
            if cached_result is not None:
                self.cache_hit.emit(query_hash)
                return cached_result
        
        self.cache_miss.emit(query_hash)
        
        start_time = time.time()
        conn = self.connection_pool.get_connection()
        
        try:
            cursor = conn.execute("""
                SELECT result_data, processing_time, created_at 
                FROM processing_results 
                WHERE file_hash = ? AND model_type = ?
            """, (file_hash, model_type))
            
            row = cursor.fetchone()
            if row:
                try:
                    result = {
                        'data': json.loads(row['result_data']),
                        'processing_time': row['processing_time'],
                        'created_at': row['created_at']
                    }
                    
                    # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
                    if use_cache:
                        self.query_cache.put(query_hash, result, ttl=3600)
                    
                    execution_time = time.time() - start_time
                    self._record_query_stats('SELECT', execution_time, 1, query_hash)
                    
                    return result
                    
                except json.JSONDecodeError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                    return None
            
            return None
            
        finally:
            self.connection_pool.return_connection(conn)
    
    def save_processing_result(self, file_hash: str, file_path: str, 
                              model_type: str, result_data: Dict, 
                              processing_time: float, batch: bool = False) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        query_data = {
            'type': 'save_result',
            'file_hash': file_hash,
            'file_path': file_path,
            'model_type': model_type,
            'result_data': json.dumps(result_data, ensure_ascii=False),
            'processing_time': processing_time
        }
        
        if batch:
            self.batch_queue.put(query_data)
            return True
        else:
            return self._execute_result_save(query_data)
    
    def _execute_result_save(self, query_data: Dict) -> bool:
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        start_time = time.time()
        conn = self.connection_pool.get_connection()
        
        try:
            conn.execute("""
                INSERT OR REPLACE INTO processing_results 
                (file_hash, file_path, model_type, result_data, processing_time)
                VALUES (?, ?, ?, ?, ?)
            """, (
                query_data['file_hash'],
                query_data['file_path'],
                query_data['model_type'],
                query_data['result_data'],
                query_data['processing_time']
            ))
            
            conn.commit()
            
            # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à
            query_hash = f"result_{query_data['file_hash']}_{query_data['model_type']}"
            self.query_cache.cache.pop(query_hash, None)
            
            execution_time = time.time() - start_time
            self._record_query_stats('INSERT/UPDATE', execution_time, 1, query_hash)
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
            return False
        finally:
            self.connection_pool.return_connection(conn)
    
    def _process_batch(self):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –æ–ø–µ—Ä–∞—Ü–∏–π"""
        if self.batch_queue.empty():
            return
        
        operations = []
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
            while len(operations) < self.batch_size:
                try:
                    operation = self.batch_queue.get_nowait()
                    operations.append(operation)
                except Empty:
                    break
            
            if not operations:
                return
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –±–∞—Ç—á
            self._execute_batch_operations(operations)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞: {e}")
    
    def _execute_batch_operations(self, operations: List[Dict]):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –±–∞—Ç—á –æ–ø–µ—Ä–∞—Ü–∏–π"""
        if not operations:
            return
        
        start_time = time.time()
        conn = self.connection_pool.get_connection()
        
        try:
            conn.execute("BEGIN TRANSACTION")
            
            for operation in operations:
                if operation['type'] == 'upsert_setting':
                    conn.execute("""
                        INSERT OR REPLACE INTO settings (key, value, category) 
                        VALUES (?, ?, ?)
                    """, (operation['key'], operation['value'], operation['category']))
                    
                elif operation['type'] == 'save_result':
                    conn.execute("""
                        INSERT OR REPLACE INTO processing_results 
                        (file_hash, file_path, model_type, result_data, processing_time)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        operation['file_hash'],
                        operation['file_path'],
                        operation['model_type'],
                        operation['result_data'],
                        operation['processing_time']
                    ))
            
            conn.execute("COMMIT")
            
            execution_time = time.time() - start_time
            self._record_query_stats('BATCH', execution_time, len(operations), 'batch')
            
            logger.info(f"‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω –±–∞—Ç—á –∏–∑ {len(operations)} –æ–ø–µ—Ä–∞—Ü–∏–π –∑–∞ {execution_time:.3f}—Å")
            
        except Exception as e:
            conn.execute("ROLLBACK")
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞: {e}")
        finally:
            self.connection_pool.return_connection(conn)
    
    def _record_query_stats(self, query_type: str, execution_time: float, 
                           rows_affected: int, query_hash: str):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
        with self.stats_lock:
            stats = QueryStats(
                query_type=query_type,
                execution_time=execution_time,
                rows_affected=rows_affected,
                timestamp=time.time(),
                query_hash=query_hash
            )
            
            self.query_stats.append(stats)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if len(self.query_stats) > 10000:
                self.query_stats = self.query_stats[-5000:]
            
            self.query_executed.emit(query_type, execution_time)
    
    def _periodic_optimization(self):
        """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîß –ó–∞–ø—É—Å–∫ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ë–î")
        
        conn = self.connection_pool.get_connection()
        try:
            # VACUUM –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –º–µ—Å—Ç–∞
            conn.execute("VACUUM")
            
            # ANALYZE –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            conn.execute("ANALYZE")
            
            # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø–∏—Å–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (—Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)
            conn.execute("""
                DELETE FROM usage_stats 
                WHERE timestamp < datetime('now', '-30 days')
            """)
            
            conn.commit()
            
            logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ë–î –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ë–î: {e}")
        finally:
            self.connection_pool.return_connection(conn)
    
    def get_statistics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã"""
        with self.stats_lock:
            if not self.query_stats:
                return {}
            
            total_queries = len(self.query_stats)
            avg_time = sum(s.execution_time for s in self.query_stats) / total_queries
            
            query_types = {}
            for stats in self.query_stats:
                if stats.query_type not in query_types:
                    query_types[stats.query_type] = {
                        'count': 0,
                        'total_time': 0,
                        'avg_time': 0
                    }
                
                query_types[stats.query_type]['count'] += 1
                query_types[stats.query_type]['total_time'] += stats.execution_time
                
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∞
            for query_type, data in query_types.items():
                data['avg_time'] = data['total_time'] / data['count']
            
            return {
                'total_queries': total_queries,
                'average_execution_time': avg_time,
                'query_types': query_types,
                'cache_size': len(self.query_cache.cache),
                'connection_pool_size': self.connection_pool.pool_size,
                'batch_queue_size': self.batch_queue.qsize()
            }
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –±–∞—Ç—á–∏
        self._process_batch()
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–µ—Ä—ã
        self.batch_timer.stop()
        self.optimization_timer.stop()
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º executor
        self.executor.shutdown(wait=True)
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
        self.connection_pool.close_all()
        
        # –û—á–∏—â–∞–µ–º –∫—ç—à
        self.query_cache.clear()
        
        logger.info("üßπ OptimizedStorageManager –æ—á–∏—â–µ–Ω")


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_storage_manager: Optional[OptimizedStorageManager] = None


def get_optimized_storage_manager() -> OptimizedStorageManager:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è"""
    global _storage_manager
    if _storage_manager is None:
        _storage_manager = OptimizedStorageManager()
    return _storage_manager 