"""
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è TrOCR –æ–±—É—á–µ–Ω–∏—è
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
"""

import os
import json
import math
import logging
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
from pathlib import Path


@dataclass
class DatasetCharacteristics:
    """–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
    size: int
    avg_text_length: float
    complexity_score: float
    label_diversity: int
    image_dimensions: Tuple[int, int]
    

@dataclass
class OptimizationResults:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    epochs: int
    batch_size: int
    learning_rate: float
    gradient_accumulation_steps: int
    warmup_steps: int
    scheduler_type: str
    optimization_reason: str
    expected_training_time: int  # –≤ –º–∏–Ω—É—Ç–∞—Ö
    memory_usage_estimate: float  # –≤ GB


class TrOCRHyperparameterOptimizer:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è TrOCR
    """
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            logger: –õ–æ–≥–≥–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        self.logger = logger or logging.getLogger(__name__)
        
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        self.size_based_configs = {
            'tiny': {  # < 20 –ø—Ä–∏–º–µ—Ä–æ–≤
                'base_epochs': 10,
                'base_lr': 3e-5,
                'base_batch_size': 2,
                'gradient_accumulation': 4
            },
            'small': {  # 20-50 –ø—Ä–∏–º–µ—Ä–æ–≤
                'base_epochs': 8,
                'base_lr': 5e-5,
                'base_batch_size': 4,
                'gradient_accumulation': 2
            },
            'medium': {  # 50-200 –ø—Ä–∏–º–µ—Ä–æ–≤
                'base_epochs': 5,
                'base_lr': 5e-5,
                'base_batch_size': 8,
                'gradient_accumulation': 1
            },
            'large': {  # > 200 –ø—Ä–∏–º–µ—Ä–æ–≤
                'base_epochs': 3,
                'base_lr': 2e-5,
                'base_batch_size': 16,
                'gradient_accumulation': 1
            }
        }
        
        # GPU memory limits –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        self.memory_configs = {
            'conservative': {'max_batch': 2, 'memory_limit': 4.0},
            'balanced': {'max_batch': 4, 'memory_limit': 8.0},
            'aggressive': {'max_batch': 8, 'memory_limit': 12.0}
        }
    
    def analyze_dataset(self, dataset_path: str) -> DatasetCharacteristics:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            
        Returns:
            DatasetCharacteristics: –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        """
        try:
            from datasets import load_from_disk
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = load_from_disk(dataset_path)
            train_dataset = dataset['train'] if 'train' in dataset else dataset
            
            size = len(train_dataset)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã
            texts = [item['text'] for item in train_dataset if 'text' in item]
            avg_text_length = sum(len(text) for text in texts) / len(texts) if texts else 0
            
            # –û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            complexity_score = min(10.0, avg_text_length / 10.0)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ç–æ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            all_labels = set()
            for item in train_dataset:
                if 'labels' in item:
                    all_labels.update(item['labels'])
            label_diversity = len(all_labels)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image_dims = (384, 384)  # TrOCR —Å—Ç–∞–Ω–¥–∞—Ä—Ç
            if train_dataset and 'image' in train_dataset[0]:
                try:
                    first_image = train_dataset[0]['image']
                    if hasattr(first_image, 'size'):
                        image_dims = first_image.size
                except:
                    pass
            
            return DatasetCharacteristics(
                size=size,
                avg_text_length=avg_text_length,
                complexity_score=complexity_score,
                label_diversity=label_diversity,
                image_dimensions=image_dims
            )
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            return DatasetCharacteristics(
                size=10,
                avg_text_length=20.0,
                complexity_score=5.0,
                label_diversity=5,
                image_dimensions=(384, 384)
            )
    
    def analyze_previous_results(self, model_output_dir: str) -> Optional[Dict]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            model_output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            Dict: –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ None
        """
        try:
            metadata_path = Path(model_output_dir) / "training_metadata.json"
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {e}")
        
        return None
    
    def optimize_hyperparameters(
        self, 
        dataset_path: str,
        gpu_memory_gb: float = 12.0,
        target_training_time_minutes: int = 30,
        previous_results: Optional[Dict] = None
    ) -> OptimizationResults:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            gpu_memory_gb: –î–æ—Å—Ç—É–ø–Ω–∞—è GPU –ø–∞–º—è—Ç—å –≤ GB
            target_training_time_minutes: –¶–µ–ª–µ–≤–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –≤ –º–∏–Ω—É—Ç–∞—Ö
            previous_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            OptimizationResults: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        characteristics = self.analyze_dataset(dataset_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if characteristics.size < 20:
            size_category = 'tiny'
        elif characteristics.size < 50:
            size_category = 'small'
        elif characteristics.size < 200:
            size_category = 'medium'
        else:
            size_category = 'large'
        
        # –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        base_config = self.size_based_configs[size_category]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–∞–º—è—Ç–∏
        if gpu_memory_gb >= 10:
            memory_category = 'aggressive'
        elif gpu_memory_gb >= 6:
            memory_category = 'balanced'
        else:
            memory_category = 'conservative'
        
        memory_config = self.memory_configs[memory_category]
        
        # –ù–∞—á–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        epochs = base_config['base_epochs']
        learning_rate = base_config['base_lr']
        batch_size = min(base_config['base_batch_size'], memory_config['max_batch'])
        gradient_accumulation = base_config['gradient_accumulation']
        
        optimization_reasons = []
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if previous_results:
            final_loss = previous_results.get('final_loss', 0)
            prev_epochs = previous_results.get('epochs', 3)
            prev_lr = previous_results.get('learning_rate', 5e-5)
            
            # –ï—Å–ª–∏ loss —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            if final_loss > 10.0:
                epochs = min(epochs + 5, 15)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —ç–ø–æ—Ö–∏
                learning_rate = prev_lr * 0.8  # –°–Ω–∏–∂–∞–µ–º LR
                optimization_reasons.append(f"–í—ã—Å–æ–∫–∏–π loss ({final_loss:.2f}) - —É–≤–µ–ª–∏—á–µ–Ω—ã —ç–ø–æ—Ö–∏ –∏ —Å–Ω–∏–∂–µ–Ω LR")
            elif final_loss > 5.0:
                epochs = min(epochs + 2, 10)
                optimization_reasons.append(f"–£–º–µ—Ä–µ–Ω–Ω—ã–π loss ({final_loss:.2f}) - —É–≤–µ–ª–∏—á–µ–Ω—ã —ç–ø–æ—Ö–∏")
            elif final_loss < 1.0:
                epochs = max(epochs - 1, 2)
                optimization_reasons.append(f"–ù–∏–∑–∫–∏–π loss ({final_loss:.2f}) - —É–º–µ–Ω—å—à–µ–Ω—ã —ç–ø–æ—Ö–∏")
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        if characteristics.size < 20:
            # –î–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ —ç–ø–æ—Ö –Ω–æ –º–µ–Ω—å—à–∏–π LR
            epochs = max(epochs, 8)
            learning_rate = min(learning_rate, 2e-5)
            batch_size = min(batch_size, 2)  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            gradient_accumulation = max(gradient_accumulation, 4)
            optimization_reasons.append(f"–ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç ({characteristics.size}) - –±–æ–ª—å—à–µ —ç–ø–æ—Ö, –º–µ–Ω—å—à–∏–π LR")
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        if characteristics.avg_text_length > 50:
            learning_rate *= 0.8  # –°–Ω–∏–∂–∞–µ–º LR –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            epochs = min(epochs + 2, 12)
            optimization_reasons.append(f"–î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (—Å—Ä. {characteristics.avg_text_length:.1f}) - —Å–Ω–∏–∂–µ–Ω LR")
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        steps_per_epoch = max(1, characteristics.size // (batch_size * gradient_accumulation))
        estimated_time = epochs * steps_per_epoch * 2  # ~2 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ —à–∞–≥
        
        if estimated_time > target_training_time_minutes * 60:
            # –°–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ - —Å–æ–∫—Ä–∞—â–∞–µ–º —ç–ø–æ—Ö–∏
            max_epochs = max(3, target_training_time_minutes * 30 // steps_per_epoch)
            if epochs > max_epochs:
                epochs = max_epochs
                optimization_reasons.append(f"–°–æ–∫—Ä–∞—â–µ–Ω—ã —ç–ø–æ—Ö–∏ –¥–æ {epochs} –¥–ª—è —É–∫–ª–∞–¥–∫–∏ –≤ {target_training_time_minutes}–º–∏–Ω")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ scheduler –∏ warmup
        total_steps = epochs * steps_per_epoch
        warmup_steps = min(total_steps // 10, 100)  # 10% –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º 100 —à–∞–≥–æ–≤
        scheduler_type = "linear" if total_steps > 50 else "constant"
        
        # –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
        memory_estimate = self._estimate_memory_usage(batch_size, gradient_accumulation)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
        if memory_estimate > gpu_memory_gb * 0.9:  # 90% –ª–∏–º–∏—Ç
            # –°–Ω–∏–∂–∞–µ–º batch_size
            while batch_size > 1 and memory_estimate > gpu_memory_gb * 0.9:
                batch_size = max(1, batch_size - 1)
                gradient_accumulation = min(gradient_accumulation + 1, 8)
                memory_estimate = self._estimate_memory_usage(batch_size, gradient_accumulation)
            optimization_reasons.append(f"–°–Ω–∏–∂–µ–Ω batch_size –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
        reason = "; ".join(optimization_reasons) if optimization_reasons else f"–ë–∞–∑–æ–≤–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è {size_category} –¥–∞—Ç–∞—Å–µ—Ç–∞"
        
        return OptimizationResults(
            epochs=epochs,
            batch_size=batch_size,
            learning_rate=learning_rate,
            gradient_accumulation_steps=gradient_accumulation,
            warmup_steps=warmup_steps,
            scheduler_type=scheduler_type,
            optimization_reason=reason,
            expected_training_time=estimated_time // 60,
            memory_usage_estimate=memory_estimate
        )
    
    def _estimate_memory_usage(self, batch_size: int, gradient_accumulation: int) -> float:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏
        
        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            gradient_accumulation: –®–∞–≥–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            
        Returns:
            float: –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ GB
        """
        # –ë–∞–∑–æ–≤–∞—è –ø–∞–º—è—Ç—å –º–æ–¥–µ–ª–∏ TrOCR
        base_memory = 1.5  # GB –¥–ª—è –º–æ–¥–µ–ª–∏
        
        # –ü–∞–º—è—Ç—å –Ω–∞ –±–∞—Ç—á (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
        batch_memory = batch_size * 0.8  # GB –Ω–∞ –µ–¥–∏–Ω–∏—Ü—É –±–∞—Ç—á–∞
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer_memory = 1.0  # GB –¥–ª—è AdamW
        
        # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –ø–∞–º—è—Ç—å
        accumulation_factor = 1 + (gradient_accumulation - 1) * 0.3
        
        total_memory = (base_memory + batch_memory + optimizer_memory) * accumulation_factor
        
        return total_memory
    
    def get_learning_rate_schedule_recommendations(self, characteristics: DatasetCharacteristics) -> Dict:
        """
        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫—É learning rate
        
        Args:
            characteristics: –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            Dict: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ LR schedule
        """
        if characteristics.size < 30:
            return {
                'scheduler': 'constant_with_warmup',
                'warmup_ratio': 0.1,
                'reason': '–ú–∞–ª–µ–Ω—å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç - —Å—Ç–∞–±–∏–ª—å–Ω—ã–π LR —Å warmup'
            }
        elif characteristics.complexity_score > 7:
            return {
                'scheduler': 'cosine',
                'warmup_ratio': 0.05,
                'reason': '–°–ª–æ–∂–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç - –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ'
            }
        else:
            return {
                'scheduler': 'linear',
                'warmup_ratio': 0.1,
                'reason': '–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ª—É—á–∞–π - –ª–∏–Ω–µ–π–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ'
            }
    
    def generate_training_report(self, optimization: OptimizationResults, characteristics: DatasetCharacteristics) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ–± –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        
        Args:
            optimization: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            characteristics: –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            str: –û—Ç—á–µ—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
        """
        report = f"""
üîß –û–¢–ß–ï–¢ –û–ë –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í TrOCR

üìä –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞:
   ‚Ä¢ –†–∞–∑–º–µ—Ä: {characteristics.size} –ø—Ä–∏–º–µ—Ä–æ–≤
   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {characteristics.avg_text_length:.1f} —Å–∏–º–≤–æ–ª–æ–≤
   ‚Ä¢ –°–ª–æ–∂–Ω–æ—Å—Ç—å: {characteristics.complexity_score:.1f}/10
   ‚Ä¢ –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –º–µ—Ç–æ–∫: {characteristics.label_diversity}
   ‚Ä¢ –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {characteristics.image_dimensions[0]}x{characteristics.image_dimensions[1]}

‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
   ‚Ä¢ –≠–ø–æ—Ö–∏: {optimization.epochs}
   ‚Ä¢ Batch size: {optimization.batch_size}
   ‚Ä¢ Learning rate: {optimization.learning_rate:.2e}
   ‚Ä¢ Gradient accumulation: {optimization.gradient_accumulation_steps}
   ‚Ä¢ Warmup steps: {optimization.warmup_steps}
   ‚Ä¢ Scheduler: {optimization.scheduler_type}

üìà –ü—Ä–æ–≥–Ω–æ–∑—ã:
   ‚Ä¢ –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: ~{optimization.expected_training_time} –º–∏–Ω—É—Ç
   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏: ~{optimization.memory_usage_estimate:.1f} GB

üí° –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
   {optimization.optimization_reason}

üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
   ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ validation loss –∫–∞–∂–¥—ã–µ 2-3 —ç–ø–æ—Ö–∏
   ‚Ä¢ –ü—Ä–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏ —É–º–µ–Ω—å—à–∏—Ç–µ learning rate –≤ 2 —Ä–∞–∑–∞
   ‚Ä¢ –ü—Ä–∏ –ø–ª–∞—Ç–æ –¥–æ–±–∞–≤—å—Ç–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–π—Ç–µ checkpoint'—ã –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è
"""
        return report.strip() 