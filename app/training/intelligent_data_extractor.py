"""
–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è InvoiceGemini
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Gemini –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –í–°–ï–• –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

import json
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import base64
from pathlib import Path

@dataclass
class ExtractedField:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ –ø–æ–ª—è"""
    name: str                    # –ù–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–Ω–æ–º–µ—Ä_—Å—á–µ—Ç–∞")
    value: str                   # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è
    confidence: float            # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ (0.0-1.0)
    field_type: str             # –¢–∏–ø –ø–æ–ª—è (text, number, date, amount, etc.)
    category: str               # –ö–∞—Ç–µ–≥–æ—Ä–∏—è (invoice_info, company_info, items, etc.)
    coordinates: Optional[Dict] = None  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
    
class IntelligentDataExtractor:
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Gemini
    """
    
    def __init__(self, gemini_processor, logger=None):
        self.gemini_processor = gemini_processor
        self.logger = logger or logging.getLogger(__name__)
        
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤—Å–µ—Ö –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.extraction_prompt = """
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–æ–∫—É–º–µ–Ω—Ç (—Å—á–µ—Ç, –Ω–∞–∫–ª–∞–¥–Ω–∞—è, –¥–æ–≥–æ–≤–æ—Ä –∏ —Ç.–¥.) –∏ –∏–∑–≤–ª–µ–∫–∏ –í–°–ï –ø–æ–ª–µ–∑–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

–í–ê–ñ–ù–û: –ù–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–π—Å—è —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø–æ–ª—è–º–∏ - –∏–∑–≤–ª–µ–∫–∞–π –í–°–ï –¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–Ω—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç–∞.

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π:

{
  "document_type": "—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—Å—á–µ—Ç, –Ω–∞–∫–ª–∞–¥–Ω–∞—è, –¥–æ–≥–æ–≤–æ—Ä, –∏ —Ç.–¥.)",
  "extracted_fields": [
    {
      "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ_–ø–æ–ª—è_–Ω–∞_—Ä—É—Å—Å–∫–æ–º",
      "value": "–∑–Ω–∞—á–µ–Ω–∏–µ",
      "confidence": 0.95,
      "field_type": "text|number|date|amount|email|phone|address|tax_id|etc",
      "category": "invoice_info|company_info|client_info|items|payment_info|legal_info|etc"
    }
  ]
}

–ö–ê–¢–ï–ì–û–†–ò–ò –ü–û–õ–ï–ô:
- invoice_info: –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞, –¥–∞—Ç–∞, —Å—Ä–æ–∫ –æ–ø–ª–∞—Ç—ã, –≤–∞–ª—é—Ç–∞
- company_info: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏, –ò–ù–ù, –ö–ü–ü, –∞–¥—Ä–µ—Å, —Ç–µ–ª–µ—Ñ–æ–Ω, email –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
- client_info: –¥–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞/–ø–æ–∫—É–ø–∞—Ç–µ–ª—è
- items: —Ç–æ–≤–∞—Ä—ã, —É—Å–ª—É–≥–∏, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, —Ü–µ–Ω—ã
- payment_info: –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã, —Å–ø–æ—Å–æ–±—ã –æ–ø–ª–∞—Ç—ã
- amounts: —Å—É–º–º—ã, –ù–î–°, –∏—Ç–æ–≥–æ
- legal_info: —é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, –ø–æ–¥–ø–∏—Å–∏, –ø–µ—á–∞—Ç–∏
- logistics: –∞–¥—Ä–µ—Å–∞ –¥–æ—Å—Ç–∞–≤–∫–∏, —Å—Ä–æ–∫–∏, —É—Å–ª–æ–≤–∏—è
- other: –ø—Ä–æ—á–∏–µ –ø–æ–ª–µ–∑–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

–¢–ò–ü–´ –ü–û–õ–ï–ô:
- text: –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç
- number: —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
- date: –¥–∞—Ç—ã
- amount: –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã
- email: email –∞–¥—Ä–µ—Å–∞
- phone: —Ç–µ–ª–µ—Ñ–æ–Ω—ã
- address: –∞–¥—Ä–µ—Å–∞
- tax_id: –Ω–∞–ª–æ–≥–æ–≤—ã–µ –Ω–æ–º–µ—Ä–∞ (–ò–ù–ù, –ö–ü–ü)
- bank_account: –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Å—á–µ—Ç–∞
- percentage: –ø—Ä–æ—Ü–µ–Ω—Ç—ã

–ò–∑–≤–ª–µ–∫–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ - –ª—É—á—à–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö, —á–µ–º –º–µ–Ω—å—à–µ!
"""

    def extract_all_data(self, image_path: str) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ –ø–æ–ª–µ–∑–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            self.logger.info(f"üß† –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {image_path}")
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Gemini
            response = self.gemini_processor.process_image_with_prompt(
                image_path, 
                self.extraction_prompt
            )
            
            if not response:
                self.logger.error("‚ùå Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç")
                return self._create_empty_result()
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            extracted_data = self._parse_gemini_response(response)
            
            if not extracted_data:
                self.logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ç–≤–µ—Ç Gemini")
                return self._create_empty_result()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
            processed_data = self._process_extracted_data(extracted_data)
            
            self.logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(processed_data.get('fields', []))} –ø–æ–ª–µ–π")
            self._log_extraction_summary(processed_data)
            
            return processed_data
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return self._create_empty_result()
    
    def _parse_gemini_response(self, response: str) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç Gemini"""
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                self.logger.error("‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ Gemini")
                return None
            
            json_str = json_match.group(0)
            return json.loads(json_str)
            
        except json.JSONDecodeError as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏ JSON
            return self._try_fix_json(response)
        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return None
    
    def _try_fix_json(self, response: str) -> Optional[Dict]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–π JSON"""
        try:
            # –£–¥–∞–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
            cleaned = re.sub(r'//.*?\n', '', response)
            cleaned = re.sub(r'/\*.*?\*/', '', cleaned, flags=re.DOTALL)
            
            # –ò—â–µ–º JSON —Å–Ω–æ–≤–∞
            json_match = re.search(r'\{.*\}', cleaned, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
                
        except Exception as e:
            self.logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å JSON: {e}")
        
        return None
    
    def _process_extracted_data(self, raw_data: Dict) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
        try:
            processed_fields = []
            
            for field_data in raw_data.get('extracted_fields', []):
                field = self._create_extracted_field(field_data)
                if field:
                    processed_fields.append(field)
            
            return {
                'document_type': raw_data.get('document_type', 'unknown'),
                'fields': processed_fields,
                'total_fields': len(processed_fields),
                'extraction_timestamp': datetime.now().isoformat(),
                'categories': self._get_categories_summary(processed_fields)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return self._create_empty_result()
    
    def _create_extracted_field(self, field_data: Dict) -> Optional[ExtractedField]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç ExtractedField –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if not all(key in field_data for key in ['name', 'value']):
                return None
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
            name = str(field_data['name']).strip().lower()
            value = str(field_data['value']).strip()
            
            if not name or not value:
                return None
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–µ
            field = ExtractedField(
                name=name,
                value=value,
                confidence=float(field_data.get('confidence', 0.8)),
                field_type=field_data.get('field_type', 'text'),
                category=field_data.get('category', 'other')
            )
            
            return field
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–ª—è: {e}")
            return None
    
    def _get_categories_summary(self, fields: List[ExtractedField]) -> Dict[str, int]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø–æ–ª–µ–π"""
        categories = {}
        for field in fields:
            category = field.category
            categories[category] = categories.get(category, 0) + 1
        return categories
    
    def _log_extraction_summary(self, data: Dict):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
        try:
            self.logger.info(f"üìä –°–≤–æ–¥–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è:")
            self.logger.info(f"   üìÑ –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞: {data.get('document_type', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            self.logger.info(f"   üìù –í—Å–µ–≥–æ –ø–æ–ª–µ–π: {data.get('total_fields', 0)}")
            
            categories = data.get('categories', {})
            for category, count in categories.items():
                self.logger.info(f"   üìÇ {category}: {count} –ø–æ–ª–µ–π")
                
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –ø–æ–ª–µ–π
            fields = data.get('fields', [])[:5]  # –ü–µ—Ä–≤—ã–µ 5 –ø–æ–ª–µ–π
            if fields:
                self.logger.info("   üîç –ü—Ä–∏–º–µ—Ä—ã –ø–æ–ª–µ–π:")
                for field in fields:
                    self.logger.info(f"      ‚Ä¢ {field.name}: {field.value[:50]}...")
                    
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}")
    
    def _create_empty_result(self) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {
            'document_type': 'unknown',
            'fields': [],
            'total_fields': 0,
            'extraction_timestamp': datetime.now().isoformat(),
            'categories': {}
        }
    
    def convert_to_training_format(self, extracted_data: Dict, ocr_data: List[Dict]) -> Dict[str, Any]:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            extracted_data: –î–∞–Ω–Ω—ã–µ –æ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
            ocr_data: OCR –¥–∞–Ω–Ω—ã–µ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ —Å–ª–æ–≤
            
        Returns:
            –î–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        try:
            self.logger.info("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è...")
            
            # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∑–Ω–∞—á–µ–Ω–∏–π –∫ —Å–ª–æ–≤–∞–º OCR
            field_mappings = self._map_fields_to_ocr(extracted_data['fields'], ocr_data)
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_labels = self._create_training_labels(field_mappings, ocr_data)
            
            result = {
                'words': [word['text'] for word in ocr_data],
                'bboxes': [word['bbox'] for word in ocr_data],
                'labels': training_labels,
                'field_mappings': field_mappings,
                'extracted_fields_count': len(extracted_data['fields']),
                'document_type': extracted_data.get('document_type', 'unknown')
            }
            
            self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(training_labels)} –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
            return {
                'words': [word['text'] for word in ocr_data],
                'bboxes': [word['bbox'] for word in ocr_data],
                'labels': ['O'] * len(ocr_data),
                'field_mappings': {},
                'extracted_fields_count': 0,
                'document_type': 'unknown'
            }
    
    def _map_fields_to_ocr(self, fields: List[ExtractedField], ocr_data: List[Dict]) -> Dict[str, List[int]]:
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—è —Å–æ —Å–ª–æ–≤–∞–º–∏ OCR"""
        mappings = {}
        
        for field in fields:
            # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ OCR –¥–∞–Ω–Ω—ã—Ö
            matches = self._find_field_matches(field.value, ocr_data)
            if matches:
                label_name = self._create_label_name(field)
                mappings[label_name] = matches
                
        return mappings
    
    def _find_field_matches(self, field_value: str, ocr_data: List[Dict]) -> List[int]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ OCR, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—é –ø–æ–ª—è"""
        matches = []
        field_words = field_value.lower().split()
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for i, word_data in enumerate(ocr_data):
            word = word_data['text'].lower()
            if word in field_words:
                matches.append(i)
        
        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        if not matches and len(field_value) > 10:
            for i, word_data in enumerate(ocr_data):
                word = word_data['text'].lower()
                if len(word) > 3 and word in field_value.lower():
                    matches.append(i)
        
        return matches
    
    def _create_label_name(self, field: ExtractedField) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–∫–∏ –¥–ª—è –ø–æ–ª—è"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ–ª—è
        normalized_name = field.name.upper().replace(' ', '_').replace('-', '_')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if field.category != 'other':
            category_prefix = field.category.upper().replace('_INFO', '')
            if not normalized_name.startswith(category_prefix):
                normalized_name = f"{category_prefix}_{normalized_name}"
        
        return normalized_name
    
    def _create_training_labels(self, field_mappings: Dict[str, List[int]], ocr_data: List[Dict]) -> List[str]:
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ BIO"""
        labels = ['O'] * len(ocr_data)
        
        for label_name, indices in field_mappings.items():
            if not indices:
                continue
                
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
            indices = sorted(indices)
            
            # –ü–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –ø–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∫—É B- (Beginning)
            labels[indices[0]] = f"B-{label_name}"
            
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –ø–æ–ª—É—á–∞—é—Ç –º–µ—Ç–∫—É I- (Inside)
            for idx in indices[1:]:
                labels[idx] = f"I-{label_name}"
        
        return labels
    
    def save_extraction_results(self, results: Dict, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≤ —Ñ–∞–π–ª"""
        try:
            output_file = Path(output_path)
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º ExtractedField –æ–±—ä–µ–∫—Ç—ã –≤ —Å–ª–æ–≤–∞—Ä–∏
            serializable_results = self._make_serializable(results)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_results, f, ensure_ascii=False, indent=2)
                
            self.logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    def _make_serializable(self, data: Any) -> Any:
        """–î–µ–ª–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–º–∏ –¥–ª—è JSON"""
        if isinstance(data, dict):
            return {k: self._make_serializable(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self._make_serializable(item) for item in data]
        elif isinstance(data, ExtractedField):
            return asdict(data)
        else:
            return data 