#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Enhanced TrOCR Dataset Preparator —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π —á–µ—Ä–µ–∑ LLM Gemini

–ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ TrOCR
—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö LLM –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö.

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞:
- Microsoft TrOCR: https://huggingface.co/docs/transformers/model_doc/trocr
- Gemini Vision API: https://ai.google.dev/docs/vision
- –õ—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: https://medium.com/@mshayan38/data-annotation-using-modern-llms-gemini-82f8823a6f12
"""

import os
import json
import logging
import tempfile
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any, Callable
from dataclasses import dataclass, field
from PIL import Image, ImageDraw, ImageFont, ImageEnhance
import pandas as pd
import torch
import numpy as np
from tqdm import tqdm
import random
import string

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è transformers
try:
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    import torchvision.transforms as transforms
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è Gemini
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
from .trocr_dataset_preparator import TrOCRDatasetPreparator, TrOCRDatasetConfig, TrOCRCustomDataset


@dataclass
class EnhancedTrOCRConfig(TrOCRDatasetConfig):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LLM –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏"""
    
    # LLM –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    enable_llm_annotation: bool = True
    llm_provider: str = "gemini"  # gemini, openai, anthropic
    llm_model: str = "models/gemini-2.0-flash-exp"
    max_llm_requests_per_minute: int = 60
    llm_confidence_threshold: float = 0.8
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è OCR —Ä–∞–∑–º–µ—Ç–∫–∏
    ocr_languages: List[str] = field(default_factory=lambda: ["ru", "en"])
    extract_bounding_boxes: bool = True
    min_word_confidence: float = 0.5
    use_layout_analysis: bool = True
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    enable_quality_filter: bool = True
    min_text_length_chars: int = 3
    max_text_length_chars: int = 500
    filter_non_text_images: bool = True
    
    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
    synthetic_templates: List[str] = field(default_factory=lambda: [
        "invoice", "receipt", "document", "form", "table"
    ])
    synthetic_fonts: List[str] = field(default_factory=lambda: [
        "arial.ttf", "times.ttf", "courier.ttf"
    ])
    synthetic_backgrounds: List[str] = field(default_factory=lambda: [
        "white", "light_gray", "cream", "light_blue"
    ])
    
    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è
    enable_document_layout_augmentation: bool = True
    enable_text_style_variation: bool = True
    enable_background_texture: bool = True


class LLMAnnotationEngine:
    """–î–≤–∏–∂–æ–∫ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é LLM"""
    
    def __init__(self, config: EnhancedTrOCRConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.client = None
        self.request_count = 0
        self.last_request_time = 0
        
        self._setup_llm_client()
    
    def _setup_llm_client(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞ LLM"""
        if self.config.llm_provider == "gemini" and GEMINI_AVAILABLE:
            try:
                # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á
                from app.settings_manager import settings_manager
                api_key = settings_manager.get_gemini_api_key()
                
                if api_key:
                    genai.configure(api_key=api_key)
                    self.client = genai.GenerativeModel(self.config.llm_model)
                    self.logger.info(f"‚úÖ Gemini –∫–ª–∏–µ–Ω—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω: {self.config.llm_model}")
                else:
                    self.logger.error("‚ùå API –∫–ª—é—á Gemini –Ω–µ –Ω–∞–π–¥–µ–Ω")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Gemini: {e}")
        else:
            self.logger.warning("‚ö†Ô∏è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
    
    def _rate_limit_check(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        current_time = time.time()
        if current_time - self.last_request_time < 60:
            if self.request_count >= self.config.max_llm_requests_per_minute:
                sleep_time = 60 - (current_time - self.last_request_time)
                self.logger.info(f"‚è±Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤, –æ–∂–∏–¥–∞–µ–º {sleep_time:.1f}—Å")
                time.sleep(sleep_time)
                self.request_count = 0
                self.last_request_time = time.time()
        else:
            self.request_count = 0
            self.last_request_time = current_time
    
    def annotate_image_for_ocr(self, image_path: str) -> Optional[Dict[str, Any]]:
        """
        –°–æ–∑–¥–∞–µ—Ç OCR –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é LLM
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            Dict —Å OCR –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None
        """
        if not self.client:
            return None
        
        try:
            self._rate_limit_check()
            self.request_count += 1
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è OCR
            prompt = self._create_ocr_annotation_prompt()
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è OCR
            generation_config = {
                "temperature": 0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                "top_p": 0.9,
                "top_k": 40,
                "max_output_tokens": 4000,
                "response_mime_type": "application/json"
            }
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = self.client.generate_content(
                [prompt, image],
                generation_config=generation_config
            )
            
            if response and response.text:
                # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
                try:
                    ocr_data = json.loads(response.text)
                    return self._validate_ocr_annotation(ocr_data)
                except json.JSONDecodeError as e:
                    self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç LLM: {e}")
                    return self._extract_text_fallback(response.text)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
        
        return None
    
    def _create_ocr_annotation_prompt(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è OCR –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        return """
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ç–∏—á–µ—Å–∫–æ–º—É —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é —Ç–µ–∫—Å—Ç–∞ (OCR). –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –∏–∑–≤–ª–µ–∫–∏ –≤–µ—Å—å –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç.

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–º JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{
    "text_blocks": [
        {
            "text": "–∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç",
            "confidence": 0.95,
            "bbox": [x1, y1, x2, y2],
            "language": "ru",
            "font_size": "medium",
            "text_type": "paragraph"
        }
    ],
    "full_text": "–≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "document_type": "invoice|receipt|document|form|other",
    "layout_structure": {
        "has_table": true,
        "has_header": true,
        "has_footer": false,
        "columns": 1
    },
    "quality_metrics": {
        "image_quality": "high|medium|low",
        "text_clarity": "clear|blurry|unclear",
        "overall_confidence": 0.85
    }
}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –ò–∑–≤–ª–µ–∫–∞–π –í–°–ï –≤–∏–¥–∏–º—ã–π —Ç–µ–∫—Å—Ç, –≤–∫–ª—é—á–∞—è —á–∏—Å–ª–∞, —Å–∏–º–≤–æ–ª—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
2. –°–æ—Ö—Ä–∞–Ω—è–π –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é –∏ —Ñ–æ—Ä–º–∞—Ç
3. –£–∫–∞–∑—ã–≤–∞–π –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –≤ –ø–∏–∫—Å–µ–ª—è—Ö
4. –û—Ü–µ–Ω–∏–≤–∞–π confidence –æ—Ç 0.0 –¥–æ 1.0
5. –û–ø—Ä–µ–¥–µ–ª—è–π —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (ru, en, mix)
6. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
7. –ù–ï –ø–µ—Ä–µ–≤–æ–¥–∏ –∏ –ù–ï –∏—Å–ø—Ä–∞–≤–ª—è–π —Ç–µ–∫—Å—Ç
8. –î–ª—è –ø–ª–æ—Ö–æ –≤–∏–¥–∏–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —É–∫–∞–∑—ã–≤–∞–π –Ω–∏–∑–∫–∏–π confidence

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
"""
    
    def _validate_ocr_annotation(self, ocr_data: Dict) -> Optional[Dict]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç OCR –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é"""
        try:
            if not isinstance(ocr_data, dict):
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            if "text_blocks" not in ocr_data or "full_text" not in ocr_data:
                return None
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –±–ª–æ–∫–∏ –ø–æ confidence
            filtered_blocks = []
            for block in ocr_data.get("text_blocks", []):
                if (isinstance(block, dict) and 
                    "confidence" in block and 
                    block["confidence"] >= self.config.min_word_confidence):
                    filtered_blocks.append(block)
            
            ocr_data["text_blocks"] = filtered_blocks
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
            overall_confidence = ocr_data.get("quality_metrics", {}).get("overall_confidence", 0.0)
            if overall_confidence < self.config.llm_confidence_threshold:
                self.logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∏–π –æ–±—â–∏–π confidence: {overall_confidence}")
            
            return ocr_data
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ OCR: {e}")
            return None
    
    def _extract_text_fallback(self, response_text: str) -> Optional[Dict]:
        """Fallback –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ JSON –Ω–µ –ø–æ–ª—É—á–∏–ª—Å—è
            return {
                "text_blocks": [
                    {
                        "text": response_text,
                        "confidence": 0.5,
                        "bbox": [0, 0, 100, 100],
                        "language": "mixed",
                        "font_size": "medium",
                        "text_type": "paragraph"
                    }
                ],
                "full_text": response_text,
                "document_type": "other",
                "layout_structure": {"columns": 1},
                "quality_metrics": {"overall_confidence": 0.5}
            }
        except:
            return None
    
    def enhance_existing_annotation(self, annotation: Dict, image_path: str) -> Dict:
        """–£–ª—É—á—à–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.client:
            return annotation
        
        try:
            self._rate_limit_check()
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
            prompt = f"""
–£–ª—É—á—à–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é OCR –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é. –ò—Å—Ö–æ–¥–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è:
{json.dumps(annotation, ensure_ascii=False, indent=2)}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏:
1. –ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ
2. –î–æ–±–∞–≤—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
3. –£–ª—É—á—à–∏ bbox –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
4. –ü–æ–≤—ã—Å—å —Ç–æ—á–Ω–æ—Å—Ç—å confidence
5. –î–æ–ø–æ–ª–Ω–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ

–í–µ—Ä–Ω–∏ —É–ª—É—á—à–µ–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –≤ —Ç–æ–º –∂–µ JSON —Ñ–æ—Ä–º–∞—Ç–µ.
"""
            
            image = Image.open(image_path).convert('RGB')
            response = self.client.generate_content([prompt, image])
            
            if response and response.text:
                try:
                    enhanced = json.loads(response.text)
                    return enhanced if enhanced else annotation
                except:
                    return annotation
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {e}")
        
        return annotation


class SyntheticDataGenerator:
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TrOCR"""
    
    def __init__(self, config: EnhancedTrOCRConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        
        # –®–∞–±–ª–æ–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.text_templates = {
            "invoice": [
                "–û–û–û ¬´{company}¬ª", "–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞ ‚Ññ{number}", 
                "–æ—Ç {date}", "–°—É–º–º–∞: {amount} —Ä—É–±.",
                "–ù–î–° 20%: {tax} —Ä—É–±.", "–ò—Ç–æ–≥–æ: {total} —Ä—É–±."
            ],
            "receipt": [
                "–ß–µ–∫ ‚Ññ{number}", "–î–∞—Ç–∞: {date}",
                "–¢–æ–≤–∞—Ä: {item}", "–¶–µ–Ω–∞: {price}",
                "–ò—Ç–æ–≥–æ: {total}"
            ],
            "document": [
                "–î–æ–∫—É–º–µ–Ω—Ç ‚Ññ{number}", "–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è: {date}",
                "–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞", "–ü–æ–¥–ø–∏—Å—å: {signature}"
            ]
        }
        
        # –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏
        self.sample_data = {
            "companies": ["–û–û–û –†–æ–≥–∞ –∏ –∫–æ–ø—ã—Ç–∞", "–ò–ü –ò–≤–∞–Ω–æ–≤ –ò.–ò.", "–ó–ê–û –°–≤–µ—Ç–ª–æ–µ –±—É–¥—É—â–µ–µ"],
            "items": ["–¢–æ–≤–∞—Ä 1", "–£—Å–ª—É–≥–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏", "–û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ"],
            "names": ["–ò–≤–∞–Ω–æ–≤", "–ü–µ—Ç—Ä–æ–≤", "–°–∏–¥–æ—Ä–æ–≤"],
            "numbers": lambda: str(random.randint(1, 9999)),
            "amounts": lambda: f"{random.randint(100, 50000)},00"
        }
    
    def generate_synthetic_dataset(self, 
                                 output_dir: Path, 
                                 num_samples: int = 1000,
                                 progress_callback: Optional[Callable] = None) -> List[Tuple[str, Dict]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        
        Args:
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
            progress_callback: Callback –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–∞—Ä (–ø—É—Ç—å_–∫_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è)
        """
        self.logger.info(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_samples} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        synthetic_pairs = []
        images_dir = output_dir / "synthetic_images"
        images_dir.mkdir(parents=True, exist_ok=True)
        
        for i in tqdm(range(num_samples), desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"):
            try:
                # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                doc_type = random.choice(self.config.synthetic_templates)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
                text_content = self._generate_text_for_type(doc_type)
                
                # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_path = images_dir / f"synthetic_{i+1:06d}.png"
                image = self._create_synthetic_image(text_content, doc_type)
                image.save(image_path, "PNG", quality=95)
                
                # –°–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                annotation = self._create_synthetic_annotation(text_content, image.size)
                
                synthetic_pairs.append((str(image_path), annotation))
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                if progress_callback:
                    progress_callback(int((i + 1) / num_samples * 100))
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–º–µ—Ä–∞ {i}: {e}")
                continue
        
        self.logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(synthetic_pairs)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        return synthetic_pairs
    
    def _generate_text_for_type(self, doc_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        templates = self.text_templates.get(doc_type, self.text_templates["document"])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º —à–∞–±–ª–æ–Ω—ã —Å–ª—É—á–∞–π–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        filled_templates = []
        for template in templates:
            try:
                filled = template.format(
                    company=random.choice(self.sample_data["companies"]),
                    number=self.sample_data["numbers"](),
                    date=f"{random.randint(1, 28):02d}.{random.randint(1, 12):02d}.2024",
                    amount=self.sample_data["amounts"](),
                    tax=self.sample_data["amounts"](),
                    total=self.sample_data["amounts"](),
                    item=random.choice(self.sample_data["items"]),
                    price=self.sample_data["amounts"](),
                    signature=random.choice(self.sample_data["names"])
                )
                filled_templates.append(filled)
            except KeyError:
                # –ï—Å–ª–∏ —à–∞–±–ª–æ–Ω –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏
                filled_templates.append(template)
        
        return "\n".join(filled_templates)
    
    def _create_synthetic_image(self, text: str, doc_type: str) -> Image.Image:
        """–°–æ–∑–¥–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ç–µ–∫—Å—Ç–æ–º"""
        # –†–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        width, height = self.config.image_size
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ü–≤–µ—Ç —Ñ–æ–Ω–∞
        bg_color = random.choice(["white", "#f8f9fa", "#f1f3f4", "#e8eaed"])
        
        # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.new("RGB", (width, height), bg_color)
        draw = ImageDraw.Draw(image)
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —à—Ä–∏—Ñ—Ç
            font_size = random.randint(12, 20)
            font = ImageFont.truetype("arial.ttf", font_size)
        except:
            # Fallback –Ω–∞ default —à—Ä–∏—Ñ—Ç
            font = ImageFont.load_default()
        
        # –¶–≤–µ—Ç —Ç–µ–∫—Å—Ç–∞
        text_color = random.choice(["black", "#333333", "#1a1a1a"])
        
        # –†–∞–∑–º–µ—â–∞–µ–º —Ç–µ–∫—Å—Ç
        lines = text.split('\n')
        y_offset = random.randint(20, 50)
        line_height = font_size + 5
        
        for line in lines:
            if line.strip():
                x_offset = random.randint(20, 60)
                draw.text((x_offset, y_offset), line, fill=text_color, font=font)
                y_offset += line_height
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —à—É–º–∞ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        if random.random() < 0.3:
            image = self._add_image_noise(image)
        
        return image
    
    def _add_image_noise(self, image: Image.Image) -> Image.Image:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ª–µ–≥–∫–∏–π —à—É–º –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é"""
        try:
            # –ù–µ–±–æ–ª—å—à–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ
            if random.random() < 0.5:
                from PIL import ImageFilter
                image = image.filter(ImageFilter.GaussianBlur(radius=0.5))
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —è—Ä–∫–æ—Å—Ç–∏
            if random.random() < 0.3:
                enhancer = ImageEnhance.Brightness(image)
                image = enhancer.enhance(random.uniform(0.9, 1.1))
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞  
            if random.random() < 0.3:
                enhancer = ImageEnhance.Contrast(image)
                image = enhancer.enhance(random.uniform(0.95, 1.05))
        except:
            pass
        
        return image
    
    def _create_synthetic_annotation(self, text: str, image_size: Tuple[int, int]) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        width, height = image_size
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –±–ª–æ–∫–∏
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        
        text_blocks = []
        y_offset = 30
        line_height = 25
        
        for line in lines:
            # –°–æ–∑–¥–∞–µ–º bbox –¥–ª—è —Å—Ç—Ä–æ–∫–∏
            x1 = 30
            y1 = y_offset
            x2 = min(width - 30, x1 + len(line) * 8)  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —à–∏—Ä–∏–Ω–∞
            y2 = y1 + line_height
            
            text_blocks.append({
                "text": line,
                "confidence": random.uniform(0.85, 0.98),
                "bbox": [x1, y1, x2, y2],
                "language": "ru",
                "font_size": "medium",
                "text_type": "line"
            })
            
            y_offset += line_height
        
        return {
            "text_blocks": text_blocks,
            "full_text": text,
            "document_type": "synthetic",
            "layout_structure": {
                "has_table": False,
                "has_header": True,
                "has_footer": False,
                "columns": 1
            },
            "quality_metrics": {
                "image_quality": "high",
                "text_clarity": "clear",
                "overall_confidence": 0.95
            },
            "is_synthetic": True
        }


class EnhancedTrOCRDatasetPreparator(TrOCRDatasetPreparator):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥–≥–æ—Ç–æ–≤—â–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ TrOCR —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π LLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
    """
    
    def __init__(self, config: Optional[EnhancedTrOCRConfig] = None):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        base_config = config or EnhancedTrOCRConfig()
        super().__init__(base_config)
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.enhanced_config = base_config
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º LLM –¥–≤–∏–∂–æ–∫
        self.llm_engine = None
        if self.enhanced_config.enable_llm_annotation:
            self.llm_engine = LLMAnnotationEngine(self.enhanced_config, self.logger)
        
        # –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        self.synthetic_generator = SyntheticDataGenerator(self.enhanced_config, self.logger)
        
        self.logger.info("üöÄ Enhanced TrOCR Dataset Preparator –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def prepare_fully_automated_dataset(self,
                                      source_images: List[str],
                                      output_path: str,
                                      num_synthetic: int = 500,
                                      progress_callback: Optional[Callable] = None) -> Dict[str, str]:
        """
        –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            source_images: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞  
            num_synthetic: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            
        Returns:
            Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º
        """
        self.logger.info(f"ü§ñ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        self.logger.info(f"üìÅ –ò—Å—Ö–æ–¥–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(source_images)}")
        self.logger.info(f"üé® –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã: {num_synthetic}")
        
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        all_data_pairs = []
        total_steps = len(source_images) + num_synthetic
        current_step = 0
        
        # –≠—Ç–∞–ø 1: –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å LLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π
        if source_images:
            self.logger.info("üìù –≠—Ç–∞–ø 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            for i, image_path in enumerate(source_images):
                try:
                    if progress_callback:
                        progress = int(current_step / total_steps * 100)
                        progress_callback(progress)
                    
                    # –°–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é —á–µ—Ä–µ–∑ LLM
                    annotation = self._create_automated_annotation(image_path)
                    
                    if annotation and self._validate_annotation_quality(annotation):
                        all_data_pairs.append((image_path, annotation))
                        self.logger.debug(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {Path(image_path).name}")
                    else:
                        self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ): {Path(image_path).name}")
                    
                    current_step += 1
                    
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
                    current_step += 1
                    continue
        
        # –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        if num_synthetic > 0:
            self.logger.info("üé® –≠—Ç–∞–ø 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
            
            def synthetic_progress(progress):
                nonlocal current_step
                synthetic_step = current_step + int(progress / 100 * num_synthetic)
                if progress_callback:
                    progress_callback(int(synthetic_step / total_steps * 100))
            
            synthetic_pairs = self.synthetic_generator.generate_synthetic_dataset(
                output_dir, num_synthetic, synthetic_progress
            )
            all_data_pairs.extend(synthetic_pairs)
            current_step = total_steps
        
        # –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.logger.info("üèóÔ∏è –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        
        if progress_callback:
            progress_callback(95)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        converted_pairs = self._convert_annotations_to_trocr_format(all_data_pairs)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        train_pairs, val_pairs, test_pairs = self._split_data(converted_pairs, 0.7, 0.15, 0.15)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        datasets = {}
        if train_pairs:
            datasets['train'] = self._create_dataset_split(
                train_pairs, output_dir / "train", is_training=True
            )
        if val_pairs:
            datasets['validation'] = self._create_dataset_split(
                val_pairs, output_dir / "validation", is_training=False
            )
        if test_pairs:
            datasets['test'] = self._create_dataset_split(
                test_pairs, output_dir / "test", is_training=False
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self._save_enhanced_metadata(output_dir, datasets, all_data_pairs)
        
        if progress_callback:
            progress_callback(100)
        
        self.logger.info(f"üéâ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤: {output_path}")
        self.logger.info(f"üìä –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(converted_pairs)}")
        self.logger.info(f"üèãÔ∏è –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö: {len(train_pairs)}")
        self.logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö: {len(val_pairs)}")
        self.logger.info(f"üß™ –¢–µ—Å—Ç–æ–≤—ã—Ö: {len(test_pairs)}")
        
        return datasets
    
    def _create_automated_annotation(self, image_path: str) -> Optional[Dict]:
        """–°–æ–∑–¥–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if not self.llm_engine:
            self.logger.warning("‚ö†Ô∏è LLM –¥–≤–∏–∂–æ–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback")
            return self._create_fallback_annotation(image_path)
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            annotation = self.llm_engine.annotate_image_for_ocr(image_path)
            
            if annotation:
                self.logger.debug(f"‚úÖ LLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ –¥–ª—è {Path(image_path).name}")
                return annotation
            else:
                self.logger.warning(f"‚ö†Ô∏è LLM –Ω–µ —Å–º–æ–≥ —Å–æ–∑–¥–∞—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è {Path(image_path).name}")
                return self._create_fallback_annotation(image_path)
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {image_path}: {e}")
            return self._create_fallback_annotation(image_path)
    
    def _create_fallback_annotation(self, image_path: str) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –±–µ–∑ LLM"""
        try:
            image = Image.open(image_path)
            width, height = image.size
            
            # –ë–∞–∑–æ–≤–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å placeholder —Ç–µ–∫—Å—Ç–æ–º
            return {
                "text_blocks": [
                    {
                        "text": f"Document content from {Path(image_path).name}",
                        "confidence": 0.5,
                        "bbox": [0, 0, width, height],
                        "language": "en",
                        "font_size": "medium",
                        "text_type": "document"
                    }
                ],
                "full_text": f"Document content from {Path(image_path).name}",
                "document_type": "document",
                "layout_structure": {"columns": 1},
                "quality_metrics": {"overall_confidence": 0.5},
                "is_fallback": True
            }
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è fallback –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {e}")
            return None
    
    def _validate_annotation_quality(self, annotation: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        if not annotation:
            return False
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–∞
            full_text = annotation.get("full_text", "")
            if len(full_text) < self.enhanced_config.min_text_length_chars:
                return False
            
            if len(full_text) > self.enhanced_config.max_text_length_chars:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π confidence
            overall_confidence = annotation.get("quality_metrics", {}).get("overall_confidence", 0.0)
            if overall_confidence < self.enhanced_config.llm_confidence_threshold:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤
            text_blocks = annotation.get("text_blocks", [])
            if len(text_blocks) == 0:
                return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {e}")
            return False
    
    def _convert_annotations_to_trocr_format(self, data_pairs: List[Tuple[str, Dict]]) -> List[Tuple[str, str]]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, —Ç–µ–∫—Å—Ç)"""
        converted_pairs = []
        
        for image_path, annotation in data_pairs:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
                full_text = annotation.get("full_text", "")
                
                if full_text.strip():
                    converted_pairs.append((image_path, full_text.strip()))
                    
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è {image_path}: {e}")
                continue
        
        return converted_pairs
    
    def _save_enhanced_metadata(self, output_dir: Path, datasets: Dict, all_pairs: List):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            super()._save_dataset_metadata(output_dir, datasets, 
                                         self._convert_annotations_to_trocr_format(all_pairs))
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            enhanced_metadata = {
                "dataset_type": "enhanced_trocr",
                "creation_method": "llm_automated",
                "llm_provider": self.enhanced_config.llm_provider,
                "llm_model": self.enhanced_config.llm_model,
                "config": {
                    "enable_llm_annotation": self.enhanced_config.enable_llm_annotation,
                    "llm_confidence_threshold": self.enhanced_config.llm_confidence_threshold,
                    "synthetic_templates": self.enhanced_config.synthetic_templates,
                    "image_size": self.enhanced_config.image_size
                },
                "quality_stats": self._calculate_quality_stats(all_pairs),
                "annotation_sources": self._analyze_annotation_sources(all_pairs)
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            enhanced_file = output_dir / "enhanced_metadata.json"
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy —Ç–∏–ø—ã –≤ native Python —Ç–∏–ø—ã –¥–ª—è JSON
            def convert_numpy_types(obj):
                import numpy as np
                if hasattr(obj, 'item'):  # numpy —Å–∫–∞–ª—è—Ä—ã
                    return obj.item()
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, dict):
                    return {k: convert_numpy_types(v) for k, v in obj.items()}
                elif isinstance(obj, list):
                    return [convert_numpy_types(item) for item in obj]
                return obj
            
            enhanced_metadata = convert_numpy_types(enhanced_metadata)
            
            with open(enhanced_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_metadata, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üíæ –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {enhanced_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _calculate_quality_stats(self, data_pairs: List[Tuple[str, Dict]]) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        try:
            confidences = []
            text_lengths = []
            languages = []
            
            for _, annotation in data_pairs:
                # Confidence
                overall_conf = annotation.get("quality_metrics", {}).get("overall_confidence", 0.0)
                confidences.append(overall_conf)
                
                # –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
                text_len = len(annotation.get("full_text", ""))
                text_lengths.append(text_len)
                
                # –Ø–∑—ã–∫–∏
                for block in annotation.get("text_blocks", []):
                    lang = block.get("language", "unknown")
                    languages.append(lang)
            
            return {
                "avg_confidence": np.mean(confidences) if confidences else 0.0,
                "min_confidence": np.min(confidences) if confidences else 0.0,
                "max_confidence": np.max(confidences) if confidences else 0.0,
                "avg_text_length": np.mean(text_lengths) if text_lengths else 0.0,
                "min_text_length": np.min(text_lengths) if text_lengths else 0.0,
                "max_text_length": np.max(text_lengths) if text_lengths else 0.0,
                "language_distribution": {lang: languages.count(lang) for lang in set(languages)}
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return {}
    
    def _analyze_annotation_sources(self, data_pairs: List[Tuple[str, Dict]]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
        try:
            sources = {
                "llm_generated": 0,
                "synthetic": 0,
                "fallback": 0,
                "manual": 0
            }
            
            for _, annotation in data_pairs:
                if annotation.get("is_synthetic", False):
                    sources["synthetic"] += 1
                elif annotation.get("is_fallback", False):
                    sources["fallback"] += 1
                elif "llm" in str(annotation).lower():
                    sources["llm_generated"] += 1
                else:
                    sources["manual"] += 1
            
            return sources
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
            return {}


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def create_automated_trocr_dataset(source_images: List[str],
                                 output_path: str,
                                 num_synthetic: int = 500,
                                 config: Optional[EnhancedTrOCRConfig] = None,
                                 progress_callback: Optional[Callable] = None) -> Dict[str, str]:
    """
    –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π TrOCR –¥–∞—Ç–∞—Å–µ—Ç
    
    Args:
        source_images: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        num_synthetic: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        progress_callback: Callback –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        
    Returns:
        Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    """
    preparator = EnhancedTrOCRDatasetPreparator(config)
    return preparator.prepare_fully_automated_dataset(
        source_images, output_path, num_synthetic, progress_callback
    )


def create_llm_annotated_dataset_from_folder(images_folder: str,
                                           output_path: str,
                                           config: Optional[EnhancedTrOCRConfig] = None) -> Dict[str, str]:
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å LLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –∏–∑ –ø–∞–ø–∫–∏ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    
    Args:
        images_folder: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        
    Returns:
        Dict —Å –ø—É—Ç—è–º–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    """
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–∞–ø–∫–∏
    images_folder = Path(images_folder)
    supported_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif', '.pdf']
    
    source_images = []
    for ext in supported_formats:
        source_images.extend(list(images_folder.glob(f"*{ext}")))
        source_images.extend(list(images_folder.glob(f"**/*{ext}")))
    
    source_images = [str(img) for img in source_images]
    
    return create_automated_trocr_dataset(
        source_images=source_images,
        output_path=output_path,
        num_synthetic=0,  # –¢–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        config=config
    )


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO)
    
    config = EnhancedTrOCRConfig(
        enable_llm_annotation=True,
        llm_model="models/gemini-2.0-flash-exp",
        max_llm_requests_per_minute=30
    )
    
    # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    source_images = ["path/to/image1.jpg", "path/to/image2.png"]
    
    datasets = create_automated_trocr_dataset(
        source_images=source_images,
        output_path="data/automated_trocr_dataset",
        num_synthetic=100,
        config=config
    )
    
    print("‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π TrOCR –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω!")
    print(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç—ã: {datasets}") 