"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π TrainingDataPreparator —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç IntelligentDataExtractor —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–æ–º
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional
from pathlib import Path
import tempfile
import shutil

from .intelligent_data_extractor import IntelligentDataExtractor, ExtractedField
from .data_preparator import TrainingDataPreparator


class EnhancedDataPreparator:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–¥–≥–æ—Ç–æ–≤—â–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º
    """
    
    def __init__(self, ocr_processor, gemini_processor, logger=None):
        self.ocr_processor = ocr_processor
        self.gemini_processor = gemini_processor
        self.logger = logger or logging.getLogger(__name__)
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä
        self.intelligent_extractor = IntelligentDataExtractor(
            gemini_processor=gemini_processor,
            logger=self.logger
        )
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ã—á–Ω—ã–π TrainingDataPreparator –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        self.data_preparator = TrainingDataPreparator(
            ocr_processor=ocr_processor,
            gemini_processor=gemini_processor,
            logger=self.logger
        )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_stats = {
            'total_files': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'total_fields_extracted': 0,
            'unique_field_types': set(),
            'document_types': {}
        }
    
    def prepare_enhanced_dataset(self, 
                                source_folder: str, 
                                output_path: str, 
                                dataset_type: str = "LayoutLM",
                                max_files: int = None,
                                progress_callback=None,
                                log_callback=None) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        
        Args:
            source_folder: –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            dataset_type: –¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞ (LayoutLM/Donut)
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—á–µ—Ç–∞ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            log_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ–∑–¥–∞–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        """
        try:
            self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            self._reset_stats()
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
            files = self._get_files_to_process(source_folder, max_files)
            if not files:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            self.processing_stats['total_files'] = len(files)
            self._log_message(log_callback, f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
            
            # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –ø–∞–ø–∫—É
            os.makedirs(output_path, exist_ok=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
            enhanced_annotations = []
            
            for i, file_path in enumerate(files):
                try:
                    self._log_message(log_callback, f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {i+1}/{len(files)}: {os.path.basename(file_path)}")
                    
                    # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
                    annotation = self._process_file_enhanced(file_path, output_path)
                    
                    if annotation:
                        enhanced_annotations.append(annotation)
                        self.processing_stats['successful_extractions'] += 1
                        self._log_message(log_callback, f"‚úÖ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                    else:
                        self.processing_stats['failed_extractions'] += 1
                        self._log_message(log_callback, f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                    if progress_callback:
                        progress = int((i + 1) / len(files) * 100)
                        progress_callback(progress)
                        
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    self.processing_stats['failed_extractions'] += 1
                    continue
            
            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            if enhanced_annotations:
                dataset_path = self._create_final_dataset(
                    enhanced_annotations, 
                    output_path, 
                    dataset_type
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self._save_processing_stats(output_path)
                
                self._log_message(log_callback, f"üéâ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ: {dataset_path}")
                self._log_processing_summary(log_callback)
                
                return dataset_path
            else:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                
        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            raise
    
    def _process_file_enhanced(self, file_path: str, output_path: str) -> Optional[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            image_paths = self._convert_to_images(file_path, output_path)
            
            annotations = []
            for image_path in image_paths:
                annotation = self._process_single_image_enhanced(image_path)
                if annotation:
                    annotations.append(annotation)
            
            return annotations[0] if annotations else None
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None
    
    def _process_single_image_enhanced(self, image_path: str) -> Optional[Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º"""
        try:
            self.logger.info(f"üîç –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {image_path}")
            
            # 1. OCR –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª–æ–≤ –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            ocr_result = self.ocr_processor.process_image(image_path)
            if not ocr_result or 'words' not in ocr_result:
                self.logger.error("‚ùå OCR –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                return None
            
            # 2. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            extracted_data = self.intelligent_extractor.extract_all_data(image_path)
            
            if not extracted_data or not extracted_data.get('fields'):
                self.logger.warning("‚ö†Ô∏è –ù–µ –∏–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                return self._create_basic_annotation(ocr_result, image_path)
            
            # 3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è
            training_data = self.intelligent_extractor.convert_to_training_format(
                extracted_data, 
                ocr_result['words']
            )
            
            # 4. –°–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
            annotation = self._create_enhanced_annotation(
                training_data, 
                extracted_data, 
                image_path
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_stats(extracted_data)
            
            return annotation
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
            return None
    
    def _create_enhanced_annotation(self, training_data: Dict, extracted_data: Dict, image_path: str) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é"""
        try:
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–∫
            labels = training_data['labels']
            label_stats = {}
            for label in labels:
                label_stats[label] = label_stats.get(label, 0) + 1
            
            # –°–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
            annotation = {
                'image_path': image_path,
                'words': training_data['words'],
                'bboxes': training_data['bboxes'],
                'labels': labels,
                'document_type': extracted_data.get('document_type', 'unknown'),
                'extracted_fields_count': extracted_data.get('total_fields', 0),
                'field_mappings': training_data.get('field_mappings', {}),
                'label_statistics': label_stats,
                'categories': extracted_data.get('categories', {}),
                'extraction_method': 'intelligent',
                'quality_score': self._calculate_quality_score(extracted_data, labels)
            }
            
            self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ —É–ª—É—á—à–µ–Ω–Ω–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è:")
            self.logger.info(f"   üìù –°–ª–æ–≤: {len(training_data['words'])}")
            self.logger.info(f"   üè∑Ô∏è –ü–æ–ª–µ–∑–Ω—ã—Ö –º–µ—Ç–æ–∫: {len([l for l in labels if l != 'O'])}")
            self.logger.info(f"   üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {extracted_data.get('total_fields', 0)}")
            
            return annotation
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {e}")
            return None
    
    def _create_basic_annotation(self, ocr_result: Dict, image_path: str) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –µ—Å–ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ"""
        words = ocr_result['words']
        return {
            'image_path': image_path,
            'words': [w['text'] for w in words],
            'bboxes': [w['bbox'] for w in words],
            'labels': ['O'] * len(words),
            'document_type': 'unknown',
            'extracted_fields_count': 0,
            'field_mappings': {},
            'label_statistics': {'O': len(words)},
            'categories': {},
            'extraction_method': 'basic',
            'quality_score': 0.1
        }
    
    def _calculate_quality_score(self, extracted_data: Dict, labels: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        try:
            total_labels = len(labels)
            useful_labels = len([l for l in labels if l != 'O'])
            fields_count = extracted_data.get('total_fields', 0)
            
            if total_labels == 0:
                return 0.0
            
            # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –ø–æ–ª–µ–∑–Ω—ã—Ö –º–µ—Ç–æ–∫
            label_ratio = useful_labels / total_labels
            
            # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
            field_bonus = min(fields_count / 20, 0.3)  # –ú–∞–∫—Å–∏–º—É–º 30% –±–æ–Ω—É—Å–∞
            
            # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            categories_count = len(extracted_data.get('categories', {}))
            category_bonus = min(categories_count / 10, 0.2)  # –ú–∞–∫—Å–∏–º—É–º 20% –±–æ–Ω—É—Å–∞
            
            quality_score = min(label_ratio + field_bonus + category_bonus, 1.0)
            
            return round(quality_score, 3)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return 0.0
    
    def _convert_to_images(self, file_path: str, output_path: str) -> List[str]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            if file_path.lower().endswith(('.png', '.jpg', '.jpeg')):
                return [file_path]
            elif file_path.lower().endswith('.pdf'):
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∏–∑ DataPreparator
                return self.data_preparator._convert_pdf_to_images(file_path, output_path)
            else:
                self.logger.warning(f"‚ö†Ô∏è –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}")
                return []
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return []
    
    def _get_files_to_process(self, source_folder: str, max_files: int = None) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            supported_extensions = ['.pdf', '.png', '.jpg', '.jpeg']
            files = []
            
            for root, dirs, filenames in os.walk(source_folder):
                for filename in filenames:
                    if any(filename.lower().endswith(ext) for ext in supported_extensions):
                        files.append(os.path.join(root, filename))
            
            if max_files:
                files = files[:max_files]
            
            return sorted(files)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤: {e}")
            return []
    
    def _create_final_dataset(self, annotations: List[Dict], output_path: str, dataset_type: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç"""
        try:
            self.logger.info("üì¶ –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            
            if dataset_type == "LayoutLM":
                return self._create_layoutlm_dataset(annotations, output_path)
            elif dataset_type == "Donut":
                return self._create_donut_dataset(annotations, output_path)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_type}")
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            raise
    
    def _create_layoutlm_dataset(self, annotations: List[Dict], output_path: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è LayoutLM"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥ –∏–∑ DataPreparator
            # –Ω–æ —Å –Ω–∞—à–∏–º–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç DataPreparator
            converted_data = []
            for annotation in annotations:
                converted_data.append({
                    'words': annotation['words'],
                    'bboxes': annotation['bboxes'],
                    'labels': annotation['labels'],
                    'image_path': annotation['image_path']
                })
            
            # –°–æ–∑–¥–∞–µ–º HuggingFace –¥–∞—Ç–∞—Å–µ—Ç
            dataset_path = os.path.join(output_path, "dataset_dict")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –∏–∑ DataPreparator –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è HF –¥–∞—Ç–∞—Å–µ—Ç–∞
            self.data_preparator.layoutlm_data = converted_data
            hf_dataset_path = self.data_preparator._create_huggingface_dataset_for_layoutlm(output_path)
            
            return hf_dataset_path
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è LayoutLM –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            raise
    
    def _create_donut_dataset(self, annotations: List[Dict], output_path: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è Donut"""
        try:
            # –î–ª—è Donut —Å–æ–∑–¥–∞–µ–º JSON –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            donut_annotations = []
            
            for annotation in annotations:
                # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è Donut
                donut_annotation = {
                    'image': os.path.basename(annotation['image_path']),
                    'ground_truth': self._create_donut_ground_truth(annotation)
                }
                donut_annotations.append(donut_annotation)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            annotations_file = os.path.join(output_path, "annotations.json")
            with open(annotations_file, 'w', encoding='utf-8') as f:
                json.dump(donut_annotations, f, ensure_ascii=False, indent=2)
            
            return output_path
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Donut –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            raise
    
    def _create_donut_ground_truth(self, annotation: Dict) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç ground truth –¥–ª—è Donut –∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—è –∏–∑ field_mappings
            fields = {}
            
            for field_name, indices in annotation.get('field_mappings', {}).items():
                if indices:
                    # –°–æ–±–∏—Ä–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ —Å–ª–æ–≤
                    words = annotation['words']
                    field_value = ' '.join([words[i] for i in indices if i < len(words)])
                    fields[field_name.lower()] = field_value
            
            return fields
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è ground truth: {e}")
            return {}
    
    def _update_stats(self, extracted_data: Dict):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            fields = extracted_data.get('fields', [])
            self.processing_stats['total_fields_extracted'] += len(fields)
            
            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã –ø–æ–ª–µ–π
            for field in fields:
                if hasattr(field, 'field_type'):
                    self.processing_stats['unique_field_types'].add(field.field_type)
            
            # –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            doc_type = extracted_data.get('document_type', 'unknown')
            self.processing_stats['document_types'][doc_type] = \
                self.processing_stats['document_types'].get(doc_type, 0) + 1
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
    
    def _save_processing_stats(self, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º set –≤ list –¥–ª—è JSON
            stats = self.processing_stats.copy()
            stats['unique_field_types'] = list(stats['unique_field_types'])
            
            stats_file = os.path.join(output_path, "processing_stats.json")
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
                
            self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {stats_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
    
    def _reset_stats(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        self.processing_stats = {
            'total_files': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'total_fields_extracted': 0,
            'unique_field_types': set(),
            'document_types': {}
        }
    
    def _log_message(self, log_callback, message: str):
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –ª–æ–≥"""
        self.logger.info(message)
        if log_callback:
            log_callback(message)
    
    def _log_processing_summary(self, log_callback):
        """–õ–æ–≥–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å–≤–æ–¥–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            stats = self.processing_stats
            
            self._log_message(log_callback, "üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            self._log_message(log_callback, f"   üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {stats['total_files']}")
            self._log_message(log_callback, f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {stats['successful_extractions']}")
            self._log_message(log_callback, f"   ‚ùå –û—à–∏–±–æ–∫: {stats['failed_extractions']}")
            self._log_message(log_callback, f"   üìù –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {stats['total_fields_extracted']}")
            self._log_message(log_callback, f"   üè∑Ô∏è –¢–∏–ø–æ–≤ –ø–æ–ª–µ–π: {len(stats['unique_field_types'])}")
            
            # –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if stats['document_types']:
                self._log_message(log_callback, "   üìÑ –¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
                for doc_type, count in stats['document_types'].items():
                    self._log_message(log_callback, f"      ‚Ä¢ {doc_type}: {count}")
                    
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–≤–æ–¥–∫–∏: {e}") 