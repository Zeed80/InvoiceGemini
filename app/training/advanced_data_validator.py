"""
–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TrOCR –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
–í–∫–ª—é—á–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—á–∏—Å—Ç–∫—É
"""

import os
import cv2
import numpy as np
import hashlib
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, asdict
from PIL import Image, ImageStat
import logging
from collections import defaultdict, Counter
import re
import difflib
from datetime import datetime

# –î–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
import imagehash
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
try:
    from skimage import measure
    from skimage.filters import laplace
    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

# –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from spellchecker import SpellChecker
    SPELLCHECK_AVAILABLE = True
except ImportError:
    SPELLCHECK_AVAILABLE = False


@dataclass
class ValidationIssue:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    type: str  # 'duplicate', 'low_quality', 'text_error', 'format_error'
    severity: str  # 'critical', 'warning', 'info'
    item_path: str
    description: str
    suggestion: str = ""
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class ImageQualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    width: int
    height: int
    channels: int
    file_size: int
    brightness: float
    contrast: float
    sharpness: float
    noise_level: float
    aspect_ratio: float
    
    @property
    def resolution_score(self) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (0-1)"""
        min_resolution = 224 * 224
        max_resolution = 2048 * 2048
        current_resolution = self.width * self.height
        
        if current_resolution < min_resolution:
            return 0.0
        elif current_resolution > max_resolution:
            return 1.0
        else:
            return (current_resolution - min_resolution) / (max_resolution - min_resolution)
    
    @property
    def overall_score(self) -> float:
        """–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (0-1)"""
        scores = [
            self.resolution_score,
            min(self.brightness / 128.0, 1.0),  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —è—Ä–∫–æ—Å—Ç—å ~128
            min(self.contrast / 50.0, 1.0),     # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç ~50
            min(self.sharpness / 100.0, 1.0),   # –í—ã—Å–æ–∫–∞—è —Ä–µ–∑–∫–æ—Å—Ç—å
            max(0.0, 1.0 - self.noise_level / 50.0)  # –ù–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
        ]
        return sum(scores) / len(scores)


@dataclass
class TextQualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
    length: int
    word_count: int
    char_diversity: float
    language_confidence: float
    spelling_errors: int
    special_chars_ratio: float
    digit_ratio: float
    uppercase_ratio: float
    
    @property
    def readability_score(self) -> float:
        """–û—Ü–µ–Ω–∫–∞ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ (0-1)"""
        if self.length == 0:
            return 0.0
            
        # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è TrOCR: 10-100 —Å–∏–º–≤–æ–ª–æ–≤
        length_score = 1.0 if 10 <= self.length <= 100 else max(0.1, 1.0 - abs(self.length - 55) / 100)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
        diversity_score = min(self.char_diversity / 0.8, 1.0)
        
        # –û—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è
        spelling_score = max(0.0, 1.0 - self.spelling_errors / max(1, self.word_count))
        
        # –ë–∞–ª–∞–Ω—Å —Ç–∏–ø–æ–≤ —Å–∏–º–≤–æ–ª–æ–≤
        balance_score = 1.0 - abs(self.special_chars_ratio - 0.1) - abs(self.uppercase_ratio - 0.1)
        balance_score = max(0.0, balance_score)
        
        return (length_score + diversity_score + spelling_score + balance_score) / 4


@dataclass
class DuplicateGroup:
    """–ì—Ä—É–ø–ø–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
    type: str  # 'exact', 'near_duplicate', 'text_duplicate'
    similarity: float
    items: List[str]
    primary_item: str
    
    @property
    def duplicates_to_remove(self) -> List[str]:
        """–≠–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (–≤—Å–µ –∫—Ä–æ–º–µ –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ)"""
        return [item for item in self.items if item != self.primary_item]


class AdvancedDataValidator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, logger: Optional[logging.Logger] = None):
        self.logger = logger or logging.getLogger(__name__)
        self.image_hashes = {}
        self.text_vectors = None
        self.vectorizer = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
        self.spell_checker = None
        if SPELLCHECK_AVAILABLE:
            try:
                self.spell_checker = SpellChecker(language='ru')
                # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫
                self.spell_checker_en = SpellChecker(language='en')
            except:
                self.spell_checker = None
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        self.min_image_size = (64, 64)
        self.max_image_size = (4096, 4096)
        self.min_text_length = 1
        self.max_text_length = 500
        self.similarity_threshold = 0.95
        self.quality_threshold = 0.3
        
    def validate_dataset(self, dataset_path: str, 
                        check_duplicates: bool = True,
                        check_quality: bool = True,
                        check_text: bool = True) -> Dict:
        """–ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        
        self.logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_path}")
        
        validation_results = {
            'dataset_path': dataset_path,
            'timestamp': datetime.now().isoformat(),
            'total_items': 0,
            'valid_items': 0,
            'issues': [],
            'duplicates': [],
            'quality_stats': {},
            'recommendations': []
        }
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            annotations = self._load_annotations(dataset_path)
            validation_results['total_items'] = len(annotations)
            
            if not annotations:
                validation_results['issues'].append(ValidationIssue(
                    type='format_error',
                    severity='critical',
                    item_path=dataset_path,
                    description='–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ',
                    suggestion='–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π'
                ))
                return validation_results
            
            # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            if check_duplicates:
                self.logger.info("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
                duplicates = self._find_duplicates(annotations)
                validation_results['duplicates'] = [asdict(dup) for dup in duplicates]
                
                # –î–æ–±–∞–≤–ª—è–µ–º issues –¥–ª—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                for dup_group in duplicates:
                    for item in dup_group.duplicates_to_remove:
                        validation_results['issues'].append(ValidationIssue(
                            type='duplicate',
                            severity='warning',
                            item_path=item,
                            description=f'–î—É–±–ª–∏–∫–∞—Ç ({dup_group.type}, similarity: {dup_group.similarity:.3f})',
                            suggestion=f'–£–¥–∞–ª–∏—Ç—å, –æ—Å—Ç–∞–≤–∏—Ç—å: {dup_group.primary_item}',
                            metadata={'duplicate_group': dup_group.type, 'similarity': dup_group.similarity}
                        ))
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            if check_quality:
                self.logger.info("üñºÔ∏è –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
                image_quality_stats = self._analyze_image_quality(annotations)
                validation_results['quality_stats']['images'] = image_quality_stats
            
            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞
            if check_text:
                self.logger.info("üìù –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞...")
                text_quality_stats = self._analyze_text_quality(annotations)
                validation_results['quality_stats']['text'] = text_quality_stats
            
            # 4. –ü–æ–¥—Å—á–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            validation_results['valid_items'] = self._count_valid_items(validation_results)
            
            # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            validation_results['recommendations'] = self._generate_recommendations(validation_results)
            
            self.logger.info(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ù–∞–π–¥–µ–Ω–æ {len(validation_results['issues'])} –ø—Ä–æ–±–ª–µ–º")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            validation_results['issues'].append(ValidationIssue(
                type='format_error',
                severity='critical',
                item_path=dataset_path,
                description=f'–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}',
                suggestion='–ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º'
            ))
        
        return validation_results
    
    def clean_dataset(self, validation_results, remove_duplicates=True, remove_low_quality=True, quality_threshold=0.3):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        try:
            dataset_path = validation_results['dataset_path']
            issues = validation_results['issues']
            duplicates = validation_results['duplicates']
            
            print(f"üßπ –ù–∞—á–∏–Ω–∞–µ–º –æ—á–∏—Å—Ç–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_path}")
            
            # –°–æ–±–∏—Ä–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            items_to_remove = set()
            removal_stats = {
                'duplicates_removed': 0,
                'low_quality_removed': 0,
                'total_removed': 0,
                'total_kept': 0,
                'removal_percentage': 0
            }
            
            # 1. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            if remove_duplicates:
                for duplicate_group in duplicates:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –≥—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                    for item_path in duplicate_group[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç
                        items_to_remove.add(item_path)
                        removal_stats['duplicates_removed'] += 1
                        
                print(f"üìã –û—Ç–º–µ—á–µ–Ω–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removal_stats['duplicates_removed']}")
            
            # 2. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            if remove_low_quality:
                for issue in issues:
                    if issue['type'] in ['low_image_quality', 'low_text_quality']:
                        if issue.get('severity', 'medium') == 'critical' or issue.get('quality_score', 1.0) < quality_threshold:
                            item_path = issue.get('file_path', issue.get('item_path'))
                            if item_path:
                                items_to_remove.add(item_path)
                                removal_stats['low_quality_removed'] += 1
                                
                print(f"üìã –û—Ç–º–µ—á–µ–Ω–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö: {removal_stats['low_quality_removed']}")
            
            # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            removal_stats['total_removed'] = len(items_to_remove)
            
            if items_to_remove:
                if os.path.exists(os.path.join(dataset_path, 'metadata.json')):
                    # TrOCR –¥–∞—Ç–∞—Å–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                    self._clean_trocr_dataset(dataset_path, items_to_remove)
                elif os.path.exists(os.path.join(dataset_path, 'dataset_dict')):
                    # Datasets —Ñ–æ—Ä–º–∞—Ç
                    self._clean_datasets_format(dataset_path, items_to_remove)
                else:
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –ø–∞–ø–æ—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
                    self._clean_folder_structure(dataset_path, items_to_remove)
                    
                print(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {removal_stats['total_removed']}")
            else:
                print("‚ÑπÔ∏è –ù–µ—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            
            # 4. –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            remaining_count = self._count_remaining_items(dataset_path)
            removal_stats['total_kept'] = remaining_count
            
            total_original = removal_stats['total_removed'] + remaining_count
            if total_original > 0:
                removal_stats['removal_percentage'] = (removal_stats['total_removed'] / total_original) * 100
            
            # 5. –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self._update_metadata_after_cleanup(dataset_path, removal_stats)
            
            return {
                'success': True,
                'cleanup_stats': removal_stats,
                'dataset_path': dataset_path
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'cleanup_stats': {}
            }
    
    def _clean_trocr_dataset(self, dataset_path, items_to_remove):
        """–û—á–∏—Å—Ç–∫–∞ TrOCR –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        try:
            metadata_file = os.path.join(dataset_path, 'metadata.json')
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            original_count = len(metadata.get('annotations', []))
            cleaned_annotations = []
            
            for annotation in metadata.get('annotations', []):
                image_path = annotation.get('image_path', '')
                full_image_path = os.path.join(dataset_path, image_path)
                
                if full_image_path not in items_to_remove:
                    cleaned_annotations.append(annotation)
                else:
                    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    if os.path.exists(full_image_path):
                        os.remove(full_image_path)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata['annotations'] = cleaned_annotations
            metadata['total_samples'] = len(cleaned_annotations)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
                
            print(f"üìã TrOCR –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {original_count} ‚Üí {len(cleaned_annotations)}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ TrOCR –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    
    def _clean_datasets_format(self, dataset_path, items_to_remove):
        """–û—á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ datasets"""
        try:
            # –î–ª—è datasets —Ñ–æ—Ä–º–∞—Ç–∞ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç
            print("üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω datasets —Ñ–æ—Ä–º–∞—Ç - —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
            from datasets import Dataset, load_from_disk
            dataset_dict_path = os.path.join(dataset_path, 'dataset_dict')
            
            if os.path.exists(dataset_dict_path):
                dataset = load_from_disk(dataset_dict_path)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã
                def filter_function(example):
                    image_path = example.get('image_path', '')
                    if not os.path.isabs(image_path):
                        image_path = os.path.join(dataset_path, image_path)
                    return image_path not in items_to_remove
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä
                cleaned_dataset = dataset.filter(filter_function)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
                cleaned_dataset.save_to_disk(dataset_dict_path)
                
                print(f"üìã Datasets —Ñ–æ—Ä–º–∞—Ç –æ–±–Ω–æ–≤–ª–µ–Ω: {len(dataset)} ‚Üí {len(cleaned_dataset)}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ datasets —Ñ–æ—Ä–º–∞—Ç–∞: {e}")
    
    def _clean_folder_structure(self, dataset_path, items_to_remove):
        """–û—á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–∞–ø–æ—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π"""
        try:
            removed_count = 0
            
            for item_path in items_to_remove:
                if os.path.exists(item_path):
                    os.remove(item_path)
                    removed_count += 1
                    
                    # –£–¥–∞–ª—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª, –µ—Å–ª–∏ –µ—Å—Ç—å
                    base_name = os.path.splitext(item_path)[0]
                    txt_path = base_name + '.txt'
                    if os.path.exists(txt_path):
                        os.remove(txt_path)
            
            print(f"üìã –£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–æ—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {removed_count}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–ø–æ—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}")
    
    def _count_remaining_items(self, dataset_path):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ —Å—á–∏—Ç–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ
            if os.path.exists(os.path.join(dataset_path, 'metadata.json')):
                # TrOCR –¥–∞—Ç–∞—Å–µ—Ç
                with open(os.path.join(dataset_path, 'metadata.json'), 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                return len(metadata.get('annotations', []))
                
            elif os.path.exists(os.path.join(dataset_path, 'dataset_dict')):
                # Datasets —Ñ–æ—Ä–º–∞—Ç
                from datasets import load_from_disk
                dataset = load_from_disk(os.path.join(dataset_path, 'dataset_dict'))
                return len(dataset)
                
            else:
                # –ü–∞–ø–æ—á–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ - —Å—á–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
                count = 0
                for root, dirs, files in os.walk(dataset_path):
                    for file in files:
                        if os.path.splitext(file.lower())[1] in image_extensions:
                            count += 1
                return count
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥—Å—á–µ—Ç–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {e}")
            return 0
    
    def _update_metadata_after_cleanup(self, dataset_path, removal_stats):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –æ—á–∏—Å—Ç–∫–∏
            cleanup_log = {
                'cleanup_timestamp': datetime.now().isoformat(),
                'cleanup_stats': removal_stats,
                'cleanup_settings': {
                    'removed_duplicates': removal_stats['duplicates_removed'] > 0,
                    'removed_low_quality': removal_stats['low_quality_removed'] > 0
                }
            }
            
            log_file = os.path.join(dataset_path, 'cleanup_log.json')
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(cleanup_log, f, ensure_ascii=False, indent=2)
                
            print(f"üìã –°–æ–∑–¥–∞–Ω –ª–æ–≥ –æ—á–∏—Å—Ç–∫–∏: {log_file}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _load_annotations(self, dataset_path: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        annotations_file = Path(dataset_path) / "annotations.json"
        
        if not annotations_file.exists():
            # –ò—â–µ–º –¥—Ä—É–≥–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ñ–∞–π–ª—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            possible_files = [
                "annotations.json", "dataset.json", "data.json",
                "labels.json", "annotations.txt"
            ]
            
            for filename in possible_files:
                file_path = Path(dataset_path) / filename
                if file_path.exists():
                    annotations_file = file_path
                    break
            else:
                return []
        
        try:
            with open(annotations_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ñ–æ—Ä–º–∞—Ç
            if isinstance(data, list):
                return data
            elif 'annotations' in data:
                return data['annotations']
            elif 'data' in data:
                return data['data']
            else:
                return [data]
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {e}")
            return []
    
    def _find_duplicates(self, annotations: List[Dict]) -> List[DuplicateGroup]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞"""
        duplicates = []
        
        # 1. –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ö–µ—à—É
        image_duplicates = self._find_image_duplicates(annotations)
        duplicates.extend(image_duplicates)
        
        # 2. –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞
        text_duplicates = self._find_text_duplicates(annotations)
        duplicates.extend(text_duplicates)
        
        return duplicates
    
    def _find_image_duplicates(self, annotations: List[Dict]) -> List[DuplicateGroup]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        hash_groups = defaultdict(list)
        
        for ann in annotations:
            image_path = ann.get('image_path', '')
            if not os.path.exists(image_path):
                continue
                
            try:
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä—Ü–µ–ø—Ç—É–∞–ª—å–Ω—ã–π —Ö–µ—à
                image = Image.open(image_path)
                img_hash = imagehash.phash(image)
                hash_groups[str(img_hash)].append(image_path)
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è {image_path}: {e}")
                continue
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        duplicates = []
        for img_hash, paths in hash_groups.items():
            if len(paths) > 1:
                duplicates.append(DuplicateGroup(
                    type='exact',
                    similarity=1.0,
                    items=paths,
                    primary_item=paths[0]  # –ü–µ—Ä–≤—ã–π –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
                ))
        
        # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Ä–∞–∑–Ω—ã–µ —Ö–µ—à–∏, –Ω–æ –±–ª–∏–∑–∫–∏–µ)
        near_duplicates = self._find_near_duplicate_images(annotations)
        duplicates.extend(near_duplicates)
        
        return duplicates
    
    def _find_near_duplicate_images(self, annotations: List[Dict]) -> List[DuplicateGroup]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö (–Ω–æ –Ω–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö) –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        if len(annotations) < 2:
            return []
            
        hashes = []
        paths = []
        
        for ann in annotations:
            image_path = ann.get('image_path', '')
            if not os.path.exists(image_path):
                continue
                
            try:
                image = Image.open(image_path)
                img_hash = imagehash.phash(image)
                hashes.append(img_hash)
                paths.append(image_path)
            except:
                continue
        
        duplicates = []
        processed = set()
        
        for i, hash1 in enumerate(hashes):
            if i in processed:
                continue
                
            similar_group = [paths[i]]
            
            for j, hash2 in enumerate(hashes[i+1:], i+1):
                if j in processed:
                    continue
                    
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –•–µ–º–º–∏–Ω–≥–∞
                hamming_distance = hash1 - hash2
                similarity = 1.0 - (hamming_distance / 64.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1
                
                if similarity >= self.similarity_threshold:
                    similar_group.append(paths[j])
                    processed.add(j)
            
            if len(similar_group) > 1:
                processed.add(i)
                duplicates.append(DuplicateGroup(
                    type='near_duplicate',
                    similarity=similarity,
                    items=similar_group,
                    primary_item=similar_group[0]
                ))
        
        return duplicates
    
    def _find_text_duplicates(self, annotations: List[Dict]) -> List[DuplicateGroup]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–µ–∫—Å—Ç–∞"""
        text_groups = defaultdict(list)
        
        # 1. –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞
        for ann in annotations:
            text = ann.get('text', '').strip()
            if text:
                text_groups[text].append(ann.get('image_path', ''))
        
        exact_duplicates = []
        for text, paths in text_groups.items():
            if len(paths) > 1:
                exact_duplicates.append(DuplicateGroup(
                    type='text_duplicate',
                    similarity=1.0,
                    items=paths,
                    primary_item=paths[0]
                ))
        
        # 2. –ü–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF)
        similar_duplicates = self._find_similar_texts(annotations)
        
        return exact_duplicates + similar_duplicates
    
    def _find_similar_texts(self, annotations: List[Dict]) -> List[DuplicateGroup]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é TF-IDF"""
        texts = []
        paths = []
        
        for ann in annotations:
            text = ann.get('text', '').strip()
            if text and len(text) > 3:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                texts.append(text)
                paths.append(ann.get('image_path', ''))
        
        if len(texts) < 2:
            return []
        
        try:
            # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            self.vectorizer = TfidfVectorizer(
                lowercase=True,
                stop_words=None,
                ngram_range=(1, 2),
                max_features=1000
            )
            
            text_vectors = self.vectorizer.fit_transform(texts)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarity_matrix = cosine_similarity(text_vectors)
            
            duplicates = []
            processed = set()
            
            for i in range(len(texts)):
                if i in processed:
                    continue
                    
                similar_group = [paths[i]]
                
                for j in range(i+1, len(texts)):
                    if j in processed:
                        continue
                        
                    similarity = similarity_matrix[i][j]
                    
                    if similarity >= self.similarity_threshold:
                        similar_group.append(paths[j])
                        processed.add(j)
                
                if len(similar_group) > 1:
                    processed.add(i)
                    duplicates.append(DuplicateGroup(
                        type='text_similarity',
                        similarity=float(similarity),
                        items=similar_group,
                        primary_item=similar_group[0]
                    ))
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤: {e}")
            return []
        
        return duplicates
    
    def _analyze_image_quality(self, annotations: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        quality_stats = {
            'total_images': 0,
            'analyzed_images': 0,
            'average_quality': 0.0,
            'low_quality_count': 0,
            'resolution_stats': {},
            'quality_distribution': {'excellent': 0, 'good': 0, 'average': 0, 'poor': 0}
        }
        
        quality_scores = []
        resolutions = []
        
        for ann in annotations:
            image_path = ann.get('image_path', '')
            quality_stats['total_images'] += 1
            
            if not os.path.exists(image_path):
                continue
            
            try:
                metrics = self._analyze_single_image(image_path)
                quality_score = metrics.overall_score
                quality_scores.append(quality_score)
                resolutions.append(metrics.width * metrics.height)
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                if quality_score >= 0.8:
                    quality_stats['quality_distribution']['excellent'] += 1
                elif quality_score >= 0.6:
                    quality_stats['quality_distribution']['good'] += 1
                elif quality_score >= 0.4:
                    quality_stats['quality_distribution']['average'] += 1
                else:
                    quality_stats['quality_distribution']['poor'] += 1
                    quality_stats['low_quality_count'] += 1
                
                quality_stats['analyzed_images'] += 1
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
                continue
        
        if quality_scores:
            quality_stats['average_quality'] = sum(quality_scores) / len(quality_scores)
            quality_stats['resolution_stats'] = {
                'min': min(resolutions),
                'max': max(resolutions),
                'average': sum(resolutions) / len(resolutions)
            }
        
        return quality_stats
    
    def _analyze_single_image(self, image_path: str) -> ImageQualityMetrics:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        
        height, width, channels = image.shape
        file_size = os.path.getsize(image_path)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –≥—Ä–∞–¥–∞—Ü–∏–∏ —Å–µ—Ä–æ–≥–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        brightness = np.mean(gray)
        contrast = np.std(gray)
        
        # –†–µ–∑–∫–æ—Å—Ç—å (Laplacian variance)
        if SKIMAGE_AVAILABLE:
            sharpness = laplace(gray).var()
        else:
            laplacian = cv2.Laplacian(gray, cv2.CV_64F)
            sharpness = laplacian.var()
        
        # –£—Ä–æ–≤–µ–Ω—å —à—É–º–∞ (–æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ –≤—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã)
        noise_level = self._estimate_noise_level(gray)
        
        aspect_ratio = width / height
        
        return ImageQualityMetrics(
            width=width,
            height=height,
            channels=channels,
            file_size=file_size,
            brightness=brightness,
            contrast=contrast,
            sharpness=sharpness,
            noise_level=noise_level,
            aspect_ratio=aspect_ratio
        )
    
    def _estimate_noise_level(self, gray_image: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è —à—É–º–∞ –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ–¥–∏–∞–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è —à—É–º–∞
        median_filtered = cv2.medianBlur(gray_image, 5)
        
        # –†–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º
        noise = cv2.absdiff(gray_image, median_filtered)
        
        # –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —à—É–º–∞
        return np.mean(noise)
    
    def _analyze_text_quality(self, annotations: List[Dict]) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞"""
        text_stats = {
            'total_texts': 0,
            'analyzed_texts': 0,
            'average_length': 0,
            'average_word_count': 0,
            'total_spelling_errors': 0,
            'language_distribution': defaultdict(int),
            'quality_distribution': {'excellent': 0, 'good': 0, 'average': 0, 'poor': 0}
        }
        
        lengths = []
        word_counts = []
        spelling_errors = []
        
        for ann in annotations:
            text = ann.get('text', '').strip()
            text_stats['total_texts'] += 1
            
            if not text:
                continue
            
            try:
                metrics = self._analyze_single_text(text)
                
                lengths.append(metrics.length)
                word_counts.append(metrics.word_count)
                spelling_errors.append(metrics.spelling_errors)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                if self._is_cyrillic(text):
                    text_stats['language_distribution']['ru'] += 1
                elif self._is_latin(text):
                    text_stats['language_distribution']['en'] += 1
                else:
                    text_stats['language_distribution']['mixed'] += 1
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                quality_score = metrics.readability_score
                if quality_score >= 0.8:
                    text_stats['quality_distribution']['excellent'] += 1
                elif quality_score >= 0.6:
                    text_stats['quality_distribution']['good'] += 1
                elif quality_score >= 0.4:
                    text_stats['quality_distribution']['average'] += 1
                else:
                    text_stats['quality_distribution']['poor'] += 1
                
                text_stats['analyzed_texts'] += 1
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ '{text[:50]}...': {e}")
                continue
        
        if lengths:
            text_stats['average_length'] = sum(lengths) / len(lengths)
            text_stats['average_word_count'] = sum(word_counts) / len(word_counts)
            text_stats['total_spelling_errors'] = sum(spelling_errors)
        
        return text_stats
    
    def _analyze_single_text(self, text: str) -> TextQualityMetrics:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        length = len(text)
        words = text.split()
        word_count = len(words)
        
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Å–∏–º–≤–æ–ª–æ–≤
        unique_chars = len(set(text.lower()))
        char_diversity = unique_chars / max(1, length)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏
        spelling_errors = 0
        if self.spell_checker and word_count > 0:
            try:
                if self._is_cyrillic(text):
                    unknown_words = self.spell_checker.unknown(words)
                else:
                    unknown_words = self.spell_checker_en.unknown(words) if hasattr(self, 'spell_checker_en') else set()
                spelling_errors = len(unknown_words)
            except:
                spelling_errors = 0
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Ç–∏–ø–æ–≤ —Å–∏–º–≤–æ–ª–æ–≤
        special_chars = sum(1 for c in text if not c.isalnum() and not c.isspace())
        digits = sum(1 for c in text if c.isdigit())
        uppercase = sum(1 for c in text if c.isupper())
        
        special_chars_ratio = special_chars / max(1, length)
        digit_ratio = digits / max(1, length)
        uppercase_ratio = uppercase / max(1, length)
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —è–∑—ã–∫–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        if self._is_cyrillic(text):
            language_confidence = 0.9
        elif self._is_latin(text):
            language_confidence = 0.8
        else:
            language_confidence = 0.5
        
        return TextQualityMetrics(
            length=length,
            word_count=word_count,
            char_diversity=char_diversity,
            language_confidence=language_confidence,
            spelling_errors=spelling_errors,
            special_chars_ratio=special_chars_ratio,
            digit_ratio=digit_ratio,
            uppercase_ratio=uppercase_ratio
        )
    
    def _is_cyrillic(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É"""
        cyrillic_chars = sum(1 for c in text if '\u0400' <= c <= '\u04FF')
        return cyrillic_chars > len(text) * 0.3
    
    def _is_latin(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Ç–µ–∫—Å—Ç –ª–∞—Ç–∏–Ω–∏—Ü—É"""
        latin_chars = sum(1 for c in text if c.isalpha() and c.isascii())
        return latin_chars > len(text) * 0.3
    
    def _count_valid_items(self, validation_results: Dict) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        total_items = validation_results['total_items']
        critical_issues = sum(1 for issue in validation_results['issues'] 
                            if issue['severity'] == 'critical')
        return max(0, total_items - critical_issues)
    
    def _generate_recommendations(self, validation_results: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º
        issues_by_type = defaultdict(int)
        for issue in validation_results['issues']:
            issues_by_type[issue['type']] += 1
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥—É–±–ª–∏–∫–∞—Ç–∞–º
        if issues_by_type['duplicate'] > 0:
            recommendations.append(
                f"üîÑ –ù–∞–π–¥–µ–Ω–æ {issues_by_type['duplicate']} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Ö —É–¥–∞–ª–∏—Ç—å –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–∏—è."
            )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        if 'images' in validation_results.get('quality_stats', {}):
            img_stats = validation_results['quality_stats']['images']
            if img_stats.get('low_quality_count', 0) > 0:
                recommendations.append(
                    f"üì∏ –ù–∞–π–¥–µ–Ω–æ {img_stats['low_quality_count']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞. "
                    "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Ö —É–ª—É—á—à–∏—Ç—å –∏–ª–∏ –∑–∞–º–µ–Ω–∏—Ç—å."
                )
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç–µ–∫—Å—Ç—É
        if 'text' in validation_results.get('quality_stats', {}):
            text_stats = validation_results['quality_stats']['text']
            if text_stats.get('total_spelling_errors', 0) > 0:
                recommendations.append(
                    f"üìù –ù–∞–π–¥–µ–Ω–æ {text_stats['total_spelling_errors']} –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫. "
                    "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç—ã."
                )
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        valid_ratio = validation_results['valid_items'] / max(1, validation_results['total_items'])
        if valid_ratio < 0.8:
            recommendations.append(
                "‚ö†Ô∏è –ú–µ–Ω–µ–µ 80% –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ö–æ–¥—è—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é. "
                "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞."
            )
        elif valid_ratio < 0.95:
            recommendations.append(
                "‚ú® –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å, —É–¥–∞–ª–∏–≤ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã."
            )
        else:
            recommendations.append(
                "‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è."
            )
        
        return recommendations
    
    def _save_clean_dataset(self, dataset_path: str, clean_annotations: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é
        original_file = Path(dataset_path) / "annotations.json"
        backup_file = Path(dataset_path) / f"annotations_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        if original_file.exists():
            import shutil
            shutil.copy2(original_file, backup_file)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        with open(original_file, 'w', encoding='utf-8') as f:
            json.dump(clean_annotations, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"üíæ –û—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {original_file}")
        self.logger.info(f"üìã –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_file}")
    
    def generate_quality_report(self, validation_results: Dict, output_path: str = None) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        if output_path is None:
            output_path = Path(validation_results['dataset_path']) / "quality_report.html"
        
        # HTML –æ—Ç—á–µ—Ç —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        html_content = self._generate_html_report(validation_results)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"üìä –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        return str(output_path)
    
    def _generate_html_report(self, validation_results: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è HTML –æ—Ç—á–µ—Ç–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π HTML –æ—Ç—á–µ—Ç (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>–û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞</title>
    <meta charset="utf-8">
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .critical {{ background: #ffe6e6; }}
        .warning {{ background: #fff3e0; }}
        .success {{ background: #e8f5e8; }}
        .stats {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; }}
        .stat-item {{ background: #f8f9fa; padding: 10px; border-radius: 3px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üìä –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞</h1>
        <p>–î–∞—Ç–∞—Å–µ—Ç: {validation_results['dataset_path']}</p>
        <p>–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {validation_results['timestamp']}</p>
    </div>
    
    <div class="section">
        <h2>üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞</h2>
        <div class="stats">
            <div class="stat-item">
                <strong>–í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤:</strong> {validation_results['total_items']}
            </div>
            <div class="stat-item">
                <strong>–í–∞–ª–∏–¥–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:</strong> {validation_results['valid_items']}
            </div>
            <div class="stat-item">
                <strong>–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º:</strong> {len(validation_results['issues'])}
            </div>
            <div class="stat-item">
                <strong>–ì—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:</strong> {len(validation_results['duplicates'])}
            </div>
        </div>
    </div>
    
    <div class="section">
        <h2>üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h2>
        <ul>
        {''.join(f'<li>{rec}</li>' for rec in validation_results['recommendations'])}
        </ul>
    </div>
</body>
</html>
        """
        
        return html 