"""
Enhanced Donut Trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ callback'–∞–º–∏
"""

import torch
import logging
from typing import Dict, Any, Optional, List, Callable
from pathlib import Path
import tempfile
import json
from datetime import datetime

from transformers import (
    VisionEncoderDecoderModel, DonutProcessor, TrainingArguments,
    Trainer, TrainerCallback, EarlyStoppingCallback
)
from datasets import Dataset

# –ò–º–ø–æ—Ä—Ç –±–∞–∑–æ–≤–æ–≥–æ LoRA trainer
from .core.base_lora_trainer import BaseLoraTrainer, ModelType

logger = logging.getLogger(__name__)


class EnhancedDonutMetricsCallback(TrainerCallback):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π callback –¥–ª—è –º–µ—Ç—Ä–∏–∫ Donut —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    def __init__(self, processor, eval_dataset, log_callback=None):
        self.processor = processor
        self.eval_dataset = eval_dataset
        self.log_callback = log_callback
        self.best_metrics = {}
        
    def on_evaluate(self, args, state, control, model=None, logs=None, **kwargs):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        if logs:
            # –õ–æ–≥–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            current_loss = logs.get('eval_loss', float('inf'))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            if 'eval_loss' not in self.best_metrics or current_loss < self.best_metrics['eval_loss']:
                self.best_metrics.update(logs)
                self.best_metrics['epoch'] = state.epoch
                
            message = f"üìä Epoch {state.epoch:.1f}: Loss={current_loss:.4f}"
            if 'eval_accuracy' in logs:
                message += f", Accuracy={logs['eval_accuracy']:.3f}"
                
            if self.log_callback:
                self.log_callback(message)
            
            logger.info(message)


class EnhancedDonutDataCollator:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π data collator —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, processor, max_length=512, quality_threshold=0.95):
        self.processor = processor
        self.max_length = max_length
        self.quality_threshold = quality_threshold
        
    def __call__(self, batch):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        try:
            pixel_values = []
            labels = []
            
            for item in batch:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if self._check_image_quality(item.get('image')):
                    pixel_values.append(item['pixel_values'])
                    labels.append(item['labels'])
            
            if not pixel_values:
                raise ValueError("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞—Ç—á–µ")
                
            return {
                'pixel_values': torch.stack(pixel_values),
                'labels': torch.stack(labels) if labels[0] is not None else None
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ data collator: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º fallback –±–∞—Ç—á
            return self._create_fallback_batch()
            
    def _check_image_quality(self, image):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if image is None:
            return False
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞
        return hasattr(image, 'size') and min(image.size) > 32
        
    def _create_fallback_batch(self):
        """–°–æ–∑–¥–∞–µ—Ç fallback –±–∞—Ç—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫"""
        return {
            'pixel_values': torch.zeros(1, 3, 224, 224),
            'labels': None
        }


class EnhancedDonutTrainer(BaseLoraTrainer):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π Donut trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π LoRA
    –ù–∞—Å–ª–µ–¥—É–µ—Ç—Å—è –æ—Ç BaseLoraTrainer –¥–ª—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
    """
    
    def __init__(self, app_config, logger=None):
        super().__init__(ModelType.DONUT, logger)
        self.app_config = app_config
        self.callbacks = {}
        self._stop_training = False
        self.logger = logger or logging.getLogger(__name__)
        
    def _apply_model_specific_optimizations(self, model, training_args: Dict[str, Any]) -> Any:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è Donut –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –¥–ª—è Donut –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è - –ø–∞—Ç—á–∏–º forward –º–µ—Ç–æ–¥
        def patched_forward(pixel_values=None, labels=None, **kwargs):
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è VisionEncoderDecoderModel
            filtered_kwargs = {k: v for k, v in kwargs.items() 
                             if k not in ['input_ids', 'attention_mask']}
            
            if labels is not None:
                # –û–±—É—á–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑—É–µ–º forward —Å labels
                return model.original_forward(
                    pixel_values=pixel_values,
                    labels=labels,
                    **filtered_kwargs
                )
            else:
                # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å: –∏—Å–ø–æ–ª—å–∑—É–µ–º generate
                return model.generate(
                    pixel_values=pixel_values,
                    **filtered_kwargs
                )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π forward
        if not hasattr(model, 'original_forward'):
            model.original_forward = model.forward
            model.forward = patched_forward
            
        self.logger.info("üîß Enhanced Donut –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        
        return model
        
    def set_callbacks(self, log_callback=None, progress_callback=None):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç callback —Ñ—É–Ω–∫—Ü–∏–∏"""
        self.log_callback = log_callback
        self.progress_callback = progress_callback
        
    def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ"""
        self._stop_training = True
        if hasattr(self, 'log_callback') and self.log_callback:
            self.log_callback("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
            
    def train_model(self, dataset_path: str, model_name: str = "naver-clova-ix/donut-base", 
                   output_dir: str = "data/trained_models/enhanced_donut",
                   training_args: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –û–±—É—á–∞–µ—Ç Enhanced Donut –º–æ–¥–µ–ª—å —Å LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            model_name: –ò–º—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏  
            output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            training_args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        
        try:
            self._log("üöÄ –ó–∞–ø—É—Å–∫ Enhanced Donut –æ–±—É—á–µ–Ω–∏—è —Å LoRA")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            processor = DonutProcessor.from_pretrained(model_name, cache_dir="data/models")
            model = VisionEncoderDecoderModel.from_pretrained(model_name, cache_dir="data/models")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
            model, lora_success = self.apply_lora_optimization(model, training_args or {})
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º memory –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏  
            model = self.apply_memory_optimizations(model, training_args or {})
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            dataset = self._load_dataset(dataset_path)
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/eval
            train_dataset, eval_dataset = self._split_dataset(dataset)
            
            # Data collator
            data_collator = EnhancedDonutDataCollator(processor)
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
            args = self._get_training_arguments(output_dir, training_args)
            
            # Callbacks
            callbacks = self._setup_callbacks(processor, eval_dataset)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ trainer
            trainer = Trainer(
                model=model,
                args=args,
                train_dataset=train_dataset,
                eval_dataset=eval_dataset,
                data_collator=data_collator,
                callbacks=callbacks
            )
            
            # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
            self._log("‚ñ∂Ô∏è –ù–∞—á–∞–ª–æ Enhanced –æ–±—É—á–µ–Ω–∏—è...")
            start_time = datetime.now()
            
            train_result = trainer.train()
            
            duration = datetime.now() - start_time
            self._log(f"‚úÖ Enhanced –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
            if lora_success:
                self._save_lora_model(trainer, output_dir)
            else:
                trainer.save_model(output_dir)
                
            return {
                'success': True,
                'model_path': output_dir,
                'train_loss': train_result.training_loss,
                'lora_applied': lora_success,
                'duration': str(duration)
            }
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ Enhanced –æ–±—É—á–µ–Ω–∏—è: {e}"
            self._log(error_msg)
            self.logger.error(error_msg, exc_info=True)
            return {'success': False, 'error': str(e)}
    
    def _load_dataset(self, dataset_path: str) -> Dataset:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç"""
        dataset_path = Path(dataset_path)
        
        if dataset_path.suffix == '.json':
            # JSON –¥–∞—Ç–∞—Å–µ—Ç
            with open(dataset_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return Dataset.from_list(data)
        else:
            # HuggingFace –¥–∞—Ç–∞—Å–µ—Ç
            from datasets import load_from_disk
            return load_from_disk(str(dataset_path))
    
    def _split_dataset(self, dataset: Dataset, test_size: float = 0.1):
        """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train/eval"""
        if len(dataset) < 10:
            return dataset, dataset  # –ú–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        
        split = dataset.train_test_split(test_size=test_size, seed=42)
        return split['train'], split['test']
    
    def _get_training_arguments(self, output_dir: str, custom_args: Optional[Dict[str, Any]]) -> TrainingArguments:
        """–°–æ–∑–¥–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
        
        default_args = {
            'output_dir': output_dir,
            'per_device_train_batch_size': 1,
            'per_device_eval_batch_size': 1,
            'num_train_epochs': 3,
            'learning_rate': 5e-5,
            'logging_steps': 10,
            'eval_steps': 50,
            'save_steps': 100,
            'evaluation_strategy': 'steps',
            'save_strategy': 'steps',
            'load_best_model_at_end': True,
            'metric_for_best_model': 'eval_loss',
            'greater_is_better': False,
            'fp16': True,
            'dataloader_pin_memory': False,
            'remove_unused_columns': False
        }
        
        if custom_args:
            default_args.update(custom_args)
        
        return TrainingArguments(**default_args)
    
    def _setup_callbacks(self, processor, eval_dataset) -> List[TrainerCallback]:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        callbacks = []
        
        # Metrics callback
        metrics_callback = EnhancedDonutMetricsCallback(
            processor, eval_dataset, self.log_callback if hasattr(self, 'log_callback') else None
        )
        callbacks.append(metrics_callback)
        
        # Early stopping
        callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))
        
        return callbacks
    
    def _save_lora_model(self, trainer, output_dir: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç LoRA –º–æ–¥–µ–ª—å"""
        try:
            trainer.model.save_pretrained(output_dir)
            self._log(f"üíæ LoRA –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è LoRA –º–æ–¥–µ–ª–∏: {e}")
            # Fallback to regular save
            trainer.save_model(output_dir)
    
    def _log(self, message: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å callback"""
        if hasattr(self, 'log_callback') and self.log_callback:
            self.log_callback(message)
        self.logger.info(message) 