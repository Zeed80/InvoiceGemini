"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä Donut —Å –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–æ–π –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
–¶–µ–ª—å: –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–µ–π > 98%
"""

import os
import json
import torch
import logging
from datetime import datetime
from typing import Dict, List, Optional, Union, Any, Tuple
from pathlib import Path
import numpy as np
from collections import defaultdict

# Transformers imports
from transformers import (
    DonutProcessor,
    VisionEncoderDecoderModel,
    VisionEncoderDecoderConfig,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TrainerCallback
)

# Dataset imports
from datasets import Dataset, DatasetDict
from torch.utils.data import DataLoader
from PIL import Image

# Local imports
from .data_quality_enhancer import DataQualityEnhancer, AnnotationQualityMetrics
from .intelligent_data_extractor import IntelligentDataExtractor
from .enhanced_data_preparator import EnhancedDataPreparator
from .core.base_lora_trainer import BaseLor–∞Trainer, ModelType

logger = logging.getLogger(__name__)


class DonutFieldExtractionMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–µ–π –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.reset()
        
    def reset(self):
        self.true_positives = defaultdict(int)
        self.false_positives = defaultdict(int)
        self.false_negatives = defaultdict(int)
        self.exact_matches = defaultdict(int)
        self.partial_matches = defaultdict(int)
        self.total_documents = 0
        self.perfect_documents = 0  # –î–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ 100% —Ç–æ—á–Ω–æ—Å—Ç—å—é
        
    def add_document(self, predicted_fields: Dict, ground_truth_fields: Dict):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        self.total_documents += 1
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ
        all_fields = set(predicted_fields.keys()) | set(ground_truth_fields.keys())
        document_perfect = True
        
        for field_name in all_fields:
            pred_value = predicted_fields.get(field_name, "").strip()
            true_value = ground_truth_fields.get(field_name, "").strip()
            
            if pred_value and true_value:
                # –ï—Å—Ç—å –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ, –∏ ground truth
                if self._normalize_value(pred_value) == self._normalize_value(true_value):
                    self.true_positives[field_name] += 1
                    self.exact_matches[field_name] += 1
                elif self._is_partial_match(pred_value, true_value):
                    self.true_positives[field_name] += 1
                    self.partial_matches[field_name] += 1
                    document_perfect = False
                else:
                    self.false_positives[field_name] += 1
                    document_perfect = False
            elif pred_value and not true_value:
                # –õ–æ–∂–Ω–æ–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ
                self.false_positives[field_name] += 1
                document_perfect = False
            elif not pred_value and true_value:
                # –ü—Ä–æ–ø—É—â–µ–Ω–Ω–æ–µ –ø–æ–ª–µ
                self.false_negatives[field_name] += 1
                document_perfect = False
                
        if document_perfect and len(ground_truth_fields) > 0:
            self.perfect_documents += 1
            
    def _normalize_value(self, value: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        value = " ".join(value.split())
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –¥–ª—è –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–æ–ª–µ–π
        value = value.lower()
        
        # –£–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –ø–æ –∫—Ä–∞—è–º
        value = value.strip(".,;:")
        
        return value
        
    def _is_partial_match(self, pred: str, true: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        # –ï—Å–ª–∏ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –≤ –¥—Ä—É–≥–æ–π
        if pred in true or true in pred:
            return True
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –¥–∞—Ç –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö)
        pred_norm = self._normalize_value(pred)
        true_norm = self._normalize_value(true)
        
        # –î–ª—è —á–∏—Å–µ–ª —É–±–∏—Ä–∞–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        pred_numbers = "".join(filter(str.isdigit, pred_norm))
        true_numbers = "".join(filter(str.isdigit, true_norm))
        
        if pred_numbers and pred_numbers == true_numbers:
            return True
            
        return False
        
    def get_metrics(self) -> Dict[str, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏"""
        metrics = {}
        
        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –ø–æ–ª—è–º
        total_tp = sum(self.true_positives.values())
        total_fp = sum(self.false_positives.values())
        total_fn = sum(self.false_negatives.values())
        
        # Precision, Recall, F1
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        metrics['overall_precision'] = precision
        metrics['overall_recall'] = recall
        metrics['overall_f1'] = f1
        
        # –¢–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        metrics['document_accuracy'] = self.perfect_documents / self.total_documents if self.total_documents > 0 else 0
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        total_exact = sum(self.exact_matches.values())
        total_partial = sum(self.partial_matches.values())
        metrics['exact_match_rate'] = total_exact / (total_exact + total_partial) if (total_exact + total_partial) > 0 else 0
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ–ª—è–º
        metrics['per_field'] = {}
        for field_name in set(self.true_positives.keys()) | set(self.false_positives.keys()) | set(self.false_negatives.keys()):
            tp = self.true_positives[field_name]
            fp = self.false_positives[field_name]
            fn = self.false_negatives[field_name]
            
            field_precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            field_recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            field_f1 = 2 * (field_precision * field_recall) / (field_precision + field_recall) if (field_precision + field_recall) > 0 else 0
            
            metrics['per_field'][field_name] = {
                'precision': field_precision,
                'recall': field_recall,
                'f1': field_f1,
                'support': tp + fn
            }
            
        return metrics


class EnhancedDonutMetricsCallback(TrainerCallback):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π callback –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ Donut"""
    
    def __init__(self, processor, eval_dataset, log_callback=None):
        self.processor = processor
        self.eval_dataset = eval_dataset
        self.log_callback = log_callback
        self.metrics_calculator = DonutFieldExtractionMetrics()
        
    def on_evaluate(self, args, state, control, model, **kwargs):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ—Ü–µ–Ω–∫–∏"""
        try:
            # –ë–µ—Ä–µ–º –≤—ã–±–æ—Ä–∫—É –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            eval_samples = self.eval_dataset.select(range(min(100, len(self.eval_dataset))))
            
            self.metrics_calculator.reset()
            
            model.eval()
            with torch.no_grad():
                for sample in eval_samples:
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    image = sample['image']
                    target_text = sample['text']
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    pixel_values = self.processor(image, return_tensors="pt").pixel_values
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π task prompt –¥–ª—è Donut
                    task_prompt = "<s_docvqa><s_question>Extract all fields from the document</s_question><s_answer>"
                    decoder_input_ids = self.processor.tokenizer(
                        task_prompt, 
                        add_special_tokens=False, 
                        return_tensors="pt"
                    ).input_ids
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    outputs = model.generate(
                        pixel_values.to(model.device),
                        decoder_input_ids=decoder_input_ids.to(model.device),
                        max_length=512,
                        num_beams=4,
                        temperature=0.1,
                        do_sample=False,
                        pad_token_id=self.processor.tokenizer.pad_token_id,
                        eos_token_id=self.processor.tokenizer.eos_token_id,
                    )
                    
                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    pred_text = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
                    pred_text = pred_text.replace(task_prompt, "").strip()
                    
                    # –ü–∞—Ä—Å–∏–º JSON –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ ground truth
                    try:
                        pred_fields = self._parse_donut_output(pred_text)
                        true_fields = self._parse_donut_output(target_text)
                        
                        self.metrics_calculator.add_document(pred_fields, true_fields)
                    except Exception as e:
                        logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                        continue
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = self.metrics_calculator.get_metrics()
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            accuracy_percentage = metrics['overall_f1'] * 100
            doc_accuracy = metrics['document_accuracy'] * 100
            exact_match = metrics['exact_match_rate'] * 100
            
            metrics_msg = (
                f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–µ–π:\n"
                f"   üéØ –û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (F1): {accuracy_percentage:.1f}%\n"
                f"   üìÑ –¢–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (100% –ø–æ–ª–µ–π): {doc_accuracy:.1f}%\n"
                f"   ‚úÖ –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {exact_match:.1f}%\n"
                f"   üìà Precision: {metrics['overall_precision']:.3f}\n"
                f"   üìä Recall: {metrics['overall_recall']:.3f}\n"
            )
            
            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            if accuracy_percentage >= 98:
                quality = "üèÜ –ü–†–ï–í–û–°–•–û–î–ù–û!"
            elif accuracy_percentage >= 95:
                quality = "üî• –û—Ç–ª–∏—á–Ω–æ"
            elif accuracy_percentage >= 90:
                quality = "‚úÖ –•–æ—Ä–æ—à–æ"
            elif accuracy_percentage >= 80:
                quality = "üü° –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ"
            else:
                quality = "üî¥ –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
                
            metrics_msg += f"   üíé –ö–∞—á–µ—Å—Ç–≤–æ: {quality}\n"
            
            # –î–µ—Ç–∞–ª–∏ –ø–æ –ø–æ–ª—è–º (—Ç–æ–ø-5 —Ö—É–¥—à–∏—Ö)
            if metrics['per_field']:
                worst_fields = sorted(
                    metrics['per_field'].items(), 
                    key=lambda x: x[1]['f1']
                )[:5]
                
                if worst_fields:
                    metrics_msg += "   üìâ –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–ª—è:\n"
                    for field_name, field_metrics in worst_fields:
                        if field_metrics['f1'] < 0.9:
                            metrics_msg += f"      ‚Ä¢ {field_name}: F1={field_metrics['f1']:.2f}\n"
            
            if self.log_callback:
                self.log_callback(metrics_msg)
                
            logger.info(metrics_msg)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            state.log_history[-1]['eval_field_f1'] = metrics['overall_f1']
            state.log_history[-1]['eval_doc_accuracy'] = metrics['document_accuracy']
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫: {str(e)}"
            if self.log_callback:
                self.log_callback(error_msg)
            logger.error(error_msg, exc_info=True)
            
    def _parse_donut_output(self, text: str) -> Dict[str, str]:
        """–ü–∞—Ä—Å–∏—Ç –≤—ã—Ö–æ–¥ Donut –≤ —Å–ª–æ–≤–∞—Ä—å –ø–æ–ª–µ–π"""
        fields = {}
        
        # –ü–æ–ø—ã—Ç–∫–∞ 1: JSON –ø–∞—Ä—Å–∏–Ω–≥
        try:
            if text.strip().startswith('{'):
                return json.loads(text)
        except (json.JSONDecodeError, ValueError, TypeError) as e:
            # –û—à–∏–±–∫–∞ JSON –ø–∞—Ä—Å–∏–Ω–≥–∞, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥
            pass
            
        # –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤ Donut (<s_field>value</s_field>)
        import re
        pattern = r'<s_([^>]+)>([^<]+)</s_\1>'
        matches = re.findall(pattern, text)
        
        for field_name, value in matches:
            fields[field_name] = value.strip()
            
        return fields


class HighQualityDonutDataCollator:
    """Data collator —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self, processor, max_length=512, quality_threshold=0.95):
        self.processor = processor
        self.max_length = max_length
        self.quality_threshold = quality_threshold
        self.quality_enhancer = DataQualityEnhancer()
        
    def __call__(self, batch):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞"""
        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        high_quality_batch = []
        
        for item in batch:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            if self._check_annotation_quality(item):
                high_quality_batch.append(item)
            else:
                logger.warning("–ü—Ä–æ–ø—É—â–µ–Ω –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä")
                
        if not high_quality_batch:
            # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã –Ω–∏–∑–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –±–∞—Ç—á
            high_quality_batch = batch
            
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç—ã
        images = [item['image'] for item in high_quality_batch]
        texts = [item['text'] for item in high_quality_batch]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        pixel_values = self.processor(
            images, 
            return_tensors="pt"
        ).pixel_values
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        formatted_texts = [self._format_target_text(text) for text in texts]
        
        labels = self.processor.tokenizer(
            formatted_texts,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).input_ids
        
        # –ó–∞–º–µ–Ω—è–µ–º padding —Ç–æ–∫–µ–Ω—ã –Ω–∞ -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ loss
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        return {
            'pixel_values': pixel_values,
            'labels': labels
        }
        
    def _check_annotation_quality(self, item) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            if 'image' not in item or 'text' not in item:
                return False
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –ø—É—Å—Ç–æ–π
            if not item['text'] or len(item['text'].strip()) < 10:
                return False
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
            if '<s_' in item['text'] or '{' in item['text']:
                # –î–∞–Ω–Ω—ã–µ –≤—ã–≥–ª—è–¥—è—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏
                return True
                
            return True
            
        except Exception:
            return False
            
    def _format_target_text(self, text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ü–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –î–æ–±–∞–≤–ª—è–µ–º task prefix –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        task_prefix = "<s_docvqa><s_question>Extract all fields from the document</s_question><s_answer>"
        
        if not text.startswith(task_prefix):
            text = task_prefix + text + "</s_answer>"
            
        return text


class EnhancedDonutTrainer(BaseLor–∞Trainer):
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π Donut trainer —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –±–∞–∑–æ–≤–æ–≥–æ LoRA –∫–ª–∞—Å—Å–∞
    –£—Å—Ç—Ä–∞–Ω—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞ —á–µ—Ä–µ–∑ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –æ—Ç BaseLor–∞Trainer
    """
    
    def __init__(self, app_config, logger=None):
        super().__init__(ModelType.DONUT, logger)
        self.app_config = app_config
        self.callbacks = {}
        self._stop_training = False
        
    def _apply_model_specific_optimizations(self, model, training_args):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è Donut –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –¥–ª—è Donut –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è - –ø–∞—Ç—á–∏–º forward –º–µ—Ç–æ–¥
        def patched_forward(pixel_values=None, labels=None, **kwargs):
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è VisionEncoderDecoderModel
            filtered_kwargs = {k: v for k, v in kwargs.items() 
                             if k not in ['input_ids', 'attention_mask']}
            
            return model.original_forward(
                pixel_values=pixel_values,
                labels=labels,
                **filtered_kwargs
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π forward –∏ –∑–∞–º–µ–Ω—è–µ–º
        if not hasattr(model, 'original_forward'):
            model.original_forward = model.forward
            model.forward = patched_forward
            self._log("‚úÖ Donut forward method patched")
        
        return model 