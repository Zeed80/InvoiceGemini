import os
import json
import torch
import logging
from datetime import datetime
from typing import Dict, List, Optional, Union, Any
from pathlib import Path

# Transformers imports
from transformers import (
    DonutProcessor,
    VisionEncoderDecoderModel,
    VisionEncoderDecoderConfig,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback,
    TrainerCallback
)

# Dataset imports
from datasets import Dataset, DatasetDict, load_from_disk
from torch.utils.data import DataLoader
from PIL import Image
import numpy as np

# Evaluation imports
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer

logger = logging.getLogger(__name__)

class DonutDataCollator:
    """–ö–∞—Å—Ç–æ–º–Ω—ã–π data collator –¥–ª—è Donut –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, processor, max_length=512):
        self.processor = processor
        self.max_length = max_length
        
    def __call__(self, batch):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Donut"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç—ã
        images = [item['image'] for item in batch]
        texts = [item['text'] for item in batch]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        pixel_values = self.processor(
            images, 
            return_tensors="pt"
        ).pixel_values
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        labels = self.processor.tokenizer(
            texts,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).input_ids
        
        # –ó–∞–º–µ–Ω—è–µ–º padding —Ç–æ–∫–µ–Ω—ã –Ω–∞ -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ loss
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        return {
            'pixel_values': pixel_values,
            'labels': labels
        }

class DonutMetricsCallback(TrainerCallback):
    """Callback –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ Donut –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, processor, eval_dataset, log_callback=None):
        self.processor = processor
        self.eval_dataset = eval_dataset
        self.log_callback = log_callback
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
        
    def on_evaluate(self, args, state, control, model, **kwargs):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ—Ü–µ–Ω–∫–∏"""
        try:
            # –ë–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
            eval_samples = self.eval_dataset.select(range(min(50, len(self.eval_dataset))))
            
            predictions = []
            references = []
            
            model.eval()
            with torch.no_grad():
                for sample in eval_samples:
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    image = sample['image']
                    target_text = sample['text']
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    pixel_values = self.processor(image, return_tensors="pt").pixel_values
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º task token –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    task_prompt = "<s_cord-v2>"
                    decoder_input_ids = self.processor.tokenizer(
                        task_prompt, 
                        add_special_tokens=False, 
                        return_tensors="pt"
                    ).input_ids
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                    outputs = model.generate(
                        pixel_values,
                        decoder_input_ids=decoder_input_ids,
                        max_length=256,
                        num_beams=1,
                        do_sample=False,
                        pad_token_id=self.processor.tokenizer.pad_token_id,
                        eos_token_id=self.processor.tokenizer.eos_token_id,
                    )
                    
                    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    pred_text = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
                    pred_text = pred_text.replace(task_prompt, "").strip()
                    
                    predictions.append(pred_text)
                    references.append(target_text)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            bleu_scores = []
            rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}
            
            for pred, ref in zip(predictions, references):
                # BLEU score
                bleu = sentence_bleu([ref.split()], pred.split())
                bleu_scores.append(bleu)
                
                # ROUGE scores
                rouge = self.rouge_scorer.score(ref, pred)
                rouge_scores['rouge1'].append(rouge['rouge1'].fmeasure)
                rouge_scores['rouge2'].append(rouge['rouge2'].fmeasure)
                rouge_scores['rougeL'].append(rouge['rougeL'].fmeasure)
            
            # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            avg_bleu = np.mean(bleu_scores)
            avg_rouge1 = np.mean(rouge_scores['rouge1'])
            avg_rouge2 = np.mean(rouge_scores['rouge2'])
            avg_rougeL = np.mean(rouge_scores['rougeL'])
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            min_bleu = np.min(bleu_scores) if bleu_scores else 0
            max_bleu = np.max(bleu_scores) if bleu_scores else 0
            std_bleu = np.std(bleu_scores) if bleu_scores else 0
            
            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics_msg = (
                f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ (–Ω–∞ {len(bleu_scores)} –ø—Ä–∏–º–µ—Ä–∞—Ö):\n"
                f"   BLEU: {avg_bleu:.4f} (–º–∏–Ω: {min_bleu:.4f}, –º–∞–∫—Å: {max_bleu:.4f}, œÉ: {std_bleu:.4f})\n"
                f"   ROUGE-1: {avg_rouge1:.4f}\n"
                f"   ROUGE-2: {avg_rouge2:.4f}\n"
                f"   ROUGE-L: {avg_rougeL:.4f}\n"
                f"   üìà –ö–∞—á–µ—Å—Ç–≤–æ: {'–û—Ç–ª–∏—á–Ω–æ' if avg_bleu > 0.8 else '–•–æ—Ä–æ—à–æ' if avg_bleu > 0.6 else '–£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ' if avg_bleu > 0.4 else '–¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è'}"
            )
            
            if self.log_callback:
                self.log_callback(metrics_msg)
                
            logger.info(metrics_msg)
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫: {str(e)}"
            if self.log_callback:
                self.log_callback(error_msg)
            logger.error(error_msg)

class DonutProgressCallback(TrainerCallback):
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è Donut"""
    
    def __init__(self, progress_callback=None, log_callback=None):
        self.progress_callback = progress_callback
        self.log_callback = log_callback
        self.start_time = None
        
    def on_train_begin(self, args, state, control, **kwargs):
        """–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è"""
        self.start_time = datetime.now()
        if self.log_callback:
            self.log_callback("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Donut...")
            
    def on_epoch_begin(self, args, state, control, **kwargs):
        """–ù–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏"""
        if self.log_callback:
            self.log_callback(f"üìà –≠–ø–æ—Ö–∞ {state.epoch + 1}/{args.num_train_epochs}")
            
    def on_step_end(self, args, state, control, **kwargs):
        """–ö–æ–Ω–µ—Ü —à–∞–≥–∞"""
        if self.progress_callback and state.max_steps > 0:
            progress = int((state.global_step / state.max_steps) * 100)
            self.progress_callback(progress)
            
    def on_log(self, args, state, control, logs=None, **kwargs):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        if logs and self.log_callback:
            # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            if 'loss' in logs:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–µ–Ω–¥ loss
                loss_trend = ""
                if hasattr(self, 'prev_loss'):
                    if logs['loss'] < self.prev_loss:
                        loss_trend = " ‚¨áÔ∏è"
                    elif logs['loss'] > self.prev_loss:
                        loss_trend = " ‚¨ÜÔ∏è"
                    else:
                        loss_trend = " ‚û°Ô∏è"
                self.prev_loss = logs['loss']
                
                self.log_callback(f"üìâ –®–∞–≥ {state.global_step}: Loss = {logs['loss']:.4f}{loss_trend}")
            
            if 'eval_loss' in logs:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ eval_loss
                eval_quality = ""
                if logs['eval_loss'] < 0.1:
                    eval_quality = " üü¢"
                elif logs['eval_loss'] < 0.5:
                    eval_quality = " üü°"
                else:
                    eval_quality = " üî¥"
                    
                self.log_callback(f"üìä Eval Loss = {logs['eval_loss']:.4f}{eval_quality}")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            if 'learning_rate' in logs:
                self.log_callback(f"üìà Learning Rate = {logs['learning_rate']:.2e}")
                
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
            if state.max_steps > 0:
                progress_pct = (state.global_step / state.max_steps) * 100
                remaining_steps = state.max_steps - state.global_step
                self.log_callback(f"‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress_pct:.1f}% (–æ—Å—Ç–∞–ª–æ—Å—å —à–∞–≥–æ–≤: {remaining_steps})")
                
            # –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
            if hasattr(self, 'start_time'):
                elapsed = datetime.now() - self.start_time
                elapsed_str = str(elapsed).split('.')[0]  # –£–±–∏—Ä–∞–µ–º –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã
                self.log_callback(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {elapsed_str}")
            
            if 'grad_norm' in logs:
                self.log_callback(f"üîÑ Gradient Norm = {logs['grad_norm']:.4f}")
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å —ç–ø–æ—Ö–∏
            if state.max_steps > 0:
                epoch_progress = (state.global_step % (state.max_steps // args.num_train_epochs)) / (state.max_steps // args.num_train_epochs) * 100
                self.log_callback(f"‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å —ç–ø–æ—Ö–∏: {epoch_progress:.1f}%")
            
            # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏
            if self.start_time and state.global_step > 0:
                elapsed = datetime.now() - self.start_time
                steps_per_second = state.global_step / elapsed.total_seconds()
                remaining_steps = state.max_steps - state.global_step
                eta = remaining_steps / steps_per_second if steps_per_second > 0 else 0
                eta_str = str(datetime.timedelta(seconds=int(eta)))
                self.log_callback(f"‚è±Ô∏è ETA: {eta_str} (—Å–∫–æ—Ä–æ—Å—Ç—å: {steps_per_second:.2f} —à–∞–≥–æ–≤/—Å–µ–∫)")
                
    def on_train_end(self, args, state, control, **kwargs):
        """–ö–æ–Ω–µ—Ü –æ–±—É—á–µ–Ω–∏—è"""
        if self.start_time and self.log_callback:
            duration = datetime.now() - self.start_time
            self.log_callback(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {duration}")

class DonutTrainer:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ Donut"""
    
    def __init__(self, app_config):
        self.app_config = app_config
        self.logger = logging.getLogger("DonutTrainer")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"DonutTrainer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # Callbacks
        self.progress_callback = None
        self.log_callback = None
        self.stop_requested = False
        
    def set_callbacks(self, log_callback=None, progress_callback=None):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞"""
        self.log_callback = log_callback
        self.progress_callback = progress_callback
        
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.stop_requested = True
        if self.log_callback:
            self.log_callback("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
            
    def _log(self, message):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π callback"""
        self.logger.info(message)
        if self.log_callback:
            self.log_callback(message)
            
    def prepare_dataset(self, dataset_path: str, task_type: str = "document_parsing") -> DatasetDict:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Donut
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (document_parsing, document_vqa, etc.)
            
        Returns:
            DatasetDict: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        """
        self._log(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_path}")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            if os.path.exists(os.path.join(dataset_path, "dataset_dict.json")):
                # Hugging Face DatasetDict
                dataset = load_from_disk(dataset_path)
                self._log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω HF DatasetDict —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏: {list(dataset.keys())}")
            else:
                # –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                dataset = self._convert_custom_dataset(dataset_path, task_type)
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            self._validate_dataset(dataset)
            
            return dataset
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}"
            self._log(error_msg)
            raise RuntimeError(error_msg)
            
    def _convert_custom_dataset(self, dataset_path: str, task_type: str) -> DatasetDict:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è Donut"""
        self._log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        images_dir = os.path.join(dataset_path, "images")
        annotations_file = os.path.join(dataset_path, "annotations.json")
        
        if not os.path.exists(images_dir):
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_dir}")
            
        if not os.path.exists(annotations_file):
            raise FileNotFoundError(f"–§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {annotations_file}")
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        with open(annotations_file, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
            
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = []
        for ann in annotations:
            image_path = os.path.join(images_dir, ann['image'])
            if os.path.exists(image_path):
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image = Image.open(image_path).convert('RGB')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
                if task_type == "document_parsing":
                    # –î–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ñ–æ—Ä–º–∏—Ä—É–µ–º JSON-–ø–æ–¥–æ–±–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                    target_text = self._format_parsing_target(ann.get('fields', {}))
                elif task_type == "document_vqa":
                    # –î–ª—è VQA —Ñ–æ—Ä–º–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
                    target_text = self._format_vqa_target(ann.get('qa_pairs', []))
                else:
                    target_text = json.dumps(ann.get('fields', {}), ensure_ascii=False)
                    
                data.append({
                    'image': image,
                    'text': target_text
                })
                
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation
        train_size = int(0.8 * len(data))
        train_data = data[:train_size]
        val_data = data[train_size:]
        
        # –°–æ–∑–¥–∞–µ–º DatasetDict
        dataset_dict = DatasetDict({
            'train': Dataset.from_list(train_data),
            'validation': Dataset.from_list(val_data)
        })
        
        self._log(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {len(train_data)} train, {len(val_data)} validation")
        
        return dataset_dict
        
    def _format_parsing_target(self, fields: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ–ª—è –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è Donut
        formatted_fields = []
        for key, value in fields.items():
            if value:  # –¢–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ –ø–æ–ª—è
                formatted_fields.append(f"<s_{key}>{value}</s_{key}>")
                
        return "".join(formatted_fields)
        
    def _format_vqa_target(self, qa_pairs: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç –¥–ª—è VQA"""
        formatted_pairs = []
        for qa in qa_pairs:
            question = qa.get('question', '')
            answer = qa.get('answer', '')
            if question and answer:
                formatted_pairs.append(f"<s_question>{question}</s_question><s_answer>{answer}</s_answer>")
                
        return "".join(formatted_pairs)
        
    def _validate_dataset(self, dataset: DatasetDict):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        required_splits = ['train']
        for split in required_splits:
            if split not in dataset:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª: {split}")
                
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        train_dataset = dataset['train']
        required_columns = ['image', 'text']
        
        for col in required_columns:
            if col not in train_dataset.column_names:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞: {col}")
                
        self._log(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≤–∞–ª–∏–¥–µ–Ω: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
    def train_donut(self, 
                   dataset_path: str,
                   base_model_id: str,
                   training_args: dict,
                   output_model_name: str) -> Optional[str]:
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Donut
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            base_model_id: ID –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            training_args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            output_model_name: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
            
        Returns:
            str: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            self._log("üç© ========== –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø DONUT ==========")
            self._log(f"üìä –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
            self._log(f"ü§ñ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model_id}")
            self._log(f"üíæ –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏: {output_model_name}")
            self._log(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            self._log("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
            for key, value in training_args.items():
                self._log(f"   {key}: {value}")
            
            # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("\nüìä ===== –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê =====")
            task_type = training_args.get('task_type', 'document_parsing')
            self._log(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
            
            dataset = self.prepare_dataset(dataset_path, task_type)
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            self._log("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
            for split_name, split_data in dataset.items():
                self._log(f"   {split_name}: {len(split_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                if len(split_data) > 0:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
                    example = split_data[0]
                    self._log(f"   –ü—Ä–∏–º–µ—Ä {split_name}:")
                    if 'image' in example:
                        img_size = example['image'].size if hasattr(example['image'], 'size') else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
                        self._log(f"     –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {img_size}")
                    if 'text' in example:
                        text_preview = example['text'][:100] + "..." if len(example['text']) > 100 else example['text']
                        self._log(f"     –¢–µ–∫—Å—Ç: {text_preview}")
            
            if self.stop_requested:
                self._log("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
                return None
                
            # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self._log("\nü§ñ ===== –≠–¢–ê–ü 2: –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò =====")
            self._log(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏–∑: {base_model_id}")
            
            cache_dir = os.path.join(self.app_config.MODELS_PATH, 'donut_cache')
            self._log(f"üíæ –ö—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_dir}")
            
            processor = DonutProcessor.from_pretrained(
                base_model_id,
                cache_dir=cache_dir
            )
            self._log("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            self._log(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {base_model_id}")
            model = VisionEncoderDecoderModel.from_pretrained(
                base_model_id,
                cache_dir=cache_dir
            )
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            self._log(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
            self._log(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
            self._log(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self._log(f"üîÑ –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            model.to(self.device)
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")
            
            if self.stop_requested:
                self._log("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")
                return None
                
            # 3. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            self._log("\n‚öôÔ∏è ===== –≠–¢–ê–ü 3: –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ò =====")
            self._configure_model_for_training(model, processor, training_args)
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # 4. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º data collator
            self._log("\nüîß ===== –≠–¢–ê–ü 4: –ü–û–î–ì–û–¢–û–í–ö–ê DATA COLLATOR =====")
            max_length = training_args.get('max_length', 512)
            self._log(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
            data_collator = DonutDataCollator(processor, max_length)
            self._log("‚úÖ Data collator —Å–æ–∑–¥–∞–Ω")
            
            # 5. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            self._log("\nüìã ===== –≠–¢–ê–ü 5: –ù–ê–°–¢–†–û–ô–ö–ê –ê–†–ì–£–ú–ï–ù–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø =====")
            output_dir = os.path.join(
                self.app_config.TRAINED_MODELS_PATH,
                f"donut_{output_model_name}"
            )
            os.makedirs(output_dir, exist_ok=True)
            self._log(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            train_args = self._create_training_arguments(training_args, output_dir)
            self._log("‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            self._log("üìä –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            self._log(f"   –≠–ø–æ—Ö: {train_args.num_train_epochs}")
            self._log(f"   Batch size (train): {train_args.per_device_train_batch_size}")
            self._log(f"   Batch size (eval): {train_args.per_device_eval_batch_size}")
            self._log(f"   Learning rate: {train_args.learning_rate}")
            self._log(f"   Gradient accumulation: {train_args.gradient_accumulation_steps}")
            self._log(f"   FP16: {getattr(train_args, 'fp16', False)}")
            
            # 6. –°–æ–∑–¥–∞–µ–º callbacks
            self._log("\nüîî ===== –≠–¢–ê–ü 6: –°–û–ó–î–ê–ù–ò–ï CALLBACKS =====")
            callbacks = self._create_callbacks(processor, dataset.get('validation'))
            self._log(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ callbacks: {len(callbacks)}")
            for i, callback in enumerate(callbacks):
                self._log(f"   {i+1}. {callback.__class__.__name__}")
            
            # 7. –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
            self._log("\nüèÉ ===== –≠–¢–ê–ü 7: –°–û–ó–î–ê–ù–ò–ï TRAINER =====")
            trainer = Trainer(
                model=model,
                args=train_args,
                train_dataset=dataset['train'],
                eval_dataset=dataset.get('validation'),
                data_collator=data_collator,
                callbacks=callbacks,
            )
            self._log("‚úÖ Trainer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
            train_dataset_size = len(dataset['train'])
            batch_size = train_args.per_device_train_batch_size
            grad_accum = train_args.gradient_accumulation_steps
            effective_batch_size = batch_size * grad_accum
            steps_per_epoch = train_dataset_size // effective_batch_size
            total_steps = steps_per_epoch * train_args.num_train_epochs
            
            self._log(f"üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏:")
            self._log(f"   üìÑ –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {train_dataset_size}")
            self._log(f"   üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch_size}")
            self._log(f"   üîÑ Gradient accumulation: {grad_accum}")
            self._log(f"   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {effective_batch_size}")
            self._log(f"   üî¢ –®–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É: {steps_per_epoch}")
            self._log(f"   üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {total_steps}")
            self._log(f"   ‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è (–ø—Ä–∏ 1 —Å–µ–∫/—à–∞–≥): {total_steps // 60} –º–∏–Ω")
            
            if self.stop_requested:
                self._log("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ–∑–¥–∞–Ω–∏—è trainer")
                return None
                
            # 8. –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            self._log("\nüöÄ ===== –≠–¢–ê–ü 8: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø =====")
            self._log("üéØ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –º–æ–¥–µ–ª–∏...")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            class StopTrainingCallback(TrainerCallback):
                def __init__(self, donut_trainer):
                    self.donut_trainer = donut_trainer
                    
                def on_step_end(self, args, state, control, **kwargs):
                    if self.donut_trainer.stop_requested:
                        control.should_training_stop = True
                        
            trainer.add_callback(StopTrainingCallback(self))
            
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è
            start_time = datetime.now()
            self._log(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%H:%M:%S')}")
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            training_result = trainer.train()
            
            if self.stop_requested:
                self._log("‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                return None
                
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            end_time = datetime.now()
            duration = end_time - start_time
            self._log(f"‚è∞ –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è: {end_time.strftime('%H:%M:%S')}")
            self._log(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {duration}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
            if hasattr(training_result, 'training_loss'):
                final_loss = training_result.training_loss
                self._log(f"üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π training loss: {final_loss:.4f}")
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ loss
                if final_loss < 0.1:
                    loss_quality = "üü¢ –û—Ç–ª–∏—á–Ω—ã–π"
                elif final_loss < 0.5:
                    loss_quality = "üü° –•–æ—Ä–æ—à–∏–π"
                elif final_loss < 1.0:
                    loss_quality = "üü† –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π"
                else:
                    loss_quality = "üî¥ –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
                    
                self._log(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è: {loss_quality}")
                
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if hasattr(training_result, 'global_step') and training_result.global_step > 0:
                total_seconds = duration.total_seconds()
                steps_per_second = training_result.global_step / total_seconds
                seconds_per_step = total_seconds / training_result.global_step
                
                self._log(f"‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
                self._log(f"   üèÉ –®–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {steps_per_second:.2f}")
                self._log(f"   ‚è±Ô∏è –°–µ–∫—É–Ω–¥ –Ω–∞ —à–∞–≥: {seconds_per_step:.2f}")
                
                if seconds_per_step < 1:
                    perf_rating = "üöÄ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ"
                elif seconds_per_step < 5:
                    perf_rating = "‚ö° –ë—ã—Å—Ç—Ä–æ"
                elif seconds_per_step < 10:
                    perf_rating = "üêé –ù–æ—Ä–º–∞–ª—å–Ω–æ"
                else:
                    perf_rating = "üêå –ú–µ–¥–ª–µ–Ω–Ω–æ"
                    
                self._log(f"   üìà –û—Ü–µ–Ω–∫–∞: {perf_rating}")
                
            # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self._log("\nüíæ ===== –≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====")
            self._log(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {output_dir}")
            
            trainer.save_model(output_dir)
            self._log("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
            
            processor.save_pretrained(output_dir)
            self._log("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'base_model': base_model_id,
                'task_type': task_type,
                'training_args': training_args,
                'created_at': datetime.now().isoformat(),
                'dataset_path': dataset_path,
                'training_duration': str(duration),
                'final_loss': getattr(training_result, 'training_loss', None),
                'total_steps': getattr(training_result, 'global_step', None)
            }
            
            metadata_path = os.path.join(output_dir, 'training_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            self._log(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
                
            self._log(f"\nüéâ ========== –û–ë–£–ß–ï–ù–ò–ï DONUT –ó–ê–í–ï–†–®–ï–ù–û ==========")
            self._log(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")
            self._log(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {duration}")
            self._log(f"üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {getattr(training_result, 'global_step', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            return output_dir
            
        except Exception as e:
            self._log(f"\nüí• ========== –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø DONUT ==========")
            error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            self._log(error_msg)
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—à–∏–±–∫–∏
            import traceback
            import sys
            
            self._log("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            self._log(f"   Python –≤–µ—Ä—Å–∏—è: {sys.version}")
            self._log(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
            self._log(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
            if torch.cuda.is_available():
                self._log(f"   CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
                self._log(f"   –¢–µ–∫—É—â–µ–µ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.current_device()}")
                self._log(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            
            self._log(f"   –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"   –î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(dataset_path) if 'dataset_path' in locals() else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}")
            self._log(f"   –ú–æ–¥–µ–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.app_config.MODELS_PATH}")
            self._log(f"   –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.app_config.TRAINED_MODELS_PATH}")
            
            # –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏
            self._log("\nüîç –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
            full_traceback = traceback.format_exc()
            for line in full_traceback.split('\n'):
                if line.strip():
                    self._log(f"   {line}")
            
            self.logger.error(f"DonutTrainer error: {error_msg}")
            self.logger.error(full_traceback)
            
            return None
            
    def _configure_model_for_training(self, model, processor, training_args):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        self._log("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_size = training_args.get('image_size', 384)
        self._log(f"   üñºÔ∏è –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_size}x{image_size}")
        if hasattr(model.config, 'encoder') and hasattr(model.config.encoder, 'image_size'):
            model.config.encoder.image_size = [image_size, image_size]
            self._log("   ‚úÖ –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ encoder")
        else:
            self._log("   ‚ö†Ô∏è Encoder –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
        max_length = training_args.get('max_length', 512)
        self._log(f"   üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
        if hasattr(model.config, 'decoder') and hasattr(model.config.decoder, 'max_length'):
            model.config.decoder.max_length = max_length
            self._log("   ‚úÖ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –≤ decoder")
        else:
            self._log("   ‚ö†Ô∏è Decoder –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã")
            
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        self._log("   üè∑Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
        
        pad_token_id = processor.tokenizer.pad_token_id
        model.config.pad_token_id = pad_token_id
        self._log(f"     pad_token_id: {pad_token_id}")
        
        eos_token_id = processor.tokenizer.eos_token_id
        model.config.eos_token_id = eos_token_id
        self._log(f"     eos_token_id: {eos_token_id}")
        
        bos_token_id = processor.tokenizer.bos_token_id
        model.config.bos_token_id = bos_token_id
        self._log(f"     bos_token_id: {bos_token_id}")
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–º–µ—Ä–µ —Å–ª–æ–≤–∞—Ä—è
        vocab_size = len(processor.tokenizer)
        self._log(f"   üìö –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {vocab_size}")
        
        # –í–∫–ª—é—á–∞–µ–º gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
        gradient_checkpointing = training_args.get('gradient_checkpointing', True)
        self._log(f"   üíæ Gradient checkpointing: {gradient_checkpointing}")
        if gradient_checkpointing:
            try:
                model.gradient_checkpointing_enable()
                self._log("   ‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω")
            except Exception as e:
                self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self._log("   üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
        if hasattr(model.config, 'vocab_size'):
            model_vocab_size = model.config.vocab_size
            self._log(f"     –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏: {model_vocab_size}")
            if model_vocab_size != vocab_size:
                self._log(f"     ‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ —Å–ª–æ–≤–∞—Ä–µ–π!")
        
        self._log("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    def _create_training_arguments(self, training_args: dict, output_dir: str) -> TrainingArguments:
        """–°–æ–∑–¥–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ë–∞–∑–æ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
        args = {
            'output_dir': output_dir,
            'num_train_epochs': training_args.get('num_train_epochs', 5),
            'per_device_train_batch_size': training_args.get('per_device_train_batch_size', 2),
            'per_device_eval_batch_size': training_args.get('per_device_eval_batch_size', 2),
            'gradient_accumulation_steps': training_args.get('gradient_accumulation_steps', 4),
            'learning_rate': training_args.get('learning_rate', 3e-5),
            'weight_decay': training_args.get('weight_decay', 0.01),
            'warmup_ratio': training_args.get('warmup_ratio', 0.1),
            'logging_steps': 10,
            'save_steps': training_args.get('save_steps', 500),
            'eval_steps': training_args.get('eval_steps', 500),
            'eval_strategy': 'steps',  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –∏—Å–ø–æ–ª—å–∑—É—é –Ω–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä
            'save_strategy': 'steps',
            'load_best_model_at_end': True,
            'metric_for_best_model': 'eval_loss',
            'greater_is_better': False,
            'save_total_limit': 3,
            'report_to': 'none',
            'remove_unused_columns': False,
            'dataloader_pin_memory': False,
        }
        
        # FP16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        if training_args.get('fp16', True) and torch.cuda.is_available():
            args['fp16'] = True
            
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è GPU
        if torch.cuda.is_available():
            args['dataloader_num_workers'] = 2
        else:
            args['dataloader_num_workers'] = 0
            
        return TrainingArguments(**args)
        
    def _create_callbacks(self, processor, eval_dataset) -> List[TrainerCallback]:
        """–°–æ–∑–¥–∞–µ—Ç callbacks –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        callbacks = []
        
        # Progress callback
        if self.progress_callback or self.log_callback:
            callbacks.append(DonutProgressCallback(
                progress_callback=self.progress_callback,
                log_callback=self.log_callback
            ))
            
        # Metrics callback
        if eval_dataset:
            callbacks.append(DonutMetricsCallback(
                processor=processor,
                eval_dataset=eval_dataset,
                log_callback=self.log_callback
            ))
            
        # Early stopping
        callbacks.append(EarlyStoppingCallback(
            early_stopping_patience=3,
            early_stopping_threshold=0.001
        ))
        
        return callbacks 