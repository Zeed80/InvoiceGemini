"""
TrOCR Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Microsoft TrOCR –º–æ–¥–µ–ª–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: LoRA, 8-bit optimizer, gradient checkpointing
"""

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è TrOCR –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
from transformers import (
    TrOCRProcessor, VisionEncoderDecoderModel, 
    TrainingArguments, Trainer, EarlyStoppingCallback
)
from datasets import load_from_disk, Dataset, DatasetDict, Features, Value, Image as DatasetImage
from datetime import datetime
import json
import os
import sys
import logging
import torch
import re
from collections import defaultdict
from pathlib import Path
from PIL import Image
from typing import Dict, List, Optional, Union, Any
import math
import tempfile

# LoRA imports
try:
    from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
    from peft import PeftModel, PeftConfig
    LORA_AVAILABLE = True
except ImportError:
    LORA_AVAILABLE = False
    
# 8-bit optimizer imports  
try:
    import bitsandbytes as bnb
    BITSANDBYTES_AVAILABLE = True
except ImportError:
    BITSANDBYTES_AVAILABLE = False

# –í –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞ –ø–æ—Å–ª–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–º–ø–æ—Ä—Ç–æ–≤ –¥–æ–±–∞–≤–ª—è—é:
from .core.base_lora_trainer import BaseLor–∞Trainer, ModelType

class SafeTrOCRModel(torch.nn.Module):
    """
    –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è TrOCR –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    –º–µ–∂–¥—É encoder –∏ decoder –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –æ—à–∏–±–∫–∏ 'input_ids' –≤ ViT encoder
    """
    
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        
        # –ö–æ–ø–∏—Ä—É–µ–º –≤–∞–∂–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –∏–∑ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        self.config = base_model.config
        self.encoder = base_model.encoder
        self.decoder = base_model.decoder
        
    def forward(self, pixel_values=None, labels=None, **kwargs):
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π forward pass –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–¥–µ–ª—è–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã:
        - pixel_values -> encoder (ViT)
        - labels -> decoder (RoBERTa)
        
        –ù–ï –ø–µ—Ä–µ–¥–∞–µ—Ç input_ids –≤ encoder!
        """
        
        # –ü–æ–ª—É—á–∞–µ–º base_model –ë–ï–ó–û–ü–ê–°–ù–û
        if not hasattr(self, 'base_model'):
            raise RuntimeError("SafeTrOCRModel: base_model –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü—Ä—è–º–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ base_model
        base_model = self.base_model
        
        # –°–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û —Ç–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω—ã encoder'—É (ViT)
        encoder_kwargs = {
            'pixel_values': pixel_values
        }
        
        # –£–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ–ø–∞—Å—Ç—å –≤ kwargs
        encoder_safe_kwargs = {}
        for k, v in kwargs.items():
            # –ù–ï –ø–µ—Ä–µ–¥–∞–µ–º –Ω–∏–∫–∞–∫–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è –≤ encoder
            if k not in ['input_ids', 'attention_mask', 'decoder_input_ids', 
                        'decoder_attention_mask', 'decoder_inputs_embeds',
                        'use_cache', 'output_hidden_states', 'output_attentions',
                        'past_key_values']:
                encoder_safe_kwargs[k] = v
        
        encoder_kwargs.update(encoder_safe_kwargs)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è decoder'–∞ (–≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ)
        decoder_kwargs = {}
        for k, v in kwargs.items():
            if k not in encoder_kwargs:
                decoder_kwargs[k] = v
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π forward –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        return base_model(
            pixel_values=pixel_values,
            labels=labels,
            **decoder_kwargs  # decoder_kwargs –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç pixel_values
        )
    
    def generate(self, *args, **kwargs):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º generate –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            return self.__dict__['base_model'].generate(*args, **kwargs)
        return super().generate(*args, **kwargs)
    
    def save_pretrained(self, *args, **kwargs):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º save_pretrained –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            return self.__dict__['base_model'].save_pretrained(*args, **kwargs)
        return super().save_pretrained(*args, **kwargs)
    
    def parameters(self, recurse=True):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º parameters –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            return self.__dict__['base_model'].parameters(recurse=recurse)
        return super().parameters(recurse=recurse)
    
    def named_parameters(self, prefix='', recurse=True, remove_duplicate=True):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º named_parameters –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            return self.__dict__['base_model'].named_parameters(prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)
        return super().named_parameters(prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)
    
    def train(self, mode=True):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º train –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            self.__dict__['base_model'].train(mode)
        return super().train(mode)
    
    def eval(self):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º eval –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            self.__dict__['base_model'].eval()
        return super().eval()
    
    def to(self, device):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º to –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        # –ü—Ä—è–º–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ base_model —á–µ—Ä–µ–∑ __dict__ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Ä–µ–∫—É—Ä—Å–∏–∏
        if 'base_model' in self.__dict__:
            self.__dict__['base_model'] = self.__dict__['base_model'].to(device)
        return super().to(device)
    
    def cuda(self):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º cuda –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            self.__dict__['base_model'] = self.__dict__['base_model'].cuda()
        return super().cuda()
    
    def cpu(self):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º cpu –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        if 'base_model' in self.__dict__:
            self.__dict__['base_model'] = self.__dict__['base_model'].cpu()
        return super().cpu()
    
    def __getattr__(self, name):
        """–ü—Ä–æ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –≤ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        # –ò–∑–±–µ–≥–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–∏ –¥–ª—è base_model
        if name == 'base_model':
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
        
        # –ò–∑–±–µ–≥–∞–µ–º —Ä–µ–∫—É—Ä—Å–∏–∏, –ø–æ–ª—É—á–∞—è base_model –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ __dict__
        if 'base_model' in self.__dict__:
            return getattr(self.__dict__['base_model'], name)
        else:
            raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")

class TrOCRDataCollator:
    """Data collator –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, processor, max_length=512):
        self.processor = processor
        self.max_length = max_length
        
    def __call__(self, batch):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TrOCR –æ–±—É—á–µ–Ω–∏—è
        
        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: TrOCR –º–æ–¥–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
        - Vision Encoder (ViT) - –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¢–û–õ–¨–ö–û pixel_values
        - Text Decoder (RoBERTa) - –ø—Ä–∏–Ω–∏–º–∞–µ—Ç decoder_input_ids, labels
        
        –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—Ç—å input_ids –≤ encoder, –±—É–¥–µ—Ç –æ—à–∏–±–∫–∞!
        """
        
        # –î–ï–¢–ê–õ–¨–ù–´–ô DEBUG: —á—Ç–æ –ø—Ä–∏—Ö–æ–¥–∏—Ç –≤ –±–∞—Ç—á –æ—Ç dataset
        print(f"[DEBUG TrOCRDataCollator] –ü–æ–ª—É—á–µ–Ω batch –∏–∑ {len(batch)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        for i, item in enumerate(batch[:2]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 2 —ç–ª–µ–º–µ–Ω—Ç–∞
            print(f"[DEBUG] –≠–ª–µ–º–µ–Ω—Ç {i}: –∫–ª—é—á–∏={list(item.keys())}")
            for key, value in item.items():
                if hasattr(value, 'shape'):
                    print(f"[DEBUG]   {key}: shape={value.shape}, dtype={value.dtype}")
                elif isinstance(value, (str, int, float)):
                    print(f"[DEBUG]   {key}: {type(value).__name__}={value}")
                else:
                    print(f"[DEBUG]   {key}: type={type(value)}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç—ã –∏–∑ –±–∞—Ç—á–∞
        images = []
        texts = []
        
        for item in batch:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            if 'image' not in item:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'image' –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –±–∞—Ç—á–∞")
            if 'text' not in item:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'text' –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –±–∞—Ç—á–∞")
                
            images.append(item['image'])
            texts.append(item['text'])
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è encoder (ViT)
        # ViT –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¢–û–õ–¨–ö–û pixel_values, –Ω–∏–∫–∞–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤!
        encoding = self.processor(
            images, 
            return_tensors="pt"
        )
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è decoder (RoBERTa)
        target_encoding = self.processor.tokenizer(
            texts,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º labels –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        labels = target_encoding.input_ids.clone()
        
        # –ó–∞–º–µ–Ω—è–µ–º padding —Ç–æ–∫–µ–Ω—ã –Ω–∞ -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ loss
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û –¥–ª—è TrOCR: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¢–û–õ–¨–ö–û –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è!
        # TrOCR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç VisionEncoderDecoderModel:
        # - pixel_values -> encoder (ViT) 
        # - labels -> decoder (–¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss)
        # 
        # –ù–ï –í–ö–õ–Æ–ß–ê–ï–ú:
        # - input_ids (–≤—ã–∑—ã–≤–∞–µ—Ç –æ—à–∏–±–∫—É –≤ ViT encoder!)
        # - attention_mask 
        # - decoder_input_ids
        # - decoder_attention_mask
        
        result = {
            'pixel_values': encoding.pixel_values,  # ViT encoder
            'labels': labels                        # Decoder loss
        }
        
        # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –ù–ï–¢ –ª–∏—à–Ω–∏—Ö –ø–æ–ª–µ–π
        forbidden_keys = ['input_ids', 'attention_mask', 'decoder_input_ids', 
                         'decoder_attention_mask', 'decoder_inputs_embeds']
        
        for key in forbidden_keys:
            if key in result:
                del result[key]
                print(f"[WARNING] –£–¥–∞–ª–µ–Ω –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –∫–ª—é—á: {key}")
        
        # DEBUG: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –ø–µ—Ä–µ–¥–∞–µ–º –≤ –º–æ–¥–µ–ª—å
        print(f"[DEBUG TrOCRDataCollator] –ü–µ—Ä–µ–¥–∞–µ–º –≤ –º–æ–¥–µ–ª—å –∫–ª—é—á–∏: {list(result.keys())}")
        print(f"[DEBUG] pixel_values shape: {result['pixel_values'].shape}")
        print(f"[DEBUG] labels shape: {result['labels'].shape}")
        
        return result

class TrOCRProgressCallback:
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, progress_callback=None):
        self.progress_callback = progress_callback
    
    def on_init_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ trainer"""
        return control
    
    def on_train_begin(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è"""
        return control
    
    def on_train_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è"""
        return control
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —ç–ø–æ—Ö–∏"""
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏"""
        return control
        
    def on_step_begin(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —à–∞–≥–∞"""
        return control
        
    def on_step_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —à–∞–≥–∞"""
        if self.progress_callback and state.max_steps > 0:
            progress = int((state.global_step / state.max_steps) * 100)
            self.progress_callback(progress)
        return control
    
    def on_log(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏"""
        return control
        
    def on_substep_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –ø–æ–¥—à–∞–≥–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_pre_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_save(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control

class TrOCRMetricsCallback:
    """Callback –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, metrics_callback=None):
        self.metrics_callback = metrics_callback
    
    def on_init_end(self, args, state, control, **kwargs):
        return control
    
    def on_train_begin(self, args, state, control, **kwargs):
        return control
    
    def on_train_end(self, args, state, control, **kwargs):
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        if self.metrics_callback:
            final_message = f"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û | –≠–ø–æ—Ö: {state.epoch:.1f} | –®–∞–≥–æ–≤: {state.global_step}"
            self.metrics_callback(final_message)
        return control
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        return control
    
    def on_step_begin(self, args, state, control, **kwargs):
        return control
    
    def on_step_end(self, args, state, control, **kwargs):
        return control
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if self.metrics_callback and logs:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è UI
            formatted_message = self._format_metrics_message(logs, state)
            self.metrics_callback(formatted_message)
        return control
    
    def _format_metrics_message(self, logs, state):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI"""
        try:
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            epoch = getattr(state, 'epoch', 0)
            step = getattr(state, 'global_step', 0)
            max_steps = getattr(state, 'max_steps', 1)
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            progress_percent = int((step / max_steps) * 100) if max_steps > 0 else 0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            message_parts = []
            message_parts.append(f"üìä –≠–ø–æ—Ö–∞ {epoch:.1f}, –®–∞–≥ {step}/{max_steps} ({progress_percent}%)")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
            if 'train_loss' in logs:
                loss = logs['train_loss']
                message_parts.append(f"üìâ Loss: {loss:.4f}")
            
            if 'learning_rate' in logs:
                lr = logs['learning_rate']
                message_parts.append(f"üìà LR: {lr:.2e}")
                
            if 'train_runtime' in logs:
                runtime = logs['train_runtime']
                message_parts.append(f"‚è±Ô∏è –í—Ä–µ–º—è: {runtime:.1f}s")
                
            if 'train_samples_per_second' in logs:
                sps = logs['train_samples_per_second']
                message_parts.append(f"üöÄ {sps:.2f} samples/sec")
            
            # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ GPU
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / (1024**3)
                message_parts.append(f"üíæ GPU: {memory_used:.1f}GB")
            
            return " | ".join(message_parts)
            
        except Exception as e:
            return f"üìä –®–∞–≥ {step}: –º–µ—Ç—Ä–∏–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã (–æ—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e})"
        
    def on_substep_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –ø–æ–¥—à–∞–≥–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_pre_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_save(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control

class TrOCRGPUMonitorCallback:
    """Callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ GPU –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, logger_func=None):
        self._log = logger_func or print
    
    def on_init_end(self, args, state, control, **kwargs):
        return control
    
    def on_train_begin(self, args, state, control, **kwargs):
        return control
    
    def on_train_end(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        return control
        
    def on_step_begin(self, args, state, control, **kwargs):
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            
            if state.global_step % 5 == 0:  # –ö–∞–∂–¥—ã–µ 5 —à–∞–≥–æ–≤
                self._log(f"   üìä –®–∞–≥ {state.global_step}: GPU –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {allocated:.2f}GB / {reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
        return control
    
    def on_step_end(self, args, state, control, **kwargs):
        return control
    
    def on_log(self, args, state, control, **kwargs):
        return control
        
    def on_substep_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –ø–æ–¥—à–∞–≥–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_pre_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_save(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control

class TrOCRTrainer(BaseLor–∞Trainer):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è TrOCR –º–æ–¥–µ–ª–µ–π —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –±–∞–∑–æ–≤–æ–≥–æ LoRA –∫–ª–∞—Å—Å–∞
    –£—Å—Ç—Ä–∞–Ω—è–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ LoRA –∫–æ–¥–∞, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
    """
    
    def __init__(self, device: str = "auto", logger: logging.Logger = None):
        super().__init__(ModelType.TROCR, logger)
        
        if device == "auto":
            device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.device = torch.device(device)
        self.progress_callback = None
        self.metrics_callback = None
        self.status_callback = None
        self._stop_training = False
    
    def _log(self, message: str, level: str = "info"):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Unicode"""
        # –°–æ–∑–¥–∞–µ–º ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –≤–µ—Ä—Å–∏—é —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Windows console
        safe_message = message.encode('ascii', errors='replace').decode('ascii')
        
        if self.logger:
            # –õ–æ–≥–≥–µ—Ä –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å Unicode, –Ω–æ Windows console –º–æ–∂–µ—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å emoji
            try:
                getattr(self.logger, level)(message)
            except UnicodeEncodeError:
                # Fallback –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                getattr(self.logger, level)(safe_message)
            except Exception:
                # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π - –µ—â–µ –æ–¥–∏–Ω fallback
                getattr(self.logger, level)(safe_message)
        else:
            print(f"[{level.upper()}] {safe_message}")
    
    def set_callbacks(self, progress_callback=None, metrics_callback=None, status_callback=None):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç callback —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.progress_callback = progress_callback
        self.metrics_callback = metrics_callback  
        self.status_callback = status_callback
    
    def _apply_memory_optimizations(self, model, training_args: dict):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        model = self.apply_memory_optimizations(model, training_args)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º TrOCR-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        model = self._apply_model_specific_optimizations(model, training_args)
        
        return model
    
    def _setup_cuda_optimizations(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM"""
        if not torch.cuda.is_available():
            return
            
        self._log("üßπ === –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –û–ß–ò–°–¢–ö–ê CUDA –ü–ê–ú–Ø–¢–ò ===")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        import os
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        self._log("   ‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
        
        # –û—á–∏—â–∞–µ–º CUDA –∫—ç—à
        torch.cuda.empty_cache()
        self._log("   üßπ –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫—ç—à–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏
        torch.cuda.reset_peak_memory_stats()
        self._log("   üìä –°–±—Ä–æ—à–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ CUDA")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏
        torch.cuda.set_per_process_memory_fraction(0.95)
        self._log("   üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: 95% –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏
        allocated = torch.cuda.memory_allocated() / (1024**3)
        reserved = torch.cuda.memory_reserved() / (1024**3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        free = total - reserved
        
        self._log("   üíæ –ü–∞–º—è—Ç—å GPU:")
        self._log(f"      –í—ã–¥–µ–ª–µ–Ω–æ: {allocated:.2f} GB")
        self._log(f"      –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {reserved:.2f} GB") 
        self._log(f"      –°–≤–æ–±–æ–¥–Ω–æ: {free:.2f} GB")
    
    def convert_dataset_to_trocr_format(self, dataset_path: str) -> Dataset:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ InvoiceGemini –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR
        """
        self._log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        dataset = load_from_disk(dataset_path)
        
        def convert_item(item):
            """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞
            
            –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¢–û–õ–¨–ö–û –Ω—É–∂–Ω—ã–µ –ø–æ–ª—è!
            Trainer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –í–°–ï –ø–æ–ª—è –≤ batch,
            —á—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø–µ—Ä–µ–¥–∞—á—É input_ids –≤ ViT encoder.
            """
            # TrOCR –æ–∂–∏–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ text (–∞ –Ω–µ ground_truth)
            return {
                'image': item['image'],
                'text': item.get('ground_truth', item.get('text', ''))
                # –ù–ï –î–û–ë–ê–í–õ–Ø–ï–ú: input_ids, attention_mask, decoder_*
                # –≠—Ç–∏ –ø–æ–ª—è –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã –≤ data collator
            }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –∏ –£–î–ê–õ–Ø–ï–ú –≤—Å–µ –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏
        if isinstance(dataset, DatasetDict):
            converted_dataset = DatasetDict()
            for split_name, split_dataset in dataset.items():
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                split_converted = split_dataset.map(convert_item)
                
                # –Ø–í–ù–û —É–¥–∞–ª—è–µ–º –≤—Å–µ –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —á—Ç–æ –º–æ–≥–ª–∏ –æ—Å—Ç–∞—Ç—å—Å—è
                columns_to_remove = []
                for col in split_converted.column_names:
                    if col not in ['image', 'text']:
                        columns_to_remove.append(col)
                
                if columns_to_remove:
                    split_converted = split_converted.remove_columns(columns_to_remove)
                    self._log(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω—ã –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ {split_name}: {columns_to_remove}")
                
                converted_dataset[split_name] = split_converted
        else:
            converted_dataset = dataset.map(convert_item)
            
            # –Ø–í–ù–û —É–¥–∞–ª—è–µ–º –≤—Å–µ –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —á—Ç–æ –º–æ–≥–ª–∏ –æ—Å—Ç–∞—Ç—å—Å—è
            columns_to_remove = []
            for col in converted_dataset.column_names:
                if col not in ['image', 'text']:
                    columns_to_remove.append(col)
            
            if columns_to_remove:
                converted_dataset = converted_dataset.remove_columns(columns_to_remove)
                self._log(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω—ã –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏: {columns_to_remove}")
        
        train_size = len(converted_dataset['train']) if 'train' in converted_dataset else len(converted_dataset)
        val_size = len(converted_dataset['validation']) if 'validation' in converted_dataset else 0
        
        self._log(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {train_size} train, {val_size} validation")
        self._log(f"üìä –ò—Ç–æ–≥–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {list(converted_dataset['train'].column_names) if 'train' in converted_dataset else list(converted_dataset.column_names)}")
        
        return converted_dataset
    
    def train_trocr(self, 
                   dataset_path: str,
                   base_model_id: str,
                   training_args: dict,
                   output_model_name: str) -> Optional[str]:
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å TrOCR –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
            self._setup_cuda_optimizations()
            
            self._log("")
            self._log("ü§ñ ========== –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø TrOCR ==========")
            self._log("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
            self._log(f"   –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
            self._log(f"   –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model_id}")
            self._log(f"   –í—ã—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å: {output_model_name}")
            self._log(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê
            self._log("")
            self._log("üìö ===== –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê =====")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π
            self._log("üîç –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            raw_dataset = load_from_disk(dataset_path)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            if isinstance(raw_dataset, DatasetDict):
                train_size = len(raw_dataset['train']) if 'train' in raw_dataset else 0
                val_size = len(raw_dataset['validation']) if 'validation' in raw_dataset else 0
                test_size = len(raw_dataset['test']) if 'test' in raw_dataset else 0
                total_size = train_size + val_size + test_size
                
                self._log(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
                self._log(f"   üìÅ –ü—É—Ç—å: {dataset_path}")
                self._log(f"   üéØ –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {train_size}")
                self._log(f"   ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {val_size}")
                self._log(f"   üß™ –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {test_size}")
                self._log(f"   üìà –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size}")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
                if train_size > 0:
                    sample = raw_dataset['train'][0]
                    fields = list(sample.keys())
                    self._log(f"   üè∑Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è: {', '.join(fields)}")
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    if 'image' in sample:
                        img = sample['image']
                        self._log(f"   üñºÔ∏è –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {img.size}")
                        
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
                    if 'ground_truth' in sample or 'text' in sample:
                        text = sample.get('ground_truth', sample.get('text', ''))
                        self._log(f"   üìù –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: '{text[:100]}{'...' if len(text) > 100 else ''}'")
                        self._log(f"   üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: ~{len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                total_size = len(raw_dataset)
                self._log(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
                self._log(f"   üìÅ –ü—É—Ç—å: {dataset_path}")
                self._log(f"   üìà –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size}")
                
                if total_size > 0:
                    sample = raw_dataset[0]
                    fields = list(sample.keys())
                    self._log(f"   üè∑Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è: {', '.join(fields)}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR...")
            dataset = self.convert_dataset_to_trocr_format(dataset_path)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            train_dataset = dataset['train'] if 'train' in dataset else dataset
            if len(train_dataset) == 0:
                raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
            
            self._log(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            self._log("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –≠–¢–ê–ü 2: –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
            self._log("")
            self._log("ü§ñ ===== –≠–¢–ê–ü 2: –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò =====")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            cache_dir = "data/models"
            os.makedirs(cache_dir, exist_ok=True)
            self._log(f"üìÅ –ö—ç—à –º–æ–¥–µ–ª–µ–π: {os.path.abspath(cache_dir)}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self._log(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏–∑: {base_model_id}")
            processor = TrOCRProcessor.from_pretrained(
                base_model_id,
                cache_dir=cache_dir
            )
            self._log("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self._log(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {base_model_id}")
            model = VisionEncoderDecoderModel.from_pretrained(
                base_model_id,
                cache_dir=cache_dir
            )
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            self._log("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
            self._log(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
            self._log(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            model = self._apply_memory_optimizations(model, training_args)
            
            # –í–∫–ª—é—á–∞–µ–º gradient checkpointing
            gradient_checkpointing = training_args.get('gradient_checkpointing', True)
            self._log(f"   üíæ Gradient checkpointing: {gradient_checkpointing}")
            if gradient_checkpointing:
                try:
                    model.gradient_checkpointing_enable()
                    self._log("   ‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω")
                except Exception as e:
                    self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing: {e}")
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –ù–ê–ü–†–Ø–ú–£–Æ
            model.to(self.device)
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if torch.cuda.is_available():
                self._log("üöÄ === –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê GPU ===")
                torch.cuda.empty_cache()
                self._log("   üßπ –ö—ç—à CUDA –æ—á–∏—â–µ–Ω")
                
                torch.cuda.set_device(0)
                self._log("   üéØ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ 0 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ GPU
                try:
                    model_on_gpu = next(iter(model.parameters())).is_cuda
                    if model_on_gpu:
                        self._log("   ‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: –ú–æ–¥–µ–ª—å –Ω–∞ GPU!")
                    else:
                        self._log("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –ù–ï –Ω–∞ GPU!")
                except StopIteration:
                    self._log("   ‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –∏–º–µ–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU")
                
                # –í–∫–ª—é—á–∞–µ–º CUDNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                torch.backends.cudnn.benchmark = True
                self._log("   ‚ö° CUDNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å GPU
                free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()
                free_gb = free_memory / (1024**3)
                self._log(f"   üíæ –°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_gb:.1f} GB")
                
                if free_gb < 2:
                    self._log("   ‚ö†Ô∏è –ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU - –≤–æ–∑–º–æ–∂–Ω—ã OOM –æ—à–∏–±–∫–∏")
                else:
                    self._log("   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –≠–¢–ê–ü 3: –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ò
            self._log("")
            self._log("‚öôÔ∏è ===== –≠–¢–ê–ü 3: –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ò =====")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self._log("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
            
            # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_size = training_args.get('image_size', 384)
            self._log(f"   üñºÔ∏è –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_size}x{image_size}")
            
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            max_length = training_args.get('max_length', 512)
            self._log(f"   üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
            model.config.pad_token_id = processor.tokenizer.pad_token_id
            model.config.eos_token_id = processor.tokenizer.sep_token_id
            
            self._log("   üè∑Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
            self._log(f"     decoder_start_token_id: {model.config.decoder_start_token_id} ‚úÖ")
            self._log(f"     pad_token_id: {model.config.pad_token_id}")
            self._log(f"     eos_token_id: {model.config.eos_token_id}")
            
            self._log("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –≠–¢–ê–ü 4: –ü–û–î–ì–û–¢–û–í–ö–ê DATA COLLATOR
            self._log("")
            self._log("üîß ===== –≠–¢–ê–ü 4: –ü–û–î–ì–û–¢–û–í–ö–ê DATA COLLATOR =====")
            self._log(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
            
            data_collator = TrOCRDataCollator(
                processor=processor,
                max_length=max_length
            )
            self._log("‚úÖ Data collator —Å–æ–∑–¥–∞–Ω")
            
            # –≠–¢–ê–ü 5: –ù–ê–°–¢–†–û–ô–ö–ê –ê–†–ì–£–ú–ï–ù–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø
            self._log("")
            self._log("üìã ===== –≠–¢–ê–ü 5: –ù–ê–°–¢–†–û–ô–ö–ê –ê–†–ì–£–ú–ï–ù–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø =====")
            
            # –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
            output_dir = f"data/trained_models/{output_model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            output_dir = os.path.abspath(output_dir.replace('\\', '/'))
            self._log(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            # –ë–∞–∑–æ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            args = {
                'output_dir': output_dir,
                'num_train_epochs': training_args.get('num_train_epochs', 2),
                'per_device_train_batch_size': training_args.get('per_device_train_batch_size', 2),
                'per_device_eval_batch_size': training_args.get('per_device_eval_batch_size', 2),
                'learning_rate': training_args.get('learning_rate', 5e-5),
                'gradient_accumulation_steps': training_args.get('gradient_accumulation_steps', 4),
                'warmup_ratio': training_args.get('warmup_ratio', 0.1),
                'weight_decay': training_args.get('weight_decay', 0.01),
                'logging_steps': training_args.get('logging_steps', 10),
                'save_steps': training_args.get('save_steps', 500),
                'eval_steps': training_args.get('eval_steps', 500),
                'save_total_limit': training_args.get('save_total_limit', 3),
                'eval_strategy': 'no',
                'save_strategy': 'epoch',
                'logging_dir': './logs',
                'report_to': [],
                'load_best_model_at_end': False,
                'remove_unused_columns': False,  # –ö–†–ò–¢–ò–ß–ù–û –¥–ª—è TrOCR
                'prediction_loss_only': True,
            }
            
            # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if torch.cuda.is_available():
                self._log("üöÄ –ù–ê–°–¢–†–û–ô–ö–ê GPU –£–°–ö–û–†–ï–ù–ò–Ø:")
                
                # FP16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                if training_args.get('fp16', True):
                    args['fp16'] = True
                    self._log("   ‚úÖ FP16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞")
                
                # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                cuda_version = torch.version.cuda
                
                self._log(f"   üéÆ GPU: {gpu_name}")
                self._log(f"   üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
                self._log(f"   ‚ö° CUDA –≤–µ—Ä—Å–∏—è: {cuda_version}")
                
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è GPU
                args['dataloader_num_workers'] = 0  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏
                args['dataloader_pin_memory'] = True
                self._log("   üß† Workers: 0 (–±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏)")
            
            self._log("‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self._log("üìä –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            self._log(f"   –≠–ø–æ—Ö: {args['num_train_epochs']}")
            self._log(f"   Batch size (train): {args['per_device_train_batch_size']}")
            self._log(f"   Batch size (eval): {args['per_device_eval_batch_size']}")
            self._log(f"   Learning rate: {args['learning_rate']}")
            self._log(f"   Gradient accumulation: {args['gradient_accumulation_steps']}")
            self._log(f"   FP16: {args.get('fp16', False)}")
            
            # –≠–¢–ê–ü 6: –°–û–ó–î–ê–ù–ò–ï CALLBACKS
            self._log("")
            self._log("üîî ===== –≠–¢–ê–ü 6: –°–û–ó–î–ê–ù–ò–ï CALLBACKS =====")
            
            callbacks = []
            
            # Progress callback
            if self.progress_callback:
                progress_cb = TrOCRProgressCallback(self.progress_callback)
                callbacks.append(progress_cb)
            
            # Metrics callback  
            if self.metrics_callback:
                metrics_cb = TrOCRMetricsCallback(self.metrics_callback)
                callbacks.append(metrics_cb)
            
            # Early stopping - remove for now since it requires metric_for_best_model
            # callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))
            
            # GPU monitoring
            gpu_monitor_cb = TrOCRGPUMonitorCallback(self._log)
            callbacks.append(gpu_monitor_cb)
            
            self._log(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ callbacks: {len(callbacks)}")
            for i, callback in enumerate(callbacks):
                self._log(f"   {i+1}. {callback.__class__.__name__}")
            
            # –≠–¢–ê–ü 7: –°–û–ó–î–ê–ù–ò–ï TRAINER
            self._log("")
            self._log("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ Trainer...")
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            class OptimizedTrOCRTrainer(Trainer):
                def __init__(self, *args, use_8bit_optimizer=False, learning_rate=5e-5, logger_func=None, **kwargs):
                    # –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑—ã–≤–∞–µ–º parent __init__
                    super().__init__(*args, **kwargs)
                    # –ó–∞—Ç–µ–º —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—à–∏ –∞—Ç—Ä–∏–±—É—Ç—ã
                    self.use_8bit_optimizer = use_8bit_optimizer
                    self.custom_learning_rate = learning_rate
                    self._log_func = logger_func or print  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                
                def _log(self, message):
                    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–µ—Ç–æ–¥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Unicode"""
                    # –°–æ–∑–¥–∞–µ–º ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –≤–µ—Ä—Å–∏—é —Å–æ–æ–±—â–µ–Ω–∏—è
                    safe_message = message.encode('ascii', errors='replace').decode('ascii')
                    
                    try:
                        if hasattr(self, '_log_func') and self._log_func:
                            self._log_func(message)
                        elif hasattr(self, 'log') and callable(self.log):
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –º–µ—Ç–æ–¥ log –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                            self.log(safe_message)
                        else:
                            # Fallback –∫ print
                            print(safe_message)
                    except Exception:
                        # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º print —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
                        print(safe_message)
                
                def training_step(self, model, inputs, num_items_in_batch=None):
                    """üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è LoRA –º–æ–¥–µ–ª–∏"""
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –≤—Ö–æ–¥—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                    self._log(f"[BEFORE FILTER] Trainer –ø–æ–ª—É—á–∏–ª –∫–ª—é—á–∏: {list(inputs.keys())}")
                    
                    # üîß –ö–õ–Æ–ß–ï–í–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –§–ò–õ–¨–¢–†–£–ï–ú –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è TrOCR+LoRA
                    # –£–¥–∞–ª—è–µ–º –≤—Å–µ —á—Ç–æ –º–æ–∂–µ—Ç –≤—ã–∑–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å encoder
                    filtered_inputs = {}
                    
                    # –†–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è TrOCR VisionEncoderDecoder
                    allowed_keys = {
                        'pixel_values',  # –î–ª—è ViT encoder
                        'labels',        # –î–ª—è decoder
                        'attention_mask',  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω—É–∂–Ω–æ –¥–ª—è decoder
                        'decoder_input_ids',  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω—É–∂–Ω–æ –¥–ª—è decoder
                        'decoder_attention_mask',  # –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω—É–∂–Ω–æ –¥–ª—è decoder
                    }
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏
                    for key, value in inputs.items():
                        if key in allowed_keys:
                            filtered_inputs[key] = value
                        else:
                            self._log(f"[FILTERED OUT] –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –∫–ª—é—á: {key}")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥—ã
                    self._log(f"[AFTER FILTER] –ü–µ—Ä–µ–¥–∞–µ–º –≤ –º–æ–¥–µ–ª—å –∫–ª—é—á–∏: {list(filtered_inputs.keys())}")
                    
                    # –í—ã–∑—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π training_step —Å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–∞–º–∏
                    return super().training_step(model, filtered_inputs, num_items_in_batch)
                
                def create_optimizer(self):
                    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 8-bit"""
                    if self.use_8bit_optimizer and BITSANDBYTES_AVAILABLE:
                        try:
                            # –°–æ–∑–¥–∞–µ–º 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                            optimizer = bnb.optim.AdamW8bit(
                                self.model.parameters(),
                                lr=self.custom_learning_rate,
                                betas=(0.9, 0.999),
                                eps=1e-8,
                                weight_decay=0.01
                            )
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–µ –¥–ª—è Trainer
                            self.optimizer = optimizer
                            
                            self._log("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-bit AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—ç–∫–æ–Ω–æ–º–∏—è ~25% –ø–∞–º—è—Ç–∏)")
                            return optimizer
                            
                        except Exception as e:
                            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: {e}")
                            # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—É
                            pass
                    
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                    return super().create_optimizer()
            
            # –°–æ–∑–¥–∞–µ–º training arguments
            training_arguments = TrainingArguments(**args)
            
            # –°–æ–∑–¥–∞–µ–º trainer
            trainer = OptimizedTrOCRTrainer(
                model=model,
                args=training_arguments,
                train_dataset=train_dataset,
                eval_dataset=dataset.get('validation', None),
                data_collator=data_collator,
                callbacks=callbacks,
                use_8bit_optimizer=training_args.get('use_8bit_optimizer', False),
                learning_rate=args['learning_rate'],
                logger_func=self._log  # –ü–µ—Ä–µ–¥–∞–µ–º –º–µ—Ç–æ–¥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            )
            
            self._log("‚úÖ Trainer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏
            train_batch_size = args['per_device_train_batch_size']
            gradient_accumulation = args['gradient_accumulation_steps']
            effective_batch_size = train_batch_size * gradient_accumulation
            
            steps_per_epoch = len(train_dataset) // effective_batch_size
            total_steps = steps_per_epoch * args['num_train_epochs']
            estimated_time_minutes = total_steps * 1 // 60  # –ü—Ä–∏–º–µ—Ä–Ω–æ 1 —Å–µ–∫/—à–∞–≥
            
            self._log("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏:")
            self._log(f"   üìÑ –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_dataset)}")
            self._log(f"   üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {train_batch_size}")
            self._log(f"   üîÑ Gradient accumulation: {gradient_accumulation}")
            self._log(f"   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {effective_batch_size}")
            self._log(f"   üî¢ –®–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É: {steps_per_epoch}")
            self._log(f"   üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {total_steps}")
            self._log(f"   ‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è (–ø—Ä–∏ 1 —Å–µ–∫/—à–∞–≥): {estimated_time_minutes} –º–∏–Ω")
            
            # –≠–¢–ê–ü 8: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
            self._log("")
            self._log("üöÄ ===== –≠–¢–ê–ü 8: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø =====")
            self._log("üéØ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –º–æ–¥–µ–ª–∏...")
            self._log(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%H:%M:%S')}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            training_result = trainer.train()
            
            # –≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò
            self._log("")
            self._log("üíæ ===== –≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            final_model_path = os.path.join(output_dir, "final_model")
            os.makedirs(final_model_path, exist_ok=True)
            
            trainer.save_model(final_model_path)
            processor.save_pretrained(final_model_path)
            
            self._log(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {final_model_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'model_type': 'trocr',
                'base_model': base_model_id,
                'training_params': training_args,
                'dataset_path': dataset_path,
                'dataset_size': len(train_dataset),
                'training_time': str(datetime.now()),
                'final_loss': float(training_result.training_loss) if hasattr(training_result, 'training_loss') else None,
                'total_steps': total_steps,
                'device': str(self.device)
            }
            
            metadata_path = os.path.join(final_model_path, "training_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self._log(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {metadata_path}")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self._log("")
            self._log("üéâ ========== –û–ë–£–ß–ï–ù–ò–ï TrOCR –ó–ê–í–ï–†–®–ï–ù–û ==========")
            self._log("üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            self._log(f"   ‚è∞ –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now().strftime('%H:%M:%S')}")
            self._log(f"   üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
            self._log(f"   üìÑ –î–∞—Ç–∞—Å–µ—Ç: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            self._log(f"   üî¢ –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")
            
            final_loss = None
            if hasattr(training_result, 'training_loss'):
                final_loss = training_result.training_loss
                self._log(f"   üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {final_loss:.4f}")
            
            # –ü–∞–º—è—Ç—å GPU
            max_memory_gb = 0
            if torch.cuda.is_available():
                max_memory_gb = torch.cuda.max_memory_allocated() / (1024**3)
                self._log(f"   üíæ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {max_memory_gb:.2f} GB")
            
            self._log("‚úÖ –û–±—É—á–µ–Ω–∏–µ TrOCR –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            
            # üéØ –†–ê–°–®–ò–†–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò
            training_time = getattr(training_result, 'train_runtime', 31.5) if hasattr(training_result, 'train_runtime') else 31.5
            quality_analysis = self._analyze_model_quality(
                final_loss=final_loss,
                dataset_size=len(train_dataset),
                total_steps=total_steps,
                training_time=training_time,
                model=model,
                dataset=dataset
            )
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ callback –¥–ª—è UI
            if self.metrics_callback:
                self.metrics_callback(quality_analysis['summary_message'])
                
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å 100% —á–µ—Ä–µ–∑ callback
            if self.progress_callback:
                self.progress_callback(100)
            return final_model_path
            
        except Exception as e:
            self._log("")
            self._log("üí• ========== –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø TrOCR ==========")
            self._log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            self._log("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            self._log(f"   Python –≤–µ—Ä—Å–∏—è: {sys.version}")
            self._log(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
            self._log(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
            
            if torch.cuda.is_available():
                self._log(f"   CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
                self._log(f"   –¢–µ–∫—É—â–µ–µ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.current_device()}")
                self._log(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
            
            self._log(f"   –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"   –î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(dataset_path)}")
            
            # –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞
            import traceback
            self._log("")
            self._log("üîç –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    self._log(f"   {line}")
            
            self.logger.error(f"TrOCRTrainer error: {e}")
            self.logger.error(traceback.format_exc())
            
            return None
    
    def _analyze_model_quality(self, final_loss, dataset_size, total_steps, training_time, model, dataset):
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π TrOCR –º–æ–¥–µ–ª–∏
        """
        try:
            # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            analysis = {
                'loss_interpretation': self._interpret_loss(final_loss, dataset_size),
                'training_efficiency': self._evaluate_training_efficiency(total_steps, training_time, dataset_size),
                'model_readiness': self._assess_model_readiness(final_loss, dataset_size),
                'recommendations': self._generate_recommendations(final_loss, dataset_size, total_steps)
            }
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å validation set)
            validation_metrics = self._evaluate_on_validation(model, dataset)
            if validation_metrics:
                analysis['validation_metrics'] = validation_metrics
                analysis['recommendations'].extend(validation_metrics.get('recommendations', []))
            
            # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            quality_score = self._calculate_quality_score(final_loss, dataset_size, validation_metrics)
            
            summary_message = self._format_quality_summary(analysis, quality_score, final_loss, dataset_size, total_steps, validation_metrics)
            
            analysis['summary_message'] = summary_message
            analysis['quality_score'] = quality_score
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            self._log("\n" + "="*60)
            self._log("üîç –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–ê–ß–ï–°–¢–í–ê –ú–û–î–ï–õ–ò")
            self._log("="*60)
            self._log(summary_message)
            
            if validation_metrics:
                self._log("\nüß™ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò:")
                self._log(f"   üìä –¢–æ—á–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤: {validation_metrics['char_accuracy']:.1f}%")
                self._log(f"   üìù –¢–æ—á–Ω–æ—Å—Ç—å —Å–ª–æ–≤: {validation_metrics['word_accuracy']:.1f}%")
                self._log(f"   üìÑ –¢–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {validation_metrics['document_accuracy']:.1f}%")
                
            self._log("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
            for i, rec in enumerate(analysis['recommendations'], 1):
                self._log(f"   {i}. {rec}")
            self._log("="*60)
            
            return analysis
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return {
                'summary_message': f"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û | Loss: {final_loss:.4f} | –î–∞—Ç–∞—Å–µ—Ç: {dataset_size} –ø—Ä–∏–º–µ—Ä–æ–≤",
                'quality_score': 'unknown'
            }
    
    def _interpret_loss(self, loss, dataset_size):
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ loss –¥–ª—è TrOCR"""
        if loss < 0.5:
            return "üèÜ –ü–†–ï–í–û–°–•–û–î–ù–û - –ú–æ–¥–µ–ª—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –∏–¥–µ–∞–ª—å–Ω–æ –Ω–∞—É—á–∏–ª–∞—Å—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç"
        elif loss < 1.0:
            return "üî• –û–¢–õ–ò–ß–ù–û - –ú–æ–¥–µ–ª—å –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ –ø–æ–Ω–∏–º–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞"
        elif loss < 2.0:
            return "‚úÖ –•–û–†–û–®–û - –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏–∑—É—á–∏–ª–∞ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã"
        elif loss < 4.0:
            return "üü° –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û - –ú–æ–¥–µ–ª—å –Ω–∞—á–∞–ª–∞ –∏–∑—É—á–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã, –Ω–æ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏—è"
        elif loss < 8.0:
            return "üü† –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø - –ú–æ–¥–µ–ª—å –µ—â–µ –ø–ª–æ—Ö–æ –ø–æ–Ω–∏–º–∞–µ—Ç –∑–∞–¥–∞—á—É"
        else:
            return "üî¥ –ö–†–ò–¢–ò–ß–ù–û - –ú–æ–¥–µ–ª—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ –æ–±—É—á–∏–ª–∞—Å—å, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–∞–Ω–Ω—ã–µ"
    
    def _evaluate_training_efficiency(self, steps, time_sec, dataset_size):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è"""
        steps_per_sec = steps / time_sec if time_sec > 0 else 0
        samples_per_sec = (dataset_size * 3) / time_sec if time_sec > 0 else 0  # 3 —ç–ø–æ—Ö–∏
        
        if steps_per_sec > 2.0:
            efficiency = "üöÄ –û–ß–ï–ù–¨ –ë–´–°–¢–†–û"
        elif steps_per_sec > 1.0:
            efficiency = "‚ö° –ë–´–°–¢–†–û"
        elif steps_per_sec > 0.5:
            efficiency = "‚è±Ô∏è –ù–û–†–ú–ê–õ–¨–ù–û"
        else:
            efficiency = "üêå –ú–ï–î–õ–ï–ù–ù–û"
            
        return f"{efficiency} - {steps_per_sec:.2f} —à–∞–≥–æ–≤/—Å–µ–∫, {samples_per_sec:.1f} –ø—Ä–∏–º–µ—Ä–æ–≤/—Å–µ–∫"
    
    def _assess_model_readiness(self, loss, dataset_size):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"""
        if loss < 1.0 and dataset_size >= 100:
            return "‚úÖ –ì–û–¢–û–í–ê –ö –ü–†–û–î–ê–ö–®–ï–ù–£ - –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö"
        elif loss < 2.0 and dataset_size >= 50:
            return "üß™ –ì–û–¢–û–í–ê –ö –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Æ - –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"
        elif loss < 4.0:
            return "üîÑ –ù–£–ñ–ù–û –î–û–û–ë–£–ß–ï–ù–ò–ï - –î–æ–±–∞–≤—å—Ç–µ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —É–≤–µ–ª–∏—á—å—Ç–µ —ç–ø–æ—Ö–∏"
        else:
            return "‚ùå –ù–ï –ì–û–¢–û–í–ê - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥"
    
    def _generate_recommendations(self, loss, dataset_size, steps):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
        recommendations = []
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ loss
        if loss > 4.0:
            recommendations.append("üìâ –°–Ω–∏–∑—å—Ç–µ learning rate –¥–æ 1e-5 –∏–ª–∏ 2e-5")
            recommendations.append("üìö –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
            recommendations.append("üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π - –≤–æ–∑–º–æ–∂–Ω—ã –æ—à–∏–±–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö")
        elif loss > 2.0:
            recommendations.append("‚è±Ô∏è –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–æ 5-10")
            recommendations.append("üéØ –î–æ–±–∞–≤—å—Ç–µ data augmentation –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è")
        elif loss < 0.5:
            recommendations.append("‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ early stopping")
            recommendations.append("üß™ –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
        if dataset_size < 50:
            recommendations.append("üìä –£–≤–µ–ª–∏—á—å—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –¥–æ 100+ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
        elif dataset_size < 200:
            recommendations.append("üìà –î–ª—è production —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è 500+ –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if steps < 30:
            recommendations.append("‚è≥ –í–æ–∑–º–æ–∂–Ω–æ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations.append("üéØ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å—á–µ—Ç–∞—Ö")
        recommendations.append("üìä –°—Ä–∞–≤–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é microsoft/trocr-base-printed")
        
        return recommendations
    
    def _evaluate_on_validation(self, model, dataset):
        """–ü—Ä–æ–≤–æ–¥–∏—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if 'validation' not in dataset or len(dataset['validation']) == 0:
                return None
                
            self._log("üß™ –ü—Ä–æ–≤–æ–¥–∏–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            
            validation_set = dataset['validation']
            total_samples = min(20, len(validation_set))  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –º–∞–∫—Å–∏–º—É–º 20 –ø—Ä–∏–º–µ—Ä–∞—Ö
            
            char_matches = 0
            word_matches = 0
            exact_matches = 0
            total_chars = 0
            total_words = 0
            
            processor = AutoProcessor.from_pretrained("microsoft/trocr-base-printed")
            
            for i in range(total_samples):
                try:
                    sample = validation_set[i]
                    image = sample['image']
                    true_text = sample['text']
                    
                    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    pixel_values = processor(image, return_tensors="pt").pixel_values
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    with torch.no_grad():
                        generated_ids = model.generate(pixel_values)
                        predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    
                    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    char_acc = self._calculate_char_accuracy(predicted_text, true_text)
                    word_acc = self._calculate_word_accuracy(predicted_text, true_text)
                    
                    char_matches += char_acc * len(true_text)
                    total_chars += len(true_text)
                    
                    word_matches += word_acc * len(true_text.split())
                    total_words += len(true_text.split())
                    
                    if predicted_text.strip() == true_text.strip():
                        exact_matches += 1
                        
                except Exception as e:
                    self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—Ä–∞–∑—Ü–∞ {i}: {e}")
                    continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            char_accuracy = (char_matches / total_chars * 100) if total_chars > 0 else 0
            word_accuracy = (word_matches / total_words * 100) if total_words > 0 else 0
            document_accuracy = (exact_matches / total_samples * 100) if total_samples > 0 else 0
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            val_recommendations = []
            if char_accuracy < 80:
                val_recommendations.append("üìù –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            if word_accuracy < 70:
                val_recommendations.append("üî§ –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Å–ª–æ–≤ - –≤–æ–∑–º–æ–∂–Ω–æ –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö")
            if document_accuracy < 50:
                val_recommendations.append("üìÑ –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —É–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö")
            
            return {
                'char_accuracy': char_accuracy,
                'word_accuracy': word_accuracy,
                'document_accuracy': document_accuracy,
                'total_samples': total_samples,
                'recommendations': val_recommendations
            }
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            return None
    
    def _calculate_char_accuracy(self, predicted, true):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤"""
        if not true:
            return 0.0
        matches = sum(1 for p, t in zip(predicted, true) if p == t)
        return matches / len(true)
    
    def _calculate_word_accuracy(self, predicted, true):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–ª–æ–≤"""
        pred_words = predicted.split()
        true_words = true.split()
        if not true_words:
            return 0.0
        matches = sum(1 for pw, tw in zip(pred_words, true_words) if pw == tw)
        return matches / len(true_words)

    def _calculate_quality_score(self, loss, dataset_size, validation_metrics=None):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        loss_score = max(0, 10 - loss)
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if dataset_size >= 200:
            size_score = 10
        elif dataset_size >= 100:
            size_score = 8
        elif dataset_size >= 50:
            size_score = 6
        else:
            size_score = 4
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, —É—á–∏—Ç—ã–≤–∞–µ–º –∏—Ö
        validation_score = 5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if validation_metrics:
            avg_accuracy = (validation_metrics['char_accuracy'] + validation_metrics['word_accuracy']) / 2
            validation_score = min(10, avg_accuracy / 10)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        if validation_metrics:
            total_score = (loss_score * 0.4) + (size_score * 0.2) + (validation_score * 0.4)
        else:
            total_score = (loss_score * 0.7) + (size_score * 0.3)
        
        if total_score >= 9:
            return "üèÜ –û–¢–õ–ò–ß–ù–û"
        elif total_score >= 7:
            return "‚úÖ –•–û–†–û–®–û"
        elif total_score >= 5:
            return "üü° –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û"
        else:
            return "üî¥ –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø"
    
    def _format_quality_summary(self, analysis, quality_score, loss, dataset_size, steps, validation_metrics=None):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ"""
        base_message = (
            f"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û | {quality_score}\n"
            f"üìä {analysis['loss_interpretation']}\n"
            f"‚ö° {analysis['training_efficiency']}\n"
            f"üéØ {analysis['model_readiness']}\n"
            f"üìâ Loss: {loss:.4f} | üìÑ –î–∞—Ç–∞—Å–µ—Ç: {dataset_size} | üî¢ –®–∞–≥–æ–≤: {steps}"
        )
        
        if validation_metrics:
            val_summary = (
                f"\nüß™ –í–ê–õ–ò–î–ê–¶–ò–Ø: –°–∏–º–≤–æ–ª—ã {validation_metrics['char_accuracy']:.1f}% | "
                f"–°–ª–æ–≤–∞ {validation_metrics['word_accuracy']:.1f}% | "
                f"–î–æ–∫—É–º–µ–Ω—Ç—ã {validation_metrics['document_accuracy']:.1f}%"
            )
            return base_message + val_summary
        
        return base_message

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            self._log("üõë –ó–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É –æ–±—É—á–µ–Ω–∏—è TrOCR...")
            if hasattr(self, 'current_trainer') and self.current_trainer is not None:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π trainer - –ø—ã—Ç–∞–µ–º—Å—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
                self._log("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ trainer...")
                # Note: Transformers Trainer –Ω–µ –∏–º–µ–µ—Ç –ø—Ä—è–º–æ–≥–æ –º–µ—Ç–æ–¥–∞ stop
                # –û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–ª–∏ —Ñ–ª–∞–≥–∏
                self._log("‚ö†Ô∏è Trainer –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–µ–µ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ")
            else:
                self._log("‚ÑπÔ∏è –ê–∫—Ç–∏–≤–Ω—ã–π trainer –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ TrOCRTrainer: {e}", "warning") 

    def _apply_model_specific_optimizations(self, model, training_args: dict):
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è TrOCR –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        
        # TrOCR specific optimizations - –±–µ–∑–æ–ø–∞—Å–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è VisionEncoderDecoder
        if hasattr(model, 'forward'):
            original_forward = model.forward
            
            def safe_trocr_forward(pixel_values=None, labels=None, **kwargs):
                """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π forward –¥–ª—è TrOCR —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤"""
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º–∞—Ç–∏—á–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è TrOCR
                filtered_kwargs = {k: v for k, v in kwargs.items() 
                                 if k not in ['input_ids', 'attention_mask']}
                
                return original_forward(
                    pixel_values=pixel_values,
                    labels=labels,
                    **filtered_kwargs
                )
            
            model.forward = safe_trocr_forward
            self._log("‚úÖ TrOCR forward method optimized")
        
        return model