"""
TrOCR Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Microsoft TrOCR –º–æ–¥–µ–ª–µ–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: LoRA, 8-bit optimizer, gradient checkpointing
"""

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è TrOCR –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
from transformers import (
    TrOCRProcessor, VisionEncoderDecoderModel, 
    TrainingArguments, Trainer, EarlyStoppingCallback
)
from datasets import load_from_disk, Dataset, DatasetDict, Features, Value, Image as DatasetImage
from datetime import datetime
import json
import os
import sys
import logging
import torch
import re
from collections import defaultdict
from pathlib import Path
from PIL import Image
from typing import Dict, List, Optional, Union, Any
import math
import tempfile

# LoRA imports
try:
    from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
    from peft import PeftModel, PeftConfig
    LORA_AVAILABLE = True
except ImportError:
    LORA_AVAILABLE = False
    
# 8-bit optimizer imports  
try:
    import bitsandbytes as bnb
    BITSANDBYTES_AVAILABLE = True
except ImportError:
    BITSANDBYTES_AVAILABLE = False

class TrOCRDataCollator:
    """Data collator –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, processor, max_length=512):
        self.processor = processor
        self.max_length = max_length
        
    def __call__(self, batch):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö –¥–ª—è TrOCR –æ–±—É—á–µ–Ω–∏—è"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–µ–∫—Å—Ç—ã –∏–∑ –±–∞—Ç—á–∞
        images = []
        texts = []
        
        for item in batch:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            if 'image' not in item:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'image' –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –±–∞—Ç—á–∞")
            if 'text' not in item:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'text' –≤ —ç–ª–µ–º–µ–Ω—Ç–µ –±–∞—Ç—á–∞")
                
            images.append(item['image'])
            texts.append(item['text'])
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è encoder
        pixel_values = self.processor(
            images, 
            return_tensors="pt"
        ).pixel_values
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è decoder —Å labels
        labels = self.processor.tokenizer(
            texts,
            max_length=self.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).input_ids
        
        # –ó–∞–º–µ–Ω—è–µ–º padding —Ç–æ–∫–µ–Ω—ã –Ω–∞ -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –≤ loss
        labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        return {
            'pixel_values': pixel_values,
            'labels': labels
        }

class TrOCRProgressCallback:
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, progress_callback=None):
        self.progress_callback = progress_callback
    
    def on_init_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ trainer"""
        return control
    
    def on_train_begin(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è"""
        return control
    
    def on_train_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –æ–±—É—á–µ–Ω–∏—è"""
        return control
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —ç–ø–æ—Ö–∏"""
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏"""
        return control
        
    def on_step_begin(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —à–∞–≥–∞"""
        return control
        
    def on_step_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —à–∞–≥–∞"""
        if self.progress_callback and state.max_steps > 0:
            progress = int((state.global_step / state.max_steps) * 100)
            self.progress_callback(progress)
        return control
    
    def on_log(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏"""
        return control
        
    def on_substep_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –ø–æ–¥—à–∞–≥–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_pre_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_save(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control

class TrOCRMetricsCallback:
    """Callback –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, metrics_callback=None):
        self.metrics_callback = metrics_callback
    
    def on_init_end(self, args, state, control, **kwargs):
        return control
    
    def on_train_begin(self, args, state, control, **kwargs):
        return control
    
    def on_train_end(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        return control
    
    def on_step_begin(self, args, state, control, **kwargs):
        return control
    
    def on_step_end(self, args, state, control, **kwargs):
        return control
        
    def on_log(self, args, state, control, logs=None, **kwargs):
        if self.metrics_callback and logs:
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è UI
            formatted_message = self._format_metrics_message(logs, state)
            self.metrics_callback(formatted_message)
        return control
    
    def _format_metrics_message(self, logs, state):
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI"""
        try:
            # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            epoch = getattr(state, 'epoch', 0)
            step = getattr(state, 'global_step', 0)
            max_steps = getattr(state, 'max_steps', 1)
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
            progress_percent = int((step / max_steps) * 100) if max_steps > 0 else 0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ
            message_parts = []
            message_parts.append(f"üìä –≠–ø–æ—Ö–∞ {epoch:.1f}, –®–∞–≥ {step}/{max_steps} ({progress_percent}%)")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
            if 'train_loss' in logs:
                loss = logs['train_loss']
                message_parts.append(f"üìâ Loss: {loss:.4f}")
            
            if 'learning_rate' in logs:
                lr = logs['learning_rate']
                message_parts.append(f"üìà LR: {lr:.2e}")
                
            if 'train_runtime' in logs:
                runtime = logs['train_runtime']
                message_parts.append(f"‚è±Ô∏è –í—Ä–µ–º—è: {runtime:.1f}s")
                
            if 'train_samples_per_second' in logs:
                sps = logs['train_samples_per_second']
                message_parts.append(f"üöÄ {sps:.2f} samples/sec")
            
            # –û—Ü–µ–Ω–∫–∞ –ø–∞–º—è—Ç–∏ GPU
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / (1024**3)
                message_parts.append(f"üíæ GPU: {memory_used:.1f}GB")
            
            return " | ".join(message_parts)
            
        except Exception as e:
            return f"üìä –®–∞–≥ {step}: –º–µ—Ç—Ä–∏–∫–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã (–æ—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e})"
        
    def on_substep_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –ø–æ–¥—à–∞–≥–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_pre_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_save(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control

class TrOCRGPUMonitorCallback:
    """Callback –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ GPU –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è TrOCR"""
    
    def __init__(self, logger_func=None):
        self._log = logger_func or print
    
    def on_init_end(self, args, state, control, **kwargs):
        return control
    
    def on_train_begin(self, args, state, control, **kwargs):
        return control
    
    def on_train_end(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        return control
    
    def on_epoch_end(self, args, state, control, **kwargs):
        return control
        
    def on_step_begin(self, args, state, control, **kwargs):
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            
            if state.global_step % 5 == 0:  # –ö–∞–∂–¥—ã–µ 5 —à–∞–≥–æ–≤
                self._log(f"   üìä –®–∞–≥ {state.global_step}: GPU –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ {allocated:.2f}GB / {reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
        return control
    
    def on_step_end(self, args, state, control, **kwargs):
        return control
    
    def on_log(self, args, state, control, **kwargs):
        return control
        
    def on_substep_end(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ –ø–æ–¥—à–∞–≥–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_pre_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ —à–∞–≥–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_optimizer_step(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control
        
    def on_save(self, args, state, control, **kwargs):
        """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ (–Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ –≤ transformers)"""
        return control

class TrOCRTrainer:
    """
    Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Microsoft TrOCR –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø–∞–º—è—Ç–∏
    """
    
    def __init__(self, device: str = "auto", logger: logging.Logger = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TrOCR Trainer
        
        Args:
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ('cuda', 'cpu', 'auto')
            logger: –õ–æ–≥–≥–µ—Ä –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if device == "auto":
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)
            
        # –õ–æ–≥–≥–µ—Ä
        self.logger = logger or logging.getLogger(__name__)
        
        # Callbacks
        self.progress_callback = None
        self.metrics_callback = None
        self.status_callback = None
        
        self._log(f"TrOCRTrainer –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
    
    def _log(self, message: str, level: str = "info"):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Unicode"""
        # –°–æ–∑–¥–∞–µ–º ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –≤–µ—Ä—Å–∏—é —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è Windows console
        safe_message = message.encode('ascii', errors='replace').decode('ascii')
        
        if self.logger:
            # –õ–æ–≥–≥–µ—Ä –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å Unicode, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            try:
                getattr(self.logger, level)(message)
            except UnicodeEncodeError:
                # Fallback –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
                getattr(self.logger, level)(safe_message)
        else:
            print(f"[{level.upper()}] {safe_message}")
    
    def set_callbacks(self, progress_callback=None, metrics_callback=None, status_callback=None):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç callback —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self.progress_callback = progress_callback
        self.metrics_callback = metrics_callback  
        self.status_callback = status_callback
    
    def _apply_lora_optimization(self, model, training_args: dict):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –¥–ª—è —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ–≥–æ —Å–Ω–∏–∂–µ–Ω–∏—è –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
        """
        if not LORA_AVAILABLE:
            self._log("‚ö†Ô∏è PEFT (LoRA) –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: pip install peft")
            return model, False
            
        # LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è TrOCR (VisionEncoderDecoderModel)
        lora_config = LoraConfig(
            task_type=TaskType.SEQ_2_SEQ_LM,  # TrOCR - —ç—Ç–æ sequence-to-sequence –º–æ–¥–µ–ª—å
            r=16,  # Rank - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ LoRA
            lora_alpha=32,  # Scaling factor
            lora_dropout=0.1,
            # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ—è–º decoder (RoBERTa)
            target_modules=[
                "query", "key", "value", "dense",  # Attention —Å–ª–æ–∏
                "intermediate.dense", "output.dense",  # Feed forward —Å–ª–æ–∏
            ],
            bias="none",  # –ù–µ –æ–±—É—á–∞–µ–º bias –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            modules_to_save=None,  # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏
        )
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è LoRA
            model = prepare_model_for_kbit_training(model)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA
            model = get_peft_model(model, lora_config)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            self._log("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ LoRA:")
            self._log(f"   –í—Å–µ–≥–æ: {total_params:,}")
            self._log(f"   –û–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,}")
            self._log(f"   –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—É—á–∞–µ–º—ã—Ö: {100 * trainable_params / total_params:.2f}%")
            self._log(f"   üöÄ –≠–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏: ~{100 - (100 * trainable_params / total_params):.1f}%")
            
            return model, True
            
        except Exception as e:
            self._log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA: {e}", "error")
            return model, False
    
    def _apply_memory_optimizations(self, model, training_args: dict):
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Å–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: LoRA, Gradient Checkpointing
        """
        optimizations_applied = []
        
        # LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        if training_args.get('use_lora', False):
            self._log("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é...")
            model, lora_success = self._apply_lora_optimization(model, training_args)
            if lora_success:
                optimizations_applied.append("LoRA (–¥–æ 95% —ç–∫–æ–Ω–æ–º–∏–∏)")
        
        # Gradient checkpointing
        if training_args.get('gradient_checkpointing', True):
            optimizations_applied.append("Gradient Checkpointing")
        
        self._log(f"üöÄ –ü—Ä–∏–º–µ–Ω–µ–Ω—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏: {', '.join(optimizations_applied)}")
        return model
    
    def _setup_cuda_optimizations(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM"""
        if not torch.cuda.is_available():
            return
            
        self._log("üßπ === –ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –û–ß–ò–°–¢–ö–ê CUDA –ü–ê–ú–Ø–¢–ò ===")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        import os
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        self._log("   ‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True")
        
        # –û—á–∏—â–∞–µ–º CUDA –∫—ç—à
        torch.cuda.empty_cache()
        self._log("   üßπ –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ CUDA –∫—ç—à–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏
        torch.cuda.reset_peak_memory_stats()
        self._log("   üìä –°–±—Ä–æ—à–µ–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ CUDA")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏
        torch.cuda.set_per_process_memory_fraction(0.95)
        self._log("   üéØ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ª–∏–º–∏—Ç –ø–∞–º—è—Ç–∏: 95% –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏
        allocated = torch.cuda.memory_allocated() / (1024**3)
        reserved = torch.cuda.memory_reserved() / (1024**3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        free = total - reserved
        
        self._log("   üíæ –ü–∞–º—è—Ç—å GPU:")
        self._log(f"      –í—ã–¥–µ–ª–µ–Ω–æ: {allocated:.2f} GB")
        self._log(f"      –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {reserved:.2f} GB") 
        self._log(f"      –°–≤–æ–±–æ–¥–Ω–æ: {free:.2f} GB")
    
    def convert_dataset_to_trocr_format(self, dataset_path: str) -> Dataset:
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ InvoiceGemini –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR
        """
        self._log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        dataset = load_from_disk(dataset_path)
        
        def convert_item(item):
            """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞"""
            # TrOCR –æ–∂–∏–¥–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ text (–∞ –Ω–µ ground_truth)
            return {
                'image': item['image'],
                'text': item.get('ground_truth', item.get('text', ''))
            }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é
        if isinstance(dataset, DatasetDict):
            converted_dataset = DatasetDict()
            for split_name, split_dataset in dataset.items():
                converted_dataset[split_name] = split_dataset.map(convert_item)
        else:
            converted_dataset = dataset.map(convert_item)
        
        train_size = len(converted_dataset['train']) if 'train' in converted_dataset else len(converted_dataset)
        val_size = len(converted_dataset['validation']) if 'validation' in converted_dataset else 0
        
        self._log(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç: {train_size} train, {val_size} validation")
        
        return converted_dataset
    
    def train_trocr(self, 
                   dataset_path: str,
                   base_model_id: str,
                   training_args: dict,
                   output_model_name: str) -> Optional[str]:
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å TrOCR –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
            self._setup_cuda_optimizations()
            
            self._log("")
            self._log("ü§ñ ========== –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø TrOCR ==========")
            self._log("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
            self._log(f"   –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
            self._log(f"   –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_model_id}")
            self._log(f"   –í—ã—Ö–æ–¥–Ω–∞—è –º–æ–¥–µ–ª—å: {output_model_name}")
            self._log(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê
            self._log("")
            self._log("üìö ===== –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê =====")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç –ø–µ—Ä–µ–¥ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π
            self._log("üîç –ê–Ω–∞–ª–∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—ã—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            raw_dataset = load_from_disk(dataset_path)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            if isinstance(raw_dataset, DatasetDict):
                train_size = len(raw_dataset['train']) if 'train' in raw_dataset else 0
                val_size = len(raw_dataset['validation']) if 'validation' in raw_dataset else 0
                test_size = len(raw_dataset['test']) if 'test' in raw_dataset else 0
                total_size = train_size + val_size + test_size
                
                self._log(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
                self._log(f"   üìÅ –ü—É—Ç—å: {dataset_path}")
                self._log(f"   üéØ –û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {train_size}")
                self._log(f"   ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {val_size}")
                self._log(f"   üß™ –¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {test_size}")
                self._log(f"   üìà –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size}")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª—è –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
                if train_size > 0:
                    sample = raw_dataset['train'][0]
                    fields = list(sample.keys())
                    self._log(f"   üè∑Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è: {', '.join(fields)}")
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    if 'image' in sample:
                        img = sample['image']
                        self._log(f"   üñºÔ∏è –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {img.size}")
                        
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
                    if 'ground_truth' in sample or 'text' in sample:
                        text = sample.get('ground_truth', sample.get('text', ''))
                        self._log(f"   üìù –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞: '{text[:100]}{'...' if len(text) > 100 else ''}'")
                        self._log(f"   üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: ~{len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            else:
                total_size = len(raw_dataset)
                self._log(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
                self._log(f"   üìÅ –ü—É—Ç—å: {dataset_path}")
                self._log(f"   üìà –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size}")
                
                if total_size > 0:
                    sample = raw_dataset[0]
                    fields = list(sample.keys())
                    self._log(f"   üè∑Ô∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è: {', '.join(fields)}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç TrOCR...")
            dataset = self.convert_dataset_to_trocr_format(dataset_path)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            train_dataset = dataset['train'] if 'train' in dataset else dataset
            if len(train_dataset) == 0:
                raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏")
            
            self._log(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            self._log("‚úÖ –î–∞—Ç–∞—Å–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –≠–¢–ê–ü 2: –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
            self._log("")
            self._log("ü§ñ ===== –≠–¢–ê–ü 2: –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò =====")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            cache_dir = "data/models"
            os.makedirs(cache_dir, exist_ok=True)
            self._log(f"üìÅ –ö—ç—à –º–æ–¥–µ–ª–µ–π: {os.path.abspath(cache_dir)}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self._log(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –∏–∑: {base_model_id}")
            processor = TrOCRProcessor.from_pretrained(
                base_model_id,
                cache_dir=cache_dir
            )
            self._log("‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self._log(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {base_model_id}")
            model = VisionEncoderDecoderModel.from_pretrained(
                base_model_id,
                cache_dir=cache_dir
            )
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            self._log("üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
            self._log(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
            self._log(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
            model = self._apply_memory_optimizations(model, training_args)
            
            # –í–∫–ª—é—á–∞–µ–º gradient checkpointing
            gradient_checkpointing = training_args.get('gradient_checkpointing', True)
            self._log(f"   üíæ Gradient checkpointing: {gradient_checkpointing}")
            if gradient_checkpointing:
                try:
                    model.gradient_checkpointing_enable()
                    self._log("   ‚úÖ Gradient checkpointing –≤–∫–ª—é—á–µ–Ω")
                except Exception as e:
                    self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å gradient checkpointing: {e}")
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU
            model.to(self.device)
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ CUDA –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if torch.cuda.is_available():
                self._log("üöÄ === –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ù–ê–°–¢–†–û–ô–ö–ê GPU ===")
                torch.cuda.empty_cache()
                self._log("   üßπ –ö—ç—à CUDA –æ—á–∏—â–µ–Ω")
                
                torch.cuda.set_device(0)
                self._log("   üéØ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ 0 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–µ")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–∞ GPU
                if next(model.parameters()).is_cuda:
                    self._log("   ‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: –ú–æ–¥–µ–ª—å –Ω–∞ GPU!")
                
                # –í–∫–ª—é—á–∞–µ–º CUDNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                torch.backends.cudnn.benchmark = True
                self._log("   ‚ö° CUDNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å GPU
                free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()
                free_gb = free_memory / (1024**3)
                self._log(f"   üíæ –°–≤–æ–±–æ–¥–Ω–∞—è –ø–∞–º—è—Ç—å GPU: {free_gb:.1f} GB")
                
                if free_gb < 2:
                    self._log("   ‚ö†Ô∏è –ú–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU - –≤–æ–∑–º–æ–∂–Ω—ã OOM –æ—à–∏–±–∫–∏")
                else:
                    self._log("   ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –≠–¢–ê–ü 3: –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ò
            self._log("")
            self._log("‚öôÔ∏è ===== –≠–¢–ê–ü 3: –ù–ê–°–¢–†–û–ô–ö–ê –ú–û–î–ï–õ–ò =====")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self._log("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
            
            # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_size = training_args.get('image_size', 384)
            self._log(f"   üñºÔ∏è –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_size}x{image_size}")
            
            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            max_length = training_args.get('max_length', 512)
            self._log(f"   üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
            model.config.pad_token_id = processor.tokenizer.pad_token_id
            model.config.eos_token_id = processor.tokenizer.sep_token_id
            
            self._log("   üè∑Ô∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
            self._log(f"     decoder_start_token_id: {model.config.decoder_start_token_id} ‚úÖ")
            self._log(f"     pad_token_id: {model.config.pad_token_id}")
            self._log(f"     eos_token_id: {model.config.eos_token_id}")
            
            self._log("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            self._log("‚úÖ –ú–æ–¥–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –≠–¢–ê–ü 4: –ü–û–î–ì–û–¢–û–í–ö–ê DATA COLLATOR
            self._log("")
            self._log("üîß ===== –≠–¢–ê–ü 4: –ü–û–î–ì–û–¢–û–í–ö–ê DATA COLLATOR =====")
            self._log(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
            
            data_collator = TrOCRDataCollator(
                processor=processor,
                max_length=max_length
            )
            self._log("‚úÖ Data collator —Å–æ–∑–¥–∞–Ω")
            
            # –≠–¢–ê–ü 5: –ù–ê–°–¢–†–û–ô–ö–ê –ê–†–ì–£–ú–ï–ù–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø
            self._log("")
            self._log("üìã ===== –≠–¢–ê–ü 5: –ù–ê–°–¢–†–û–ô–ö–ê –ê–†–ì–£–ú–ï–ù–¢–û–í –û–ë–£–ß–ï–ù–ò–Ø =====")
            
            # –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
            output_dir = f"data/trained_models/{output_model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            output_dir = os.path.abspath(output_dir.replace('\\', '/'))
            self._log(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            # –ë–∞–∑–æ–≤—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            args = {
                'output_dir': output_dir,
                'num_train_epochs': training_args.get('num_train_epochs', 2),
                'per_device_train_batch_size': training_args.get('per_device_train_batch_size', 2),
                'per_device_eval_batch_size': training_args.get('per_device_eval_batch_size', 2),
                'learning_rate': training_args.get('learning_rate', 5e-5),
                'gradient_accumulation_steps': training_args.get('gradient_accumulation_steps', 4),
                'warmup_ratio': training_args.get('warmup_ratio', 0.1),
                'weight_decay': training_args.get('weight_decay', 0.01),
                'logging_steps': training_args.get('logging_steps', 10),
                'save_steps': training_args.get('save_steps', 500),
                'eval_steps': training_args.get('eval_steps', 500),
                'save_total_limit': training_args.get('save_total_limit', 3),
                'eval_strategy': 'no',
                'save_strategy': 'epoch',
                'logging_dir': './logs',
                'report_to': [],
                'load_best_model_at_end': False,
                'remove_unused_columns': False,  # –ö–†–ò–¢–ò–ß–ù–û –¥–ª—è TrOCR
                'prediction_loss_only': True,
            }
            
            # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
            if torch.cuda.is_available():
                self._log("üöÄ –ù–ê–°–¢–†–û–ô–ö–ê GPU –£–°–ö–û–†–ï–ù–ò–Ø:")
                
                # FP16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
                if training_args.get('fp16', True):
                    args['fp16'] = True
                    self._log("   ‚úÖ FP16 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–∫–ª—é—á–µ–Ω–∞")
                
                # GPU –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                cuda_version = torch.version.cuda
                
                self._log(f"   üéÆ GPU: {gpu_name}")
                self._log(f"   üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
                self._log(f"   ‚ö° CUDA –≤–µ—Ä—Å–∏—è: {cuda_version}")
                
                # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è GPU
                args['dataloader_num_workers'] = 0  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏
                args['dataloader_pin_memory'] = True
                self._log("   üß† Workers: 0 (–±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏)")
            
            self._log("‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self._log("üìä –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
            self._log(f"   –≠–ø–æ—Ö: {args['num_train_epochs']}")
            self._log(f"   Batch size (train): {args['per_device_train_batch_size']}")
            self._log(f"   Batch size (eval): {args['per_device_eval_batch_size']}")
            self._log(f"   Learning rate: {args['learning_rate']}")
            self._log(f"   Gradient accumulation: {args['gradient_accumulation_steps']}")
            self._log(f"   FP16: {args.get('fp16', False)}")
            
            # –≠–¢–ê–ü 6: –°–û–ó–î–ê–ù–ò–ï CALLBACKS
            self._log("")
            self._log("üîî ===== –≠–¢–ê–ü 6: –°–û–ó–î–ê–ù–ò–ï CALLBACKS =====")
            
            callbacks = []
            
            # Progress callback
            if self.progress_callback:
                progress_cb = TrOCRProgressCallback(self.progress_callback)
                callbacks.append(progress_cb)
            
            # Metrics callback  
            if self.metrics_callback:
                metrics_cb = TrOCRMetricsCallback(self.metrics_callback)
                callbacks.append(metrics_cb)
            
            # Early stopping - remove for now since it requires metric_for_best_model
            # callbacks.append(EarlyStoppingCallback(early_stopping_patience=3))
            
            # GPU monitoring
            gpu_monitor_cb = TrOCRGPUMonitorCallback(self._log)
            callbacks.append(gpu_monitor_cb)
            
            self._log(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ callbacks: {len(callbacks)}")
            for i, callback in enumerate(callbacks):
                self._log(f"   {i+1}. {callback.__class__.__name__}")
            
            # –≠–¢–ê–ü 7: –°–û–ó–î–ê–ù–ò–ï TRAINER
            self._log("")
            self._log("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ Trainer...")
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π Trainer —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
            class OptimizedTrOCRTrainer(Trainer):
                def __init__(self, *args, use_8bit_optimizer=False, learning_rate=5e-5, logger_func=None, **kwargs):
                    # –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑—ã–≤–∞–µ–º parent __init__
                    super().__init__(*args, **kwargs)
                    # –ó–∞—Ç–µ–º —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—à–∏ –∞—Ç—Ä–∏–±—É—Ç—ã
                    self.use_8bit_optimizer = use_8bit_optimizer
                    self.custom_learning_rate = learning_rate
                    self._log_func = logger_func or print  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                
                def _log(self, message):
                    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –º–µ—Ç–æ–¥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Unicode"""
                    # –°–æ–∑–¥–∞–µ–º ASCII-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é –≤–µ—Ä—Å–∏—é —Å–æ–æ–±—â–µ–Ω–∏—è
                    safe_message = message.encode('ascii', errors='replace').decode('ascii')
                    
                    try:
                        if hasattr(self, '_log_func') and self._log_func:
                            self._log_func(message)
                        elif hasattr(self, 'log') and callable(self.log):
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –º–µ—Ç–æ–¥ log –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
                            self.log(safe_message)
                        else:
                            # Fallback –∫ print
                            print(safe_message)
                    except Exception:
                        # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º print —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
                        print(safe_message)
                
                def create_optimizer(self):
                    """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π 8-bit"""
                    if self.use_8bit_optimizer and BITSANDBYTES_AVAILABLE:
                        try:
                            # –°–æ–∑–¥–∞–µ–º 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                            optimizer = bnb.optim.AdamW8bit(
                                self.model.parameters(),
                                lr=self.custom_learning_rate,
                                betas=(0.9, 0.999),
                                eps=1e-8,
                                weight_decay=0.01
                            )
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–µ –¥–ª—è Trainer
                            self.optimizer = optimizer
                            
                            self._log("‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-bit AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—ç–∫–æ–Ω–æ–º–∏—è ~25% –ø–∞–º—è—Ç–∏)")
                            return optimizer
                            
                        except Exception as e:
                            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è 8-bit –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞: {e}")
                            # Fallback –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—É
                            pass
                    
                    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                    return super().create_optimizer()
            
            # –°–æ–∑–¥–∞–µ–º training arguments
            training_arguments = TrainingArguments(**args)
            
            # –°–æ–∑–¥–∞–µ–º trainer
            trainer = OptimizedTrOCRTrainer(
                model=model,
                args=training_arguments,
                train_dataset=train_dataset,
                eval_dataset=dataset.get('validation', None),
                data_collator=data_collator,
                callbacks=callbacks,
                use_8bit_optimizer=training_args.get('use_8bit_optimizer', False),
                learning_rate=args['learning_rate'],
                logger_func=self._log  # –ü–µ—Ä–µ–¥–∞–µ–º –º–µ—Ç–æ–¥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            )
            
            self._log("‚úÖ Trainer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏
            train_batch_size = args['per_device_train_batch_size']
            gradient_accumulation = args['gradient_accumulation_steps']
            effective_batch_size = train_batch_size * gradient_accumulation
            
            steps_per_epoch = len(train_dataset) // effective_batch_size
            total_steps = steps_per_epoch * args['num_train_epochs']
            estimated_time_minutes = total_steps * 1 // 60  # –ü—Ä–∏–º–µ—Ä–Ω–æ 1 —Å–µ–∫/—à–∞–≥
            
            self._log("üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏:")
            self._log(f"   üìÑ –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(train_dataset)}")
            self._log(f"   üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {train_batch_size}")
            self._log(f"   üîÑ Gradient accumulation: {gradient_accumulation}")
            self._log(f"   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {effective_batch_size}")
            self._log(f"   üî¢ –®–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É: {steps_per_epoch}")
            self._log(f"   üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {total_steps}")
            self._log(f"   ‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è (–ø—Ä–∏ 1 —Å–µ–∫/—à–∞–≥): {estimated_time_minutes} –º–∏–Ω")
            
            # –≠–¢–ê–ü 8: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø
            self._log("")
            self._log("üöÄ ===== –≠–¢–ê–ü 8: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø =====")
            self._log("üéØ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –º–æ–¥–µ–ª–∏...")
            self._log(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now().strftime('%H:%M:%S')}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            training_result = trainer.train()
            
            # –≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò
            self._log("")
            self._log("üíæ ===== –≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            final_model_path = os.path.join(output_dir, "final_model")
            os.makedirs(final_model_path, exist_ok=True)
            
            trainer.save_model(final_model_path)
            processor.save_pretrained(final_model_path)
            
            self._log(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {final_model_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'model_type': 'trocr',
                'base_model': base_model_id,
                'training_params': training_args,
                'dataset_path': dataset_path,
                'dataset_size': len(train_dataset),
                'training_time': str(datetime.now()),
                'final_loss': float(training_result.training_loss) if hasattr(training_result, 'training_loss') else None,
                'total_steps': total_steps,
                'device': str(self.device)
            }
            
            metadata_path = os.path.join(final_model_path, "training_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self._log(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {metadata_path}")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            self._log("")
            self._log("üéâ ========== –û–ë–£–ß–ï–ù–ò–ï TrOCR –ó–ê–í–ï–†–®–ï–ù–û ==========")
            self._log("üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            self._log(f"   ‚è∞ –í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now().strftime('%H:%M:%S')}")
            self._log(f"   üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}")
            self._log(f"   üìÑ –î–∞—Ç–∞—Å–µ—Ç: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            self._log(f"   üî¢ –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {total_steps}")
            
            if hasattr(training_result, 'training_loss'):
                self._log(f"   üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π loss: {training_result.training_loss:.4f}")
            
            # –ü–∞–º—è—Ç—å GPU
            if torch.cuda.is_available():
                max_memory = torch.cuda.max_memory_allocated() / (1024**3)
                self._log(f"   üíæ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU: {max_memory:.2f} GB")
            
            self._log("‚úÖ –û–±—É—á–µ–Ω–∏–µ TrOCR –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
            return final_model_path
            
        except Exception as e:
            self._log("")
            self._log("üí• ========== –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø TrOCR ==========")
            self._log(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            self._log("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            self._log(f"   Python –≤–µ—Ä—Å–∏—è: {sys.version}")
            self._log(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
            self._log(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
            
            if torch.cuda.is_available():
                self._log(f"   CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
                self._log(f"   –¢–µ–∫—É—â–µ–µ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.current_device()}")
                self._log(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB")
            
            self._log(f"   –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"   –î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(dataset_path)}")
            
            # –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞
            import traceback
            self._log("")
            self._log("üîç –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    self._log(f"   {line}")
            
            self.logger.error(f"TrOCRTrainer error: {e}")
            self.logger.error(traceback.format_exc())
            
            return None
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            self._log("üõë –ó–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É –æ–±—É—á–µ–Ω–∏—è TrOCR...")
            if hasattr(self, 'current_trainer') and self.current_trainer is not None:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–π trainer - –ø—ã—Ç–∞–µ–º—Å—è –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
                self._log("üîÑ –ü–æ–ø—ã—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ trainer...")
                # Note: Transformers Trainer –Ω–µ –∏–º–µ–µ—Ç –ø—Ä—è–º–æ–≥–æ –º–µ—Ç–æ–¥–∞ stop
                # –û–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –∏–ª–∏ —Ñ–ª–∞–≥–∏
                self._log("‚ö†Ô∏è Trainer –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —á–µ—Ä–µ–∑ –≤–Ω–µ—à–Ω–µ–µ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ")
            else:
                self._log("‚ÑπÔ∏è –ê–∫—Ç–∏–≤–Ω—ã–π trainer –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ TrOCRTrainer: {e}", "warning") 