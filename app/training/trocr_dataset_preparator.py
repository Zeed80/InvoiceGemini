#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
TrOCR Dataset Preparator

–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è TrOCR –º–æ–¥–µ–ª–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã.

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö Microsoft Research –∏ —Å–æ–æ–±—â–µ—Å—Ç–≤–∞ HuggingFace.
"""

import os
import json
import shutil
import logging
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass
from PIL import Image, ImageEnhance, ImageOps
import pandas as pd
import torch
from torch.utils.data import Dataset
import numpy as np
from tqdm import tqdm

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è transformers - —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
try:
    from transformers import TrOCRProcessor, VisionEncoderDecoderModel
    import torchvision.transforms as transforms
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch torchvision")


@dataclass
class TrOCRDatasetConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ TrOCR"""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    model_name: str = "microsoft/trocr-base-stage1"  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è fine-tuning
    max_target_length: int = 128  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    image_size: Tuple[int, int] = (384, 384)  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    enable_augmentation: bool = True
    brightness_range: Tuple[float, float] = (0.7, 1.3)
    contrast_range: Tuple[float, float] = (0.8, 1.2)
    saturation_range: Tuple[float, float] = (0.8, 1.2)
    hue_range: Tuple[float, float] = (-0.1, 0.1)
    gaussian_blur_prob: float = 0.3
    gaussian_blur_kernel: Tuple[int, int] = (3, 7)
    gaussian_blur_sigma: Tuple[float, float] = (0.1, 2.0)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞—á–µ—Å—Ç–≤–∞
    min_image_size: Tuple[int, int] = (32, 32)  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    max_text_length: int = 200  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    min_text_length: int = 1   # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
    
    # –§–æ—Ä–º–∞—Ç—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
    supported_image_formats: List[str] = None
    
    def __post_init__(self):
        if self.supported_image_formats is None:
            self.supported_image_formats = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif']


class TrOCRCustomDataset(Dataset):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π Dataset –¥–ª—è TrOCR —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π
    """
    
    def __init__(self, 
                 data_pairs: List[Tuple[str, str]], 
                 processor,
                 config: TrOCRDatasetConfig,
                 is_training: bool = True):
        """
        Args:
            data_pairs: –°–ø–∏—Å–æ–∫ –ø–∞—Ä (–ø—É—Ç—å_–∫_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é, —Ç–µ–∫—Å—Ç)
            processor: TrOCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            is_training: –§–ª–∞–≥ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ (–¥–ª—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π)
        """
        self.data_pairs = data_pairs
        self.processor = processor
        self.config = config
        self.is_training = is_training
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        if is_training and config.enable_augmentation and TRANSFORMERS_AVAILABLE:
            self.augmentation_transform = transforms.Compose([
                transforms.ColorJitter(
                    brightness=config.brightness_range,
                    contrast=config.contrast_range,
                    saturation=config.saturation_range,
                    hue=config.hue_range
                ),
                transforms.RandomApply([
                    transforms.GaussianBlur(
                        kernel_size=config.gaussian_blur_kernel,
                        sigma=config.gaussian_blur_sigma
                    )
                ], p=config.gaussian_blur_prob)
            ])
        else:
            self.augmentation_transform = None
    
    def __len__(self):
        return len(self.data_pairs)
    
    def __getitem__(self, idx):
        image_path, text = self.data_pairs[idx]
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ —ç—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
            if self.augmentation_transform and self.is_training:
                image = self.augmentation_transform(image)
            
            if not TRANSFORMERS_AVAILABLE:
                # Fallback —Ä–µ–∂–∏–º –±–µ–∑ transformers
                return {
                    "image_path": image_path,
                    "text": text,
                    "image": image
                }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ TrOCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            pixel_values = self.processor(image, return_tensors='pt').pixel_values
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            labels = self.processor.tokenizer(
                text,
                padding='max_length',
                max_length=self.config.max_target_length,
                truncation=True,
                return_tensors='pt'
            ).input_ids
            
            # –ó–∞–º–µ–Ω—è–µ–º padding —Ç–æ–∫–µ–Ω—ã –Ω–∞ -100 (–∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è –≤ loss)
            labels = labels.squeeze()
            labels[labels == self.processor.tokenizer.pad_token_id] = -100
            
            return {
                "pixel_values": pixel_values.squeeze(),
                "labels": labels
            }
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if TRANSFORMERS_AVAILABLE:
                return {
                    "pixel_values": torch.zeros((3, self.config.image_size[0], self.config.image_size[1])),
                    "labels": torch.full((self.config.max_target_length,), -100, dtype=torch.long)
                }
            else:
                return {
                    "image_path": image_path,
                    "text": text,
                    "error": str(e)
                }


class TrOCRDatasetPreparator:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ TrOCR
    """
    
    def __init__(self, config: Optional[TrOCRDatasetConfig] = None):
        self.config = config or TrOCRDatasetConfig()
        self.logger = logging.getLogger(__name__)
        self.processor = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –µ—Å–ª–∏ transformers –¥–æ—Å—Ç—É–ø–µ–Ω
        if TRANSFORMERS_AVAILABLE:
            try:
                self.processor = TrOCRProcessor.from_pretrained(
                    self.config.model_name,
                    cache_dir="data/models"
                )
                self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω TrOCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä: {self.config.model_name}")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {e}")
                self.processor = None
        else:
            self.logger.warning("Transformers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —Ä–∞–±–æ—Ç–∞ –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ")
    
    def prepare_from_folder_structure(self,
                                    source_folder: str,
                                    output_path: str,
                                    train_split: float = 0.8,
                                    val_split: float = 0.1,
                                    test_split: float = 0.1) -> Dict[str, str]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫.
        
        –û–∂–∏–¥–∞–µ–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:
        source_folder/
        ‚îú‚îÄ‚îÄ images/
        ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg
        ‚îÇ   ‚îú‚îÄ‚îÄ img2.png
        ‚îÇ   ‚îî‚îÄ‚îÄ ...
        ‚îî‚îÄ‚îÄ annotations.txt  # –∏–ª–∏ .json, .csv
        
        Args:
            source_folder: –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ—Ç–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            train_split: –î–æ–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            val_split: –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            test_split: –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º
        """
        source_path = Path(source_folder)
        if not source_path.exists():
            raise FileNotFoundError(f"–ü–∞–ø–∫–∞ {source_folder} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        self.logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {source_folder}")
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        annotation_files = list(source_path.glob("*.txt")) + \
                          list(source_path.glob("*.json")) + \
                          list(source_path.glob("*.csv"))
        
        if not annotation_files:
            raise FileNotFoundError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (.txt, .json, .csv)")
        
        # –ü–∞—Ä—Å–∏–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        data_pairs = []
        for ann_file in annotation_files:
            pairs = self._parse_annotation_file(ann_file, source_path)
            data_pairs.extend(pairs)
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(data_pairs)} –ø–∞—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ-—Ç–µ–∫—Å—Ç")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –≤–∞–ª–∏–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        valid_pairs = self._validate_data_pairs(data_pairs)
        self.logger.info(f"–ü–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {len(valid_pairs)} –ø–∞—Ä")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test
        train_pairs, val_pairs, test_pairs = self._split_data(
            valid_pairs, train_split, val_split, test_split
        )
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        datasets = {}
        if train_pairs:
            datasets['train'] = self._create_dataset_split(
                train_pairs, output_dir / "train", is_training=True
            )
        if val_pairs:
            datasets['validation'] = self._create_dataset_split(
                val_pairs, output_dir / "validation", is_training=False
            )
        if test_pairs:
            datasets['test'] = self._create_dataset_split(
                test_pairs, output_dir / "test", is_training=False
            )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self._save_dataset_metadata(output_dir, datasets, valid_pairs)
        
        self.logger.info(f"–î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –≤: {output_path}")
        return datasets
    
    def prepare_from_invoice_annotations(self,
                                       images_folder: str,
                                       annotations_file: str,
                                       output_path: str,
                                       field_mapping: Optional[Dict[str, str]] = None) -> Dict[str, str]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å—á–µ—Ç–æ–≤ (JSON —Ñ–æ—Ä–º–∞—Ç).
        
        Args:
            images_folder: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å—á–µ—Ç–æ–≤
            annotations_file: JSON —Ñ–∞–π–ª —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            field_mapping: –ú–∞–ø–ø–∏–Ω–≥ –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            
        Returns:
            Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º
        """
        if field_mapping is None:
            field_mapping = {
                'invoice_number': '–ù–æ–º–µ—Ä —Å—á–µ—Ç–∞',
                'date': '–î–∞—Ç–∞',
                'supplier': '–ü–æ—Å—Ç–∞–≤—â–∏–∫',
                'total_amount': '–°—É–º–º–∞',
                'customer': '–ü–æ–∫—É–ø–∞—Ç–µ–ª—å'
            }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        with open(annotations_file, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
        
        data_pairs = []
        images_path = Path(images_folder)
        
        for annotation in tqdm(annotations, desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"):
            image_file = annotation.get('image_file')
            if not image_file:
                continue
                
            image_path = images_path / image_file
            if not image_path.exists():
                self.logger.warning(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_path}")
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
            extracted_data = annotation.get('extracted_data', {})
            for field, value in extracted_data.items():
                if field in field_mapping and value:
                    # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–µ—Ç–∫—É —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                    text_label = f"{field_mapping[field]}: {value}"
                    data_pairs.append((str(image_path), text_label))
        
        self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ {len(data_pairs)} –ø–∞—Ä –∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å—á–µ—Ç–æ–≤")
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∏ —Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        valid_pairs = self._validate_data_pairs(data_pairs)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        train_pairs, val_pairs, test_pairs = self._split_data(valid_pairs, 0.7, 0.15, 0.15)
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        datasets = {}
        if train_pairs:
            datasets['train'] = self._create_dataset_split(
                train_pairs, output_dir / "train", is_training=True
            )
        if val_pairs:
            datasets['validation'] = self._create_dataset_split(
                val_pairs, output_dir / "validation", is_training=False
            )
        if test_pairs:
            datasets['test'] = self._create_dataset_split(
                test_pairs, output_dir / "test", is_training=False
            )
        
        self._save_dataset_metadata(output_dir, datasets, valid_pairs)
        return datasets
    
    def prepare_synthetic_dataset(self,
                                output_path: str,
                                num_samples: int = 10000,
                                text_sources: Optional[List[str]] = None) -> Dict[str, str]:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è TrOCR.
        
        Args:
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
            text_sources: –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º
        """
        if text_sources is None:
            # –ë–∞–∑–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
            text_sources = [
                "–û–û–û \"–†–æ–≥–∞ –∏ –∫–æ–ø—ã—Ç–∞\"",
                "–°—á–µ—Ç-—Ñ–∞–∫—Ç—É—Ä–∞ ‚Ññ",
                "–æ—Ç {} –≥.",
                "–°—É–º–º–∞ –∫ –æ–ø–ª–∞—Ç–µ:",
                "–ù–î–° 20%:",
                "–ò—Ç–æ–≥–æ:",
                "–ü–æ–∫—É–ø–∞—Ç–µ–ª—å:",
                "–ü–æ—Å—Ç–∞–≤—â–∏–∫:",
                "–ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ–∫–≤–∏–∑–∏—Ç—ã",
                "–ò–ù–ù/–ö–ü–ü:",
                "–†–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç:",
            ]
        
        self.logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_samples} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        output_dir = Path(output_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        synthetic_pairs = self._generate_synthetic_data(
            text_sources, num_samples, output_dir
        )
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        train_pairs, val_pairs, test_pairs = self._split_data(
            synthetic_pairs, 0.8, 0.1, 0.1
        )
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
        datasets = {}
        datasets['train'] = self._create_dataset_split(
            train_pairs, output_dir / "train", is_training=True
        )
        datasets['validation'] = self._create_dataset_split(
            val_pairs, output_dir / "validation", is_training=False
        )
        datasets['test'] = self._create_dataset_split(
            test_pairs, output_dir / "test", is_training=False
        )
        
        self._save_dataset_metadata(output_dir, datasets, synthetic_pairs)
        return datasets
    
    def _parse_annotation_file(self, ann_file: Path, source_path: Path) -> List[Tuple[str, str]]:
        """–ü–∞—Ä—Å–∏—Ç —Ñ–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö"""
        
        pairs = []
        
        if ann_file.suffix == '.txt':
            # –§–æ—Ä–º–∞—Ç: filename.jpg\ttext_content
            with open(ann_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    
                    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –ø–µ—Ä–≤–æ–º—É —Ç–∞–±—É –∏–ª–∏ –ø—Ä–æ–±–µ–ª—É
                    parts = line.split('\t', 1)
                    if len(parts) != 2:
                        parts = line.split(' ', 1)
                    
                    if len(parts) == 2:
                        filename, text = parts
                        image_path = source_path / "images" / filename
                        if image_path.exists():
                            pairs.append((str(image_path), text.strip()))
        
        elif ann_file.suffix == '.json':
            # JSON —Ñ–æ—Ä–º–∞—Ç
            with open(ann_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                for item in data:
                    if 'image' in item and 'text' in item:
                        image_path = source_path / "images" / item['image']
                        if image_path.exists():
                            pairs.append((str(image_path), item['text']))
            elif isinstance(data, dict):
                for filename, text in data.items():
                    image_path = source_path / "images" / filename
                    if image_path.exists():
                        pairs.append((str(image_path), str(text)))
        
        elif ann_file.suffix == '.csv':
            # CSV —Ñ–æ—Ä–º–∞—Ç
            df = pd.read_csv(ann_file)
            
            # –ò—â–µ–º —Å—Ç–æ–ª–±—Ü—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
            image_cols = [col for col in df.columns if 'image' in col.lower() or 'file' in col.lower()]
            text_cols = [col for col in df.columns if 'text' in col.lower() or 'label' in col.lower()]
            
            if image_cols and text_cols:
                image_col = image_cols[0]
                text_col = text_cols[0]
                
                for _, row in df.iterrows():
                    filename = row[image_col]
                    text = row[text_col]
                    image_path = source_path / "images" / filename
                    if image_path.exists() and pd.notna(text):
                        pairs.append((str(image_path), str(text)))
        
        return pairs
    
    def _validate_data_pairs(self, data_pairs: List[Tuple[str, str]]) -> List[Tuple[str, str]]:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –ø–∞—Ä—ã –¥–∞–Ω–Ω—ã—Ö"""
        
        valid_pairs = []
        
        for image_path, text in tqdm(data_pairs, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"):
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image = Image.open(image_path)
                width, height = image.size
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if width < self.config.min_image_size[0] or height < self.config.min_image_size[1]:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç
                text = text.strip()
                if (len(text) < self.config.min_text_length or 
                    len(text) > self.config.max_text_length):
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image_ext = Path(image_path).suffix.lower()
                if image_ext not in self.config.supported_image_formats:
                    continue
                
                valid_pairs.append((image_path, text))
                
            except Exception as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ {image_path}: {e}")
                continue
        
        return valid_pairs
    
    def _split_data(self, data_pairs: List[Tuple[str, str]], 
                   train_split: float, val_split: float, test_split: float) -> Tuple[List, List, List]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ train/val/test"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—É–º–º–∞ –¥–æ–ª–µ–π —Ä–∞–≤–Ω–∞ 1
        total = train_split + val_split + test_split
        if abs(total - 1.0) > 0.01:
            raise ValueError(f"–°—É–º–º–∞ –¥–æ–ª–µ–π –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0, –ø–æ–ª—É—á–µ–Ω–æ: {total}")
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        np.random.shuffle(data_pairs)
        
        n_total = len(data_pairs)
        n_train = int(n_total * train_split)
        n_val = int(n_total * val_split)
        
        train_pairs = data_pairs[:n_train]
        val_pairs = data_pairs[n_train:n_train + n_val]
        test_pairs = data_pairs[n_train + n_val:]
        
        self.logger.info(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö: train={len(train_pairs)}, "
                        f"val={len(val_pairs)}, test={len(test_pairs)}")
        
        return train_pairs, val_pairs, test_pairs
    
    def _create_dataset_split(self, data_pairs: List[Tuple[str, str]], 
                            output_dir: Path, is_training: bool) -> str:
        """–°–æ–∑–¥–∞–µ—Ç split –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        annotations = []
        images_dir = output_dir / "images"
        images_dir.mkdir(exist_ok=True)
        
        for i, (image_path, text) in enumerate(tqdm(data_pairs, desc=f"–°–æ–∑–¥–∞–Ω–∏–µ {output_dir.name}")):
            try:
                # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_name = f"{i:06d}{Path(image_path).suffix}"
                target_image_path = images_dir / image_name
                shutil.copy2(image_path, target_image_path)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                annotations.append({
                    "image": image_name,
                    "text": text
                })
                
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {image_path}: {e}")
                continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        annotations_file = output_dir / "annotations.json"
        with open(annotations_file, 'w', encoding='utf-8') as f:
            json.dump(annotations, f, ensure_ascii=False, indent=2)
        
        # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º PyTorch Dataset –µ—Å–ª–∏ transformers –¥–æ—Å—Ç—É–ø–µ–Ω
        if TRANSFORMERS_AVAILABLE and self.processor:
            dataset = TrOCRCustomDataset(
                [(str(images_dir / ann["image"]), ann["text"]) for ann in annotations],
                self.processor,
                self.config,
                is_training=is_training
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º dataset
            dataset_file = output_dir / "dataset.pt"
            torch.save({
                'dataset': dataset,
                'config': self.config,
                'annotations': annotations
            }, dataset_file)
        
        return str(output_dir)
    
    def _generate_synthetic_data(self, text_sources: List[str], 
                               num_samples: int, output_dir: Path) -> List[Tuple[str, str]]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ"""
        
        pairs = []
        synthetic_dir = output_dir / "synthetic_images"
        synthetic_dir.mkdir(exist_ok=True)
        
        from PIL import ImageDraw, ImageFont
        
        # –ë–∞–∑–æ–≤—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        image_size = (400, 100)
        background_color = (255, 255, 255)
        text_color = (0, 0, 0)
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π —à—Ä–∏—Ñ—Ç
            font = ImageFont.truetype("arial.ttf", 24)
        except:
            font = ImageFont.load_default()
        
        for i in tqdm(range(num_samples), desc="–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"):
            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ç–µ–∫—Å—Ç
            text = np.random.choice(text_sources)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —á–∏—Å–ª–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
            if "‚Ññ" in text:
                text += str(np.random.randint(1000, 99999))
            elif ":" in text:
                text += f" {np.random.randint(100, 999999)} —Ä—É–±."
            
            # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.new('RGB', image_size, background_color)
            draw = ImageDraw.Draw(image)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é —Ç–µ–∫—Å—Ç–∞
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = (image_size[0] - text_width) // 2
            y = (image_size[1] - text_height) // 2
            
            # –†–∏—Å—É–µ–º —Ç–µ–∫—Å—Ç
            draw.text((x, y), text, fill=text_color, font=font)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à–æ–π —à—É–º
            if np.random.random() < 0.3:
                # –ù–µ–±–æ–ª—å—à–æ–µ —Ä–∞–∑–º—ã—Ç–∏–µ
                try:
                    image = image.filter(Image.BLUR)
                except:
                    pass  # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_name = f"synthetic_{i:06d}.png"
            image_path = synthetic_dir / image_name
            image.save(image_path)
            
            pairs.append((str(image_path), text))
        
        return pairs
    
    def _save_dataset_metadata(self, output_dir: Path, 
                             datasets: Dict[str, str], 
                             all_pairs: List[Tuple[str, str]]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        
        metadata = {
            "config": {
                "model_name": self.config.model_name,
                "max_target_length": self.config.max_target_length,
                "image_size": self.config.image_size,
                "enable_augmentation": self.config.enable_augmentation
            },
            "statistics": {
                "total_samples": len(all_pairs),
                "splits": {},
                "text_lengths": {
                    "min": min(len(text) for _, text in all_pairs) if all_pairs else 0,
                    "max": max(len(text) for _, text in all_pairs) if all_pairs else 0,
                    "avg": np.mean([len(text) for _, text in all_pairs]) if all_pairs else 0
                }
            },
            "paths": datasets,
            "created_at": pd.Timestamp.now().isoformat(),
            "transformers_available": TRANSFORMERS_AVAILABLE
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã splits
        for name, path in datasets.items():
            try:
                ann_file = Path(path) / "annotations.json"
                if ann_file.exists():
                    with open(ann_file, 'r', encoding='utf-8') as f:
                        annotations = json.load(f)
                    metadata["statistics"]["splits"][name] = len(annotations)
            except:
                metadata["statistics"]["splits"][name] = 0
        
        metadata_file = output_dir / "metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {metadata_file}")
    
    def load_prepared_dataset(self, dataset_path: str, split: str = "train"):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
            split: –ù–∞–∑–≤–∞–Ω–∏–µ split'–∞ ('train', 'validation', 'test')
            
        Returns:
            TrOCRCustomDataset –∏–ª–∏ dict —Å –¥–∞–Ω–Ω—ã–º–∏
        """
        dataset_file = Path(dataset_path) / split / "dataset.pt"
        
        if not dataset_file.exists():
            # –ï—Å–ª–∏ –Ω–µ—Ç .pt —Ñ–∞–π–ª–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            ann_file = Path(dataset_path) / split / "annotations.json"
            if ann_file.exists():
                with open(ann_file, 'r', encoding='utf-8') as f:
                    annotations = json.load(f)
                
                images_dir = Path(dataset_path) / split / "images"
                data_pairs = [(str(images_dir / ann["image"]), ann["text"]) for ann in annotations]
                
                if TRANSFORMERS_AVAILABLE and self.processor:
                    return TrOCRCustomDataset(data_pairs, self.processor, self.config, split=="train")
                else:
                    return {"data_pairs": data_pairs, "split": split}
            else:
                raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_file} –∏–ª–∏ {ann_file}")
        
        data = torch.load(dataset_file)
        return data['dataset']
    
    def get_dataset_info(self, dataset_path: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
        """
        metadata_file = Path(dataset_path) / "metadata.json"
        
        if not metadata_file.exists():
            raise FileNotFoundError(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {metadata_file}")
        
        with open(metadata_file, 'r', encoding='utf-8') as f:
            return json.load(f)


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º

def create_trocr_dataset_from_invoices(images_folder: str,
                                     annotations_file: str,
                                     output_path: str,
                                     config: Optional[TrOCRDatasetConfig] = None) -> Dict[str, str]:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è TrOCR –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π —Å—á–µ—Ç–æ–≤
    
    Args:
        images_folder: –ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å—á–µ—Ç–æ–≤  
        annotations_file: JSON —Ñ–∞–π–ª —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º split'–∞–º
    """
    preparator = TrOCRDatasetPreparator(config)
    return preparator.prepare_from_invoice_annotations(
        images_folder, annotations_file, output_path
    )


def create_synthetic_trocr_dataset(output_path: str,
                                 num_samples: int = 10000,
                                 config: Optional[TrOCRDatasetConfig] = None) -> Dict[str, str]:
    """
    –°–æ–∑–¥–∞–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π TrOCR –¥–∞—Ç–∞—Å–µ—Ç
    
    Args:
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        Dict —Å –ø—É—Ç—è–º–∏ –∫ —Å–æ–∑–¥–∞–Ω–Ω—ã–º split'–∞–º
    """
    preparator = TrOCRDatasetPreparator(config)
    return preparator.prepare_synthetic_dataset(output_path, num_samples)


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    logging.basicConfig(level=logging.INFO)
    
    print("üöÄ TrOCR Dataset Preparator")
    print(f"üì¶ Transformers –¥–æ—Å—Ç—É–ø–µ–Ω: {'‚úÖ' if TRANSFORMERS_AVAILABLE else '‚ùå'}")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = TrOCRDatasetConfig(
        model_name="microsoft/trocr-base-stage1",
        enable_augmentation=True,
        max_target_length=128
    )
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–ø–∞—Ä–∞—Ç–æ—Ä
    preparator = TrOCRDatasetPreparator(config)
    
    # –ü—Ä–∏–º–µ—Ä —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    try:
        datasets = preparator.prepare_synthetic_dataset(
            output_path="data/trocr_synthetic_dataset",
            num_samples=100  # –ú–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–∏–º–µ—Ä
        )
        
        print("‚úÖ –°–æ–∑–¥–∞–Ω—ã –¥–∞—Ç–∞—Å–µ—Ç—ã:")
        for split_name, split_path in datasets.items():
            print(f"  üìÅ {split_name}: {split_path}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}") 