"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è
–û—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ ML/AI –¥–ª—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö

–†–µ–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
1. –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã (Golden datasets)
2. –ö–æ–Ω—Å–µ–Ω—Å—É—Å-–∞–ª–≥–æ—Ä–∏—Ç–º—ã (Multiple passthroughs)
3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –æ—à–∏–±–æ–∫
4. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (Cohen's Kappa, Fleiss' Kappa)
5. –í–∞–ª–∏–¥–∞—Ü–∏—é —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã
6. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
- https://www.damcogroup.com/blogs/strategies-to-enhance-data-annotation-accuracy
- https://medium.com/datatorch/6-qa-tactics-for-data-annotation-jobs-8a17b83a46e6
"""

import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any, Set
from dataclasses import dataclass
from collections import defaultdict, Counter
import re
import math
from difflib import SequenceMatcher
from statistics import mode, median, mean


@dataclass
class AnnotationQualityMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    inter_annotator_agreement: float
    confidence_score: float
    edge_cases_detected: int
    consensus_level: float


@dataclass
class FieldExtraction:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª—è"""
    field_name: str
    value: str
    confidence: float
    method: str  # 'gemini', 'ocr', 'pdf_text', 'pattern'
    position: Optional[Tuple[int, int, int, int]] = None
    normalized_value: Optional[str] = None


@dataclass
class ConsensusResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å-–∞–ª–≥–æ—Ä–∏—Ç–º–∞"""
    final_value: str
    confidence: float
    agreement_score: float
    participating_methods: List[str]
    is_edge_case: bool
    conflict_details: Optional[Dict] = None


class DataQualityEnhancer:
    """
    –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    –†–µ–∞–ª–∏–∑—É–µ—Ç –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–Ω–¥—É—Å—Ç—Ä–∏–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    """
    
    def __init__(self, logger=None):
        self.logger = logger or logging.getLogger(__name__)
        
        # –≠—Ç–∞–ª–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–ª–µ–π (Golden patterns)
        self.golden_patterns = {
            'invoice_number': [
                r'‚Ññ\s*(\d+(?:[-/]\d+)*)',
                r'—Å—á–µ—Ç\s*‚Ññ?\s*(\d+(?:[-/]\d+)*)',
                r'invoice\s*#?\s*(\d+(?:[-/]\d+)*)',
                r'–∏–Ω–≤–æ–π—Å\s*‚Ññ?\s*(\d+(?:[-/]\d+)*)',
                r'–£–¢-(\d+)',
                r'–°–ß-(\d+)',
                r'N\s*(\d+)',
            ],
            'invoice_date': [
                r'(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})',
                r'(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
                r'–¥–∞—Ç–∞:?\s*(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})',
                r'–æ—Ç\s*(\d{1,2}[.\-/]\d{1,2}[.\-/]\d{4})',
                r'(\d{1,2}\s+\w+\s+\d{4})',  # "25 –∞–ø—Ä–µ–ª—è 2024"
            ],
            'company_inn': [
                r'–∏–Ω–Ω:?\s*(\d{10,12})',
                r'–ò–ù–ù:?\s*(\d{10,12})',
                r'inn:?\s*(\d{10,12})',
                r'(\d{10})',  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ò–ù–ù
                r'(\d{12})',  # –ò–ü –ò–ù–ù
            ],
            'company_kpp': [
                r'–∫–ø–ø:?\s*(\d{9})',
                r'–ö–ü–ü:?\s*(\d{9})',
                r'kpp:?\s*(\d{9})',
            ],
            'total_amount': [
                r'–∏—Ç–æ–≥–æ:?\s*(\d+(?:[.,]\d+)?)',
                r'—Å—É–º–º–∞:?\s*(\d+(?:[.,]\d+)?)',
                r'–∫\s*–æ–ø–ª–∞—Ç–µ:?\s*(\d+(?:[.,]\d+)?)',
                r'–≤—Å–µ–≥–æ:?\s*(\d+(?:[.,]\d+)?)',
                r'total:?\s*(\d+(?:[.,]\d+)?)',
            ],
        }
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        self.month_names = {
            '—è–Ω–≤–∞—Ä—è': '01', '—Ñ–µ–≤—Ä–∞–ª—è': '02', '–º–∞—Ä—Ç–∞': '03', '–∞–ø—Ä–µ–ª—è': '04',
            '–º–∞—è': '05', '–∏—é–Ω—è': '06', '–∏—é–ª—è': '07', '–∞–≤–≥—É—Å—Ç–∞': '08',
            '—Å–µ–Ω—Ç—è–±—Ä—è': '09', '–æ–∫—Ç—è–±—Ä—è': '10', '–Ω–æ—è–±—Ä—è': '11', '–¥–µ–∫–∞–±—Ä—è': '12'
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_stats = defaultdict(list)
        
        self.logger.info("‚úÖ DataQualityEnhancer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def extract_fields_with_multiple_methods(self, 
                                           text: str, 
                                           pdf_text_blocks: List[Dict] = None,
                                           gemini_result: Dict = None,
                                           ocr_result: Dict = None) -> Dict[str, List[FieldExtraction]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é "Multiple passthroughs" –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        
        Returns:
            Dict[field_name, List[FieldExtraction]] - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
        """
        all_extractions = defaultdict(list)
        
        # 1. –ü–∞—Ç—Ç–µ—Ä–Ω-–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
        pattern_results = self._extract_with_patterns(text)
        for field_name, value in pattern_results.items():
            if value:
                all_extractions[field_name].append(
                    FieldExtraction(
                        field_name=field_name,
                        value=value,
                        confidence=0.8,
                        method='pattern',
                        normalized_value=self._normalize_field_value(field_name, value)
                    )
                )
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ PDF —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤
        if pdf_text_blocks:
            pdf_results = self._extract_from_pdf_blocks(pdf_text_blocks)
            for field_name, value in pdf_results.items():
                if value:
                    all_extractions[field_name].append(
                        FieldExtraction(
                            field_name=field_name,
                            value=value,
                            confidence=0.9,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è PDF —Ç–µ–∫—Å—Ç–∞
                            method='pdf_text',
                            normalized_value=self._normalize_field_value(field_name, value)
                        )
                    )
        
        # 3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã Gemini
        if gemini_result:
            for field_name, value in gemini_result.items():
                if value:
                    all_extractions[field_name].append(
                        FieldExtraction(
                            field_name=field_name,
                            value=str(value),
                            confidence=0.85,
                            method='gemini',
                            normalized_value=self._normalize_field_value(field_name, str(value))
                        )
                    )
        
        # 4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR
        if ocr_result:
            for field_name, value in ocr_result.items():
                if value:
                    all_extractions[field_name].append(
                        FieldExtraction(
                            field_name=field_name,
                            value=str(value),
                            confidence=0.7,  # –ù–∏–∂–µ –∏–∑-–∑–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—à–∏–±–æ–∫ OCR
                            method='ocr',
                            normalized_value=self._normalize_field_value(field_name, str(value))
                        )
                    )
        
        return dict(all_extractions)
    
    def apply_consensus_algorithm(self, 
                                extractions: Dict[str, List[FieldExtraction]]) -> Dict[str, ConsensusResult]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∫–æ–Ω—Å–µ–Ω—Å—É—Å-–∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è
        –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ Fleiss' Kappa –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤
        
        –†–µ–∞–ª–∏–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:
        - Weighted voting based on method reliability
        - Automatic edge case detection
        - Confidence scoring
        """
        consensus_results = {}
        
        for field_name, field_extractions in extractions.items():
            if not field_extractions:
                continue
                
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º
            value_groups = defaultdict(list)
            for extraction in field_extractions:
                normalized = extraction.normalized_value or extraction.value
                value_groups[normalized].append(extraction)
            
            # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            if len(value_groups) == 1:
                consensus_value = list(value_groups.keys())[0]
                all_extractions = list(value_groups.values())[0]
                
                # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                avg_confidence = mean([e.confidence for e in all_extractions])
                
                consensus_results[field_name] = ConsensusResult(
                    final_value=consensus_value,
                    confidence=min(0.95, avg_confidence + 0.1),  # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Å–µ–Ω—Å—É—Å
                    agreement_score=1.0,
                    participating_methods=[e.method for e in all_extractions],
                    is_edge_case=False
                )
            
            # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π - –Ω—É–∂–µ–Ω –∫–æ–Ω—Å–µ–Ω—Å—É—Å
            else:
                consensus_result = self._resolve_conflict(field_name, value_groups)
                consensus_results[field_name] = consensus_result
        
        return consensus_results
    
    def _resolve_conflict(self, 
                         field_name: str, 
                         value_groups: Dict[str, List[FieldExtraction]]) -> ConsensusResult:
        """
        –†–∞–∑—Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ–ª—è
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤
        """
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
        weighted_values = []
        
        for value, extractions in value_groups.items():
            # –ë–∞–∑–æ–≤—ã–π –≤–µ—Å = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤ * —Å—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            base_weight = len(extractions) * mean([e.confidence for e in extractions])
            
            # –ë–æ–Ω—É—Å—ã –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            method_bonus = 0
            methods = [e.method for e in extractions]
            
            if 'pdf_text' in methods:
                method_bonus += 0.3  # PDF —Ç–µ–∫—Å—Ç –æ—á–µ–Ω—å –Ω–∞–¥–µ–∂–µ–Ω
            if 'gemini' in methods:
                method_bonus += 0.2  # Gemini —Ö–æ—Ä–æ—à –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if 'pattern' in methods:
                method_bonus += 0.15  # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–¥–µ–∂–Ω—ã –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è
            validity_bonus = self._validate_field_value(field_name, value)
            
            total_weight = base_weight + method_bonus + validity_bonus
            
            weighted_values.append((value, total_weight, extractions, methods))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–µ—Å—É
        weighted_values.sort(key=lambda x: x[1], reverse=True)
        
        best_value, best_weight, best_extractions, best_methods = weighted_values[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º agreement score
        total_extractions = sum(len(extractions) for extractions in value_groups.values())
        agreement_score = len(best_extractions) / total_extractions
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å–ø–æ—Ä–Ω—ã–º —Å–ª—É—á–∞–µ–º
        is_edge_case = agreement_score < 0.7 or len(value_groups) > 2
        
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        confidence = min(0.9, best_weight / 2.0 * agreement_score)
        
        return ConsensusResult(
            final_value=best_value,
            confidence=confidence,
            agreement_score=agreement_score,
            participating_methods=list(set(best_methods)),
            is_edge_case=is_edge_case,
            conflict_details={
                'all_values': list(value_groups.keys()),
                'weights': [w for _, w, _, _ in weighted_values],
                'chosen_reason': f'Highest weight: {best_weight:.2f}'
            }
        )
    
    def _extract_with_patterns(self, text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π —Å –ø–æ–º–æ—â—å—é —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö —Ä–µ–≥—É–ª—è—Ä–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π"""
        results = {}
        
        for field_name, patterns in self.golden_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                if matches:
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    value = matches[0] if isinstance(matches[0], str) else matches[0][0]
                    results[field_name] = value.strip()
                    break
        
        return results
    
    def _extract_from_pdf_blocks(self, pdf_blocks: List[Dict]) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π –∏–∑ PDF —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤"""
        # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ –±–ª–æ–∫–æ–≤
        all_text = " ".join([block.get('text', '') for block in pdf_blocks])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫ —Ç–µ–∫—Å—Ç—É PDF
        return self._extract_with_patterns(all_text)
    
    def _normalize_field_value(self, field_name: str, value: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è –¥–ª—è –ª—É—á—à–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not value:
            return value
            
        value = value.strip()
        
        if field_name == 'invoice_date':
            return self._normalize_date(value)
        elif field_name in ['company_inn', 'company_kpp']:
            return re.sub(r'[^\d]', '', value)  # –¢–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã
        elif field_name == 'total_amount':
            return self._normalize_amount(value)
        elif field_name == 'invoice_number':
            return self._normalize_invoice_number(value)
        
        return value.lower().strip()
    
    def _normalize_date(self, date_str: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞—Ç—ã –∫ —Ñ–æ—Ä–º–∞—Ç—É DD.MM.YYYY"""
        # –ó–∞–º–µ–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –º–µ—Å—è—Ü–µ–≤ –Ω–∞ —á–∏—Å–ª–∞
        normalized = date_str.lower()
        for month_name, month_num in self.month_names.items():
            normalized = normalized.replace(month_name, month_num)
        
        # –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–∞—Ç—ã
        patterns = [
            r'(\d{1,2})[.\-/](\d{1,2})[.\-/](\d{4})',
            r'(\d{4})[.\-/](\d{1,2})[.\-/](\d{1,2})',
            r'(\d{1,2})\s+(\d{1,2})\s+(\d{4})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, normalized)
            if match:
                d, m, y = match.groups()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –¥–Ω—è –∏ –º–µ—Å—è—Ü–∞
                if len(d) == 4:  # –ì–æ–¥ –≤ –Ω–∞—á–∞–ª–µ
                    y, m, d = d, m, y
                
                return f"{d.zfill(2)}.{m.zfill(2)}.{y}"
        
        return date_str
    
    def _normalize_amount(self, amount_str: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–º–º—ã"""
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
        cleaned = re.sub(r'[^\d.,]', '', amount_str)
        
        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É
        cleaned = cleaned.replace(',', '.')
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Ç–æ—á–∫–∏
        parts = cleaned.split('.')
        if len(parts) > 2:
            cleaned = '.'.join(parts[:2])
        
        return cleaned
    
    def _normalize_invoice_number(self, number_str: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–æ–º–µ—Ä–∞ —Å—á–µ—Ç–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–∏–º–≤–æ–ª—ã
        cleaned = re.sub(r'[^\w\-/]', '', number_str)
        return cleaned.upper()
    
    def _validate_field_value(self, field_name: str, value: str) -> float:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è
        Returns: –±–æ–Ω—É—Å –∫ –≤–µ—Å—É (0.0 - 0.5)
        """
        if not value:
            return 0.0
        
        if field_name == 'company_inn':
            # –ò–ù–ù –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 10 –∏–ª–∏ 12 —Ü–∏—Ñ—Ä
            if re.match(r'^\d{10}$', value) or re.match(r'^\d{12}$', value):
                return 0.4
            return 0.0
        
        elif field_name == 'company_kpp':
            # –ö–ü–ü –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 9 —Ü–∏—Ñ—Ä
            if re.match(r'^\d{9}$', value):
                return 0.4
            return 0.0
        
        elif field_name == 'invoice_date':
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
            if re.match(r'^\d{1,2}\.\d{1,2}\.\d{4}$', value):
                return 0.3
            return 0.1
        
        elif field_name == 'total_amount':
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Å—É–º–º—ã
            if re.match(r'^\d+(\.\d{1,2})?$', value):
                return 0.3
            return 0.1
        
        elif field_name == 'invoice_number':
            # –ù–æ–º–µ—Ä —Å—á–µ—Ç–∞ –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—É—Å—Ç—ã–º –∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º
            if 1 <= len(value) <= 50:
                return 0.2
            return 0.0
        
        return 0.1  # –ù–µ–±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    
    def calculate_quality_metrics(self, 
                                 consensus_results: Dict[str, ConsensusResult],
                                 expected_fields: Set[str] = None) -> AnnotationQualityMetrics:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ML –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        if expected_fields is None:
            expected_fields = set(self.golden_patterns.keys())
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        total_fields = len(expected_fields)
        extracted_fields = len(consensus_results)
        high_confidence_fields = len([r for r in consensus_results.values() if r.confidence > 0.8])
        edge_cases = len([r for r in consensus_results.values() if r.is_edge_case])
        
        # Accuracy = –¥–æ–ª—è —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
        accuracy = extracted_fields / total_fields if total_fields > 0 else 0.0
        
        # Precision = –¥–æ–ª—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏–π
        precision = high_confidence_fields / extracted_fields if extracted_fields > 0 else 0.0
        
        # Recall = –¥–æ–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –æ–∂–∏–¥–∞–µ–º—ã—Ö –ø–æ–ª–µ–π
        recall = extracted_fields / total_fields if total_fields > 0 else 0.0
        
        # F1 Score
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # Inter-annotator agreement (—Å—Ä–µ–¥–Ω–∏–π agreement score)
        agreement_scores = [r.agreement_score for r in consensus_results.values()]
        inter_annotator_agreement = mean(agreement_scores) if agreement_scores else 0.0
        
        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence_scores = [r.confidence for r in consensus_results.values()]
        confidence_score = mean(confidence_scores) if confidence_scores else 0.0
        
        # –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞
        consensus_level = len([r for r in consensus_results.values() if not r.is_edge_case]) / len(consensus_results) if consensus_results else 0.0
        
        return AnnotationQualityMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            inter_annotator_agreement=inter_annotator_agreement,
            confidence_score=confidence_score,
            edge_cases_detected=edge_cases,
            consensus_level=consensus_level
        )
    
    def generate_quality_report(self, 
                              metrics: AnnotationQualityMetrics,
                              consensus_results: Dict[str, ConsensusResult]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        """
        report = []
        report.append("=" * 70)
        report.append("üìä –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –ê–ù–ù–û–¢–ê–¶–ò–ò –î–ê–ù–ù–´–•")
        report.append("=" * 70)
        
        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        overall_quality = (metrics.accuracy + metrics.f1_score + metrics.confidence_score) / 3
        quality_grade = "üî• –û–¢–õ–ò–ß–ù–û" if overall_quality > 0.9 else "‚úÖ –•–û–†–û–®–û" if overall_quality > 0.7 else "‚ö†Ô∏è –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø"
        
        report.append(f"üéØ –û–ë–©–ê–Ø –û–¶–ï–ù–ö–ê: {quality_grade} ({overall_quality:.1%})")
        report.append("")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        report.append("üìà –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
        report.append(f"  üéØ –¢–æ—á–Ω–æ—Å—Ç—å (Accuracy): {metrics.accuracy:.1%}")
        report.append(f"  üîç –¢–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (Precision): {metrics.precision:.1%}")
        report.append(f"  üìä –ü–æ–ª–Ω–æ—Ç–∞ (Recall): {metrics.recall:.1%}")
        report.append(f"  ‚öñÔ∏è F1-Score: {metrics.f1_score:.1%}")
        report.append(f"  ü§ù –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤: {metrics.inter_annotator_agreement:.1%}")
        report.append(f"  üí™ –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {metrics.confidence_score:.1%}")
        report.append(f"  ‚úÖ –£—Ä–æ–≤–µ–Ω—å –∫–æ–Ω—Å–µ–Ω—Å—É—Å–∞: {metrics.consensus_level:.1%}")
        report.append(f"  ‚ö†Ô∏è –°–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤: {metrics.edge_cases_detected}")
        
        report.append("\n" + "=" * 50)
        report.append("üìã –î–ï–¢–ê–õ–ò –ü–û –ü–û–õ–Ø–ú:")
        report.append("=" * 50)
        
        for field_name, result in consensus_results.items():
            status = "‚úÖ" if not result.is_edge_case else "‚ö†Ô∏è"
            quality_indicator = "üî•" if result.confidence > 0.9 else "‚úÖ" if result.confidence > 0.7 else "‚ö†Ô∏è"
            
            report.append(f"{status} {quality_indicator} {field_name.upper()}:")
            report.append(f"    üí∞ –ó–Ω–∞—á–µ–Ω–∏–µ: {result.final_value}")
            report.append(f"    üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.1%}")
            report.append(f"    ü§ù –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å: {result.agreement_score:.1%}")
            report.append(f"    üîß –ú–µ—Ç–æ–¥—ã: {', '.join(result.participating_methods)}")
            
            if result.is_edge_case and result.conflict_details:
                report.append(f"    ‚ö†Ô∏è –ö–æ–Ω—Ñ–ª–∏–∫—Ç: {result.conflict_details.get('chosen_reason', 'N/A')}")
                report.append(f"    üîÑ –í–∞—Ä–∏–∞–Ω—Ç—ã: {', '.join(result.conflict_details.get('all_values', []))}")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        report.append("\n" + "=" * 50)
        report.append("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ:")
        report.append("=" * 50)
        
        recommendations = []
        
        if metrics.accuracy < 0.9:
            recommendations.append("üîß –ù–∏–∑–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å - –¥–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        
        if metrics.inter_annotator_agreement < 0.8:
            recommendations.append("üîß –ù–∏–∑–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ –º–µ—Ç–æ–¥—ã")
        
        if metrics.edge_cases_detected > len(consensus_results) * 0.2:
            recommendations.append("üîß –ú–Ω–æ–≥–æ —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ - —Ç—Ä–µ–±—É–µ—Ç—Å—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è")
        
        if metrics.confidence_score < 0.8:
            recommendations.append("üîß –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - —É–ª—É—á—à–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        
        if not recommendations:
            recommendations.append("üéâ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ! –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ.")
        
        for rec in recommendations:
            report.append(f"  {rec}")
        
        # –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏
        report.append("\n" + "=" * 50)
        report.append("üöÄ –°–õ–ï–î–£–Æ–©–ò–ï –®–ê–ì–ò:")
        report.append("=" * 50)
        
        if overall_quality > 0.9:
            report.append("  ‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–º—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            report.append("  üîÑ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞")
        elif overall_quality > 0.7:
            report.append("  üîß –ü—Ä–æ–≤–µ–¥–∏—Ç–µ —Ç–æ—á–µ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø–æ–ª–µ–π")
            report.append("  üìä –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        else:
            report.append("  ‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã")
            report.append("  üë• –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
        
        report.append("\n" + "=" * 70)
        
        return "\n".join(report)
    
    def save_quality_report(self, 
                          report: str, 
                          output_path: Path,
                          metrics: AnnotationQualityMetrics,
                          consensus_results: Dict[str, ConsensusResult]):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∏ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–∞–π–ª—ã
        """
        output_path = Path(output_path)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        report_file = output_path / "data_quality_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        # JSON —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        metrics_file = output_path / "quality_metrics.json"
        metrics_data = {
            'timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else 'N/A',
            'overall_quality': (metrics.accuracy + metrics.f1_score + metrics.confidence_score) / 3,
            'metrics': {
                'accuracy': metrics.accuracy,
                'precision': metrics.precision,
                'recall': metrics.recall,
                'f1_score': metrics.f1_score,
                'inter_annotator_agreement': metrics.inter_annotator_agreement,
                'confidence_score': metrics.confidence_score,
                'edge_cases_detected': metrics.edge_cases_detected,
                'consensus_level': metrics.consensus_level
            },
            'field_details': {
                field_name: {
                    'final_value': result.final_value,
                    'confidence': result.confidence,
                    'agreement_score': result.agreement_score,
                    'participating_methods': result.participating_methods,
                    'is_edge_case': result.is_edge_case,
                    'conflict_details': result.conflict_details
                }
                for field_name, result in consensus_results.items()
            }
        }
        
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"‚úÖ –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        self.logger.info(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_file}")
        
        return report_file, metrics_file


def calculate_fleiss_kappa(ratings_matrix: np.ndarray) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Fleiss' Kappa –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤
    
    Args:
        ratings_matrix: –º–∞—Ç—Ä–∏—Ü–∞ –æ—Ü–µ–Ω–æ–∫ [n_items, n_categories]
    
    Returns:
        float: –∑–Ω–∞—á–µ–Ω–∏–µ Fleiss' Kappa (-1 –¥–æ 1)
        
    –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
        < 0.00: –ü–ª–æ—Ö–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
        0.00-0.20: –°–ª–∞–±–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ  
        0.21-0.40: –°–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
        0.41-0.60: –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
        0.61-0.80: –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
        0.81-1.00: –ü–æ—á—Ç–∏ –∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
    """
    n_items, n_categories = ratings_matrix.shape
    n_raters = ratings_matrix.sum(axis=1).max()
    
    # –ü—Ä–æ–ø–æ—Ä—Ü–∏—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    p_j = ratings_matrix.sum(axis=0) / (n_items * n_raters)
    
    # –°—Ä–µ–¥–Ω—è—è –ø—Ä–æ–ø–æ—Ä—Ü–∏—è —Å–æ–≥–ª–∞—Å–∏—è
    P_e = (p_j ** 2).sum()
    
    # –ù–∞–±–ª—é–¥–∞–µ–º–∞—è –ø—Ä–æ–ø–æ—Ä—Ü–∏—è —Å–æ–≥–ª–∞—Å–∏—è
    P_i = []
    for i in range(n_items):
        r_i = ratings_matrix[i]
        if r_i.sum() > 1:
            P_i.append((r_i * (r_i - 1)).sum() / (r_i.sum() * (r_i.sum() - 1)))
        else:
            P_i.append(0)
    
    P_o = np.mean(P_i)
    
    if P_e == 1:
        return 1 if P_o == 1 else 0
    
    kappa = (P_o - P_e) / (1 - P_e)
    return kappa


def calculate_cohens_kappa(rater1: List, rater2: List) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç Cohen's Kappa –¥–ª—è –¥–≤—É—Ö –∞–Ω–Ω–æ—Ç–∞—Ç–æ—Ä–æ–≤
    
    Args:
        rater1, rater2: —Å–ø–∏—Å–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    
    Returns:
        float: –∑–Ω–∞—á–µ–Ω–∏–µ Cohen's Kappa (-1 –¥–æ 1)
    """
    if len(rater1) != len(rater2):
        raise ValueError("–°–ø–∏—Å–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã")
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø—É—Ç–∞–Ω–∏—Ü—ã
    categories = list(set(rater1 + rater2))
    n_categories = len(categories)
    cat_to_idx = {cat: idx for idx, cat in enumerate(categories)}
    
    confusion_matrix = np.zeros((n_categories, n_categories))
    
    for r1, r2 in zip(rater1, rater2):
        i, j = cat_to_idx[r1], cat_to_idx[r2]
        confusion_matrix[i, j] += 1
    
    n = len(rater1)
    
    # –ù–∞–±–ª—é–¥–∞–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    po = np.trace(confusion_matrix) / n
    
    # –û–∂–∏–¥–∞–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    pe = 0
    for i in range(n_categories):
        pe += (confusion_matrix[i, :].sum() / n) * (confusion_matrix[:, i].sum() / n)
    
    if pe == 1:
        return 1 if po == 1 else 0
    
    kappa = (po - pe) / (1 - pe)
    return kappa 