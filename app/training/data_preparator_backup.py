import logging
import os
import sys
import json
import tempfile
import re
import difflib
from PIL import Image
import torch
from pdf2image import convert_from_path
from datetime import datetime
import shutil
import numpy as np
import albumentations as A
from typing import Dict, List, Optional, Tuple, Union
from collections import Counter
from sklearn.utils.class_weight import compute_class_weight

# NEW: –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è Hugging Face Datasets –∏ Transformers
from datasets import Dataset, Features, Sequence, ClassLabel, Value, Array2D, DatasetDict, load_from_disk
from transformers import AutoTokenizer, AutoProcessor, LayoutLMv3Processor, DonutProcessor

# –ò–º–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ PDF
from app.pdf_text_analyzer import PDFTextAnalyzer

def normalize_text_for_matching(text):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è.
    
    Args:
        text (str): –¢–µ–∫—Å—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        
    Returns:
        str: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    if not isinstance(text, str):
        return ''
        
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
    replacements = {
        '–æ–æ–æ': '–æ–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é',
        '–æ–∞–æ': '–æ—Ç–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ',
        '–∑–∞–æ': '–∑–∞–∫—Ä—ã—Ç–æ–µ –∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ',
        '–∏–ø': '–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å',
        '—Ä—É–±.': '—Ä—É–±',
        '—Ä.': '—Ä—É–±',
        '‚ÇΩ': '—Ä—É–±',
        '—Ä—É–±–ª–µ–π': '—Ä—É–±',
        '—Ä—É–±–ª—å': '—Ä—É–±',
        '—Ä—É–±–ª—è': '—Ä—É–±',
        '–∫–æ–ø.': '–∫–æ–ø',
        '–∫–æ–ø–µ–µ–∫': '–∫–æ–ø',
        '–∫–æ–ø–µ–π–∫–∞': '–∫–æ–ø',
        '–∫–æ–ø–µ–π–∫–∏': '–∫–æ–ø',
        '—Ç–µ–ª.': '—Ç–µ–ª–µ—Ñ–æ–Ω',
        '—Ç.': '—Ç–µ–ª–µ—Ñ–æ–Ω',
        '–∏–Ω–Ω': '–∏–Ω–Ω',
        '–∫–ø–ø': '–∫–ø–ø',
        '—Ä/—Å': '—Ä–∞—Å—á–µ—Ç–Ω—ã–π —Å—á–µ—Ç',
        '–∫/—Å': '–∫–æ—Ä—Ä–µ—Å–ø–æ–Ω–¥–µ–Ω—Ç—Å–∫–∏–π —Å—á–µ—Ç',
        '–±–∏–∫': '–±–∏–∫',
        '–Ω–¥—Å': '–Ω–¥—Å',
        '–±–µ–∑ –Ω–¥—Å': '–±–µ–∑ –Ω–¥—Å',
        '–≤ —Ç.—á.': '–≤ —Ç–æ–º —á–∏—Å–ª–µ',
        '–≤–∫–ª.': '–≤–∫–ª—é—á–∞—è',
        '–∏—Å—Ö.': '–∏—Å—Ö–æ–¥—è—â–∏–π',
        '–≤—Ö.': '–≤—Ö–æ–¥—è—â–∏–π',
    }
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.lower()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–∞ –∏ –¥–∞—Ç—ã
    numbers = []
    dates = []
    
    # –ù–∞—Ö–æ–¥–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç—ã (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ñ–æ—Ä–º–∞—Ç–æ–≤)
    date_patterns = [
        r'\d{2}[./-]\d{2}[./-]\d{4}',  # DD.MM.YYYY
        r'\d{4}[./-]\d{2}[./-]\d{2}',  # YYYY.MM.DD
        r'\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{4}',  # DD Month YYYY
        r'(?:–æ—Ç\s+)?\d{1,2}[./-]\d{2}[./-]\d{4}',  # "–æ—Ç" DD.MM.YYYY
    ]
    
    for pattern in date_patterns:
        for date in re.finditer(pattern, text):
            date_text = date.group()
            dates.append(date_text)
            text = text.replace(date_text, ' DATE ')
    
    # –ù–∞—Ö–æ–¥–∏–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å–ª–∞ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —á–∏—Å–ª–∞ —Å –≤–∞–ª—é—Ç–æ–π
    currency_pattern = r'[\d\s.,]+(?=\s*(?:—Ä—É–±|‚ÇΩ|—Ä\.|–∫–æ–ø|%|–ø—Ä–æ—Ü–µ–Ω—Ç))'
    for number in re.finditer(currency_pattern, text):
        # –û—á–∏—â–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∏—Å–ª–æ
        clean_number = _normalize_number(number.group())
        numbers.append(clean_number)
        text = text.replace(number.group(), ' NUMBER ')
    
    # –ó–∞—Ç–µ–º –∏—â–µ–º –ø—Ä–æ—Å—Ç–æ —á–∏—Å–ª–∞
    number_pattern = r'(?<!\w)[\d\s.,]+(?!\w)'
    for number in re.finditer(number_pattern, text):
        if ' NUMBER ' not in number.group():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ
            clean_number = _normalize_number(number.group())
            numbers.append(clean_number)
            text = text.replace(number.group(), ' NUMBER ')
    
    # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–∂–Ω—ã–µ
    text = re.sub(r'[¬´¬ª\"\'\(\)\[\]\{\}]', ' ', text)
    text = re.sub(r'[.,;:]+(?!\d)', ' ', text)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ—á–∫–∏ –∏ –∑–∞–ø—è—Ç—ã–µ –≤ —á–∏—Å–ª–∞—Ö
    
    # –ó–∞–º–µ–Ω—è–µ–º —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
    for abbr, full in replacements.items():
        text = re.sub(r'\b' + re.escape(abbr) + r'\b', full, text)
    
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = ' '.join(text.split())
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç—ã
    for date in dates:
        text = text.replace('DATE', date, 1)
    
    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —á–∏—Å–ª–∞
    for number in numbers:
        text = text.replace('NUMBER', number, 1)
    
    return text

def _normalize_number(number_str):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.
    
    Args:
        number_str (str): –°—Ç—Ä–æ–∫–∞ —Å —á–∏—Å–ª–æ–º
        
    Returns:
        str: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —á–∏—Å–ª–æ
    """
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã
    clean_number = number_str.replace(' ', '')
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–ø—è—Ç—ã–µ –∏ —Ç–æ—á–∫–∏
    if ',' in clean_number and '.' not in clean_number:
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–ø—è—Ç–∞—è –∏ –Ω–µ—Ç —Ç–æ—á–∫–∏, —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—è—Ç—É—é –¥–µ—Å—è—Ç–∏—á–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º
        clean_number = clean_number.replace(',', '.')
    elif ',' in clean_number and '.' in clean_number:
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∏ –∑–∞–ø—è—Ç–∞—è –∏ —Ç–æ—á–∫–∞, —Å—á–∏—Ç–∞–µ–º –∑–∞–ø—è—Ç—É—é —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º —Ç—ã—Å—è—á
        clean_number = clean_number.replace(',', '')
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ü–µ–ª—É—é –∏ –¥—Ä–æ–±–Ω—É—é —á–∞—Å—Ç–∏
    parts = clean_number.split('.')
    if len(parts) == 2:
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ü–µ–ª—É—é —á–∞—Å—Ç—å (—É–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        whole = parts[0].replace('.', '')
        # –û—Å—Ç–∞–≤–ª—è–µ–º –¥—Ä–æ–±–Ω—É—é —á–∞—Å—Ç—å –∫–∞–∫ –µ—Å—Ç—å
        fraction = parts[1]
        clean_number = whole + '.' + fraction
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç –¥—Ä–æ–±–Ω–æ–π —á–∞—Å—Ç–∏, –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª—è–µ–º –≤—Å–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
        clean_number = clean_number.replace('.', '')
    
    return clean_number

def find_best_match_indices(target_text, prepared_words, similarity_threshold):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤ –≤ OCR —Ç–µ–∫—Å—Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–∏–ª—É—á—à–∏–º –æ–±—Ä–∞–∑–æ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ü–µ–ª–µ–≤–æ–º—É —Ç–µ–∫—Å—Ç—É.
    
    Args:
        target_text (str): –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞
        prepared_words (list): –°–ø–∏—Å–æ–∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ OCR
        similarity_threshold (float): –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        
    Returns:
        list: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏–ª–∏ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –µ—Å–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    """
    if not target_text or not prepared_words:
        return []
        
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ü–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç
    target_text = normalize_text_for_matching(target_text)
    target_words = target_text.split()
    
    if not target_words:
        return []
    
    # –ò—â–µ–º –Ω–∞–∏–ª—É—á—à–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
    best_matches = []
    used_indices = set()
    
    for target_word in target_words:
        best_match = {
            'index': -1,
            'ratio': 0.0
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–∑ OCR
        for word in prepared_words:
            if word['index'] in used_indices:
                continue
                
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            ratio = calculate_text_similarity(target_word, word['text'])
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if ratio > best_match['ratio'] and ratio >= similarity_threshold:
                best_match = {
                    'index': word['index'],
                    'ratio': ratio
                }
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –¥–æ–±–∞–≤–ª—è–µ–º –µ–≥–æ
        if best_match['index'] >= 0:
            best_matches.append(best_match)
            used_indices.add(best_match['index'])
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
    if len(best_matches) < len(target_words) * 0.5:  # –¢—Ä–µ–±—É–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ö–æ—Ç—è –±—ã 50% —Å–ª–æ–≤
        return []
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ –ø–æ—Ä—è–¥–∫—É –∏—Ö –ø–æ—è–≤–ª–µ–Ω–∏—è –≤ —Ç–µ–∫—Å—Ç–µ
    return sorted(match['index'] for match in best_matches)

def calculate_text_similarity(text1, text2, log_callback=None):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏ —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤.
    
    Args:
        text1 (str): –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        text2 (str): –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        log_callback (callable, optional): –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        
    Returns:
        float: –ó–Ω–∞—á–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ –æ—Ç 0.0 –¥–æ 1.0
    """
    def log_debug(message):
        if log_callback:
            log_callback(message)
        print(f"[DEBUG] {message}")
    
    if not isinstance(text1, str) or not isinstance(text2, str):
        return 0.0
        
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã
    text1 = normalize_text_for_matching(text1)
    text2 = normalize_text_for_matching(text2)
    
    if not text1 or not text2:
        return 0.0
        
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    if text1 == text2:
        log_debug(f"–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: '{text1}' == '{text2}'")
        return 1.0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è—é—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç—ã —á–∏—Å–ª–∞–º–∏
    def is_number(text):
        try:
            float(text.replace(' ', '').replace(',', '.'))
            return True
        except ValueError:
            return False
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —á–∏—Å–µ–ª
    if is_number(text1) and is_number(text2):
        try:
            num1 = float(text1.replace(' ', '').replace(',', '.'))
            num2 = float(text2.replace(' ', '').replace(',', '.'))
            # –î–ª—è —á–∏—Å–µ–ª –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —Ä–∞–∑–Ω–∏—Ü—É
            if num1 == num2:
                return 1.0
            diff = abs(num1 - num2) / max(abs(num1), abs(num2))
            similarity = max(0.0, 1.0 - diff)
            log_debug(f"–ß–∏—Å–ª–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: {num1} ~ {num2} = {similarity:.2f}")
            return similarity
        except ValueError:
            pass
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è—é—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç—ã –¥–∞—Ç–∞–º–∏
    def parse_date(text):
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
        date_formats = [
            '%d.%m.%Y', '%Y.%m.%d',
            '%d/%m/%Y', '%Y/%m/%d',
            '%d-%m-%Y', '%Y-%m-%d'
        ]
        text = text.replace(' ', '')
        for fmt in date_formats:
            try:
                return datetime.strptime(text, fmt)
            except ValueError:
                continue
        return None
    
    date1 = parse_date(text1)
    date2 = parse_date(text2)
    if date1 and date2:
        # –î–ª—è –¥–∞—Ç –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑–Ω–∏—Ü—É –≤ –¥–Ω—è—Ö
        days_diff = abs((date1 - date2).days)
        if days_diff == 0:
            return 1.0
        similarity = max(0.0, 1.0 - (days_diff / 30))  # 30 –¥–Ω–µ–π –∫–∞–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞
        log_debug(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞—Ç: {date1.date()} ~ {date2.date()} = {similarity:.2f}")
        return similarity
    
    # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–¥–æ 5 —Å–∏–º–≤–æ–ª–æ–≤) –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    if len(text1) <= 5 or len(text2) <= 5:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫
        distance = levenshtein_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        similarity = 1.0 - (distance / max_len)
        log_debug(f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫: '{text1}' ~ '{text2}' = {similarity:.2f}")
        return similarity
        
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π –¥—Ä—É–≥–æ–≥–æ
    if text1 in text2 or text2 in text1:
        shorter = text1 if len(text1) < len(text2) else text2
        longer = text2 if len(text1) < len(text2) else text1
        # –£—á–∏—Ç—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø–æ–¥—Å—Ç—Ä–æ–∫–∏
        similarity = 0.7 + 0.3 * (len(shorter) / len(longer))
        log_debug(f"–ü–æ–¥—Å—Ç—Ä–æ–∫–∞: '{shorter}' –≤ '{longer}' = {similarity:.2f}")
        return similarity
    
    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ –Ω–µ—á–µ—Ç–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–æ–≤–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    words1 = set(text1.split())
    words2 = set(text2.split())
    if words1 and words2:
        word_similarity = len(words1.intersection(words2)) / max(len(words1), len(words2))
    else:
        word_similarity = 0.0
    
    # –ó–∞—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º sequence matcher –¥–ª—è –æ–±—â–µ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
    sequence_similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
    
    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤–µ—Å–∞–º–∏
    similarity = 0.4 * word_similarity + 0.6 * sequence_similarity
    log_debug(f"–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ: '{text1}' ~ '{text2}' = {similarity:.2f}")
    
    return similarity

def levenshtein_distance(s1, s2):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –º–µ–∂–¥—É –¥–≤—É–º—è —Å—Ç—Ä–æ–∫–∞–º–∏.
    
    Args:
        s1 (str): –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞
        s2 (str): –í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞
        
    Returns:
        int: –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

class TrainingDataPreparator:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫ –æ–±—É—á–µ–Ω–∏—é –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π."""
    
    def __init__(self, app_config, ocr_processor, gemini_processor):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            app_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            ocr_processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä OCR
            gemini_processor: –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä Gemini
        """
        self.app_config = app_config
        self.ocr_processor = ocr_processor
        self.gemini_processor = gemini_processor
        self.stop_requested = False
        self.log_callback = None
        self.progress_callback = None
        
        # –†–µ–∂–∏–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.intelligent_mode = False
        self.intelligent_extractor = None
        
        # PDF –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–ª–æ–µ–º
        self.pdf_analyzer = PDFTextAnalyzer(logger=self.logger)
        
        # üöÄ GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        import torch
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"DataPreparator: –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU –∏ –≤—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        if torch.cuda.is_available():
            print(f"DataPreparator: üéÆ GPU: {torch.cuda.get_device_name()}")
            print(f"DataPreparator: üíæ GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("DataPreparator: ‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫—ç—à –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        self._processor_cache = {}
        self._model_cache = {}
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞ GPU
        self.batch_size = 8 if torch.cuda.is_available() else 4
        self.use_gpu_augmentation = torch.cuda.is_available()
        
        print(f"DataPreparator: Batch —Ä–∞–∑–º–µ—Ä: {self.batch_size}")
        print(f"DataPreparator: GPU –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: {self.use_gpu_augmentation}")
        
        # üéØ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å FieldManager –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤
        try:
            from ..field_manager import field_manager
            self.field_manager = field_manager
            self._log("‚úÖ FieldManager –ø–æ–¥–∫–ª—é—á–µ–Ω –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤")
        except ImportError:
            self.field_manager = None
            self._log("‚ö†Ô∏è FieldManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã")

    def _log(self, message):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π"""
        print(f"[DataPreparator] {message}")
        if self.log_callback:
            self.log_callback(message)
    
    def _init_intelligent_extractor(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
        if self.intelligent_mode and not self.intelligent_extractor:
            try:
                from .intelligent_data_extractor import IntelligentDataExtractor
                self.intelligent_extractor = IntelligentDataExtractor(
                    gemini_processor=self.gemini_processor,
                    logger=None  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à _log –º–µ—Ç–æ–¥
                )
                self._log("üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                self._log(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞: {e}")
                self.intelligent_mode = False

    def _get_cached_processor(self, model_name: str, processor_type: str = "layoutlm"):
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            processor_type: –¢–∏–ø –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ (layoutlm, donut)
            
        Returns:
            –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –º–æ–¥–µ–ª–∏
        """
        cache_key = f"{processor_type}_{model_name}"
        
        if cache_key not in self._processor_cache:
            try:
                self._log(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ {processor_type} –¥–ª—è {model_name}...")
                
                if processor_type == "layoutlm":
                    from transformers import LayoutLMv3Processor
                    processor = LayoutLMv3Processor.from_pretrained(
                        model_name, 
                        apply_ocr=False,
                        cache_dir=os.path.join(self.app_config.MODELS_PATH, 'cache')
                    )
                elif processor_type == "donut":
                    from transformers import DonutProcessor
                    processor = DonutProcessor.from_pretrained(
                        model_name,
                        cache_dir=os.path.join(self.app_config.MODELS_PATH, 'cache')
                    )
                else:
                    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞: {processor_type}")
                
                self._processor_cache[cache_key] = processor
                self._log(f"‚úÖ –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä {processor_type} –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω")
                
            except Exception as e:
                self._log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ {processor_type}: {e}")
                return None
        
        return self._processor_cache[cache_key]

    def _batch_tokenize_layoutlm(self, images, words_list, bboxes_list, labels_list=None, 
                                model_name="microsoft/layoutlmv3-base"):
        """
        Batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è LayoutLM —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
        
        Args:
            images: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            words_list: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ —Å–ª–æ–≤
            bboxes_list: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ bbox
            labels_list: –°–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤ –º–µ—Ç–æ–∫ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            
        Returns:
            dict: –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        processor = self._get_cached_processor(model_name, "layoutlm")
        if not processor:
            return None
        
        try:
            self._log(f"üîÑ Batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è {len(images)} –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ {self.device}...")
            
            # –†–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–µ —Å–ø–∏—Å–∫–∏
            all_input_ids = []
            all_attention_masks = []
            all_token_type_ids = []
            all_bboxes = []
            all_pixel_values = []
            all_labels = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏
            for i in range(0, len(images), self.batch_size):
                batch_end = min(i + self.batch_size, len(images))
                batch_images = images[i:batch_end]
                batch_words = words_list[i:batch_end]
                batch_bboxes = bboxes_list[i:batch_end]
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–∞—Ç—á–∞
                try:
                    encoding = processor(
                        batch_images,
                        batch_words,
                        boxes=batch_bboxes,
                        truncation=True,
                        padding="max_length",
                        max_length=512,
                        return_tensors="pt"
                    )
                    
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
                    if torch.cuda.is_available():
                        encoding = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                                  for k, v in encoding.items()}
                    
                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    all_input_ids.extend(encoding["input_ids"].cpu().tolist())
                    all_attention_masks.extend(encoding["attention_mask"].cpu().tolist())
                    all_token_type_ids.extend(encoding["token_type_ids"].cpu().tolist())
                    all_bboxes.extend(encoding["bbox"].cpu().tolist())
                    all_pixel_values.extend(encoding["pixel_values"].cpu().tolist())
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
                    if labels_list and i < len(labels_list):
                        batch_labels = labels_list[i:batch_end]
                        for labels in batch_labels:
                            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –º–µ—Ç–æ–∫ –≤ ID –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
                            if isinstance(labels[0], str):
                                # –ï—Å–ª–∏ –º–µ—Ç–∫–∏ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ ID
                                label_ids = self._convert_string_labels_to_ids(labels)
                            else:
                                label_ids = labels
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º [CLS] –∏ [SEP] —Ç–æ–∫–µ–Ω—ã
                            label_ids = [-100] + label_ids + [-100]
                            # –ü–∞–¥–¥–∏–Ω–≥ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                            if len(label_ids) < 512:
                                label_ids.extend([-100] * (512 - len(label_ids)))
                            else:
                                label_ids = label_ids[:512]
                            
                            all_labels.append(label_ids)
                    
                    # –ü—Ä–æ–≥—Ä–µ—Å—Å
                    if self.progress_callback:
                        progress = int((batch_end / len(images)) * 100)
                        self.progress_callback(progress)
                        
                except Exception as batch_error:
                    self._log(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {i}-{batch_end}: {batch_error}")
                    continue
            
            result = {
                'input_ids': all_input_ids,
                'attention_mask': all_attention_masks,
                'token_type_ids': all_token_type_ids,
                'bbox': all_bboxes,
                'pixel_values': all_pixel_values
            }
            
            if all_labels:
                result['labels'] = all_labels
            
            self._log(f"‚úÖ Batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(all_input_ids)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            return result
            
        except Exception as e:
            self._log(f"‚ùå –û—à–∏–±–∫–∞ batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏: {e}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def _convert_string_labels_to_ids(self, labels, entity_types=None):
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã–µ ID
        
        Args:
            labels: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –º–µ—Ç–æ–∫
            entity_types: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö ID
        """
        if not entity_types:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –º–µ—Ç–æ–∫
            entity_types = set()
            for label in labels:
                if label.startswith(('B-', 'I-')):
                    entity_types.add(label[2:])
            entity_types = sorted(list(entity_types))
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫
        label2id = {'O': 0}
        current_id = 1
        for entity_type in entity_types:
            label2id[f"B-{entity_type}"] = current_id
            current_id += 1
            label2id[f"I-{entity_type}"] = current_id
            current_id += 1
        
        return [label2id.get(label, 0) for label in labels]

    def apply_augmentation(self, image: Image.Image) -> Image.Image:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é GPU —É—Å–∫–æ—Ä–µ–Ω–∏—è
        
        Args:
            image: –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ PIL
            
        Returns:
            Image.Image: –ê—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        try:
            import albumentations as A
            import numpy as np
            
            # –°–æ–∑–¥–∞–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            augmentation = A.Compose([
                A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),
                A.GaussNoise(var_limit=(0.0, 25.0), p=0.3),
                A.Blur(blur_limit=2, p=0.2),
                A.Rotate(limit=2, p=0.3),  # –ù–µ–±–æ–ª—å—à–∏–µ –ø–æ–≤–æ—Ä–æ—Ç—ã –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                A.RandomGamma(gamma_limit=(95, 105), p=0.3),
            ])
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL –≤ numpy –¥–ª—è albumentations
            image_np = np.array(image)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
            augmented = augmentation(image=image_np)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ PIL
            return Image.fromarray(augmented['image'])
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            return image

    def apply_batch_augmentation(self, images: List[Image.Image], 
                               augmentation_factor: int = 1) -> List[Image.Image]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç batch –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        
        Args:
            images: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            augmentation_factor: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Ä—Å–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            
        Returns:
            List[Image.Image]: –°–ø–∏—Å–æ–∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö –∏ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        """
        try:
            self._log(f"üé® Batch –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (—Ñ–∞–∫—Ç–æ—Ä: {augmentation_factor})")
            
            result_images = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            result_images.extend(images)
            
            # –ï—Å–ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ
            if augmentation_factor <= 0:
                return result_images
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é
            total_augmentations = len(images) * augmentation_factor
            processed = 0
            
            for i, image in enumerate(images):
                for aug_idx in range(augmentation_factor):
                    try:
                        aug_image = self.apply_augmentation(image)
                        result_images.append(aug_image)
                        processed += 1
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                        if self.progress_callback and processed % 10 == 0:
                            progress = int((processed / total_augmentations) * 100)
                            self.progress_callback(progress)
                            
                    except Exception as aug_error:
                        self._log(f"‚ùå –û—à–∏–±–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i}.{aug_idx}: {aug_error}")
                        continue
            
            self._log(f"‚úÖ Batch –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ–∑–¥–∞–Ω–æ {len(result_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return result_images
            
        except Exception as e:
            self._log(f"‚ùå –û—à–∏–±–∫–∞ batch –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏: {e}")
            return images  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

    def _match_gemini_and_ocr(self, gemini_data: Dict, ocr_data: Dict) -> Dict:
        """
        –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ Gemini —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ OCR –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            gemini_data: –î–∞–Ω–Ω—ã–µ –æ—Ç Gemini API
            ocr_data: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã OCR
            
        Returns:
            Dict: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å —Å –º–µ—Ç–∫–∞–º–∏
        """
        try:
            self._log("–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö Gemini –∏ OCR")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –∏ –∏–º–µ—é—Ç –Ω—É–∂–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            if not gemini_data or not ocr_data:
                self._log("–û–®–ò–ë–ö–ê: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –¥–∞–Ω–Ω—ã–µ Gemini –∏–ª–∏ OCR")
                return {}
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π –≤ OCR –¥–∞–Ω–Ω—ã—Ö
            if 'words' not in ocr_data:
                self._log("–û–®–ò–ë–ö–ê: OCR –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –ø–æ–ª—è 'words'")
                return {}
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ —Å–ª–æ–≤–∞—Ä–µ words —Å–æ–¥–µ—Ä–∂–∞—Ç—Å—è –¥–∞–Ω–Ω—ã–µ –æ –±–æ–∫—Å–∞—Ö
            if not ocr_data['words'] or not isinstance(ocr_data['words'], list) or not all(isinstance(word, dict) and 'bbox' in word for word in ocr_data['words']):
                self._log("–û–®–ò–ë–ö–ê: OCR –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –±–æ–∫—Å–∞—Ö –≤ –ø–æ–ª–µ 'words'")
                return {}
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ gemini_data –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            if not isinstance(gemini_data, dict):
                self._log(f"–û–®–ò–ë–ö–ê: –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö Gemini: {type(gemini_data)}")
                return {}
                
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –∏ –∏—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏–∑ OCR
            ocr_words = [word.get('text', '') for word in ocr_data['words']]
            
            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤ OCR –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if ocr_words:
                sample_words = ocr_words[:10]
                self._log(f"–û–±—Ä–∞–∑–µ—Ü OCR –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤): {sample_words}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö Gemini - –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–∏–±–æ —Å –ø–æ–ª–µ–º 'fields', –ª–∏–±–æ –Ω–∞–ø—Ä—è–º—É—é —Å –∫–ª—é—á–∞–º–∏
            has_fields = 'fields' in gemini_data and isinstance(gemini_data['fields'], list)
            has_direct_keys = any(key in gemini_data for key in ['invoice_number', 'date', 'total_amount', 'supplier_name'])
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            table_keys = ['items', 'line_items', 'products', 'services', 'positions', 'table', 'tables']
            has_table_data = any(key in gemini_data for key in table_keys)
            
            if not (has_fields or has_direct_keys or has_table_data):
                self._log("–û–®–ò–ë–ö–ê: –î–∞–Ω–Ω—ã–µ Gemini –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –Ω–∏ –ø–æ–ª–µ 'fields', –Ω–∏ –ø—Ä—è–º—ã–µ –∫–ª—é—á–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ —Å—á–µ—Ç–∞, –Ω–∏ —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                return {}
                
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –∏ –±–æ–∫–∑—ã –∏–∑ OCR
            words = [word['text'] for word in ocr_data.get('words', [])]
            raw_bboxes = [word['bbox'] for word in ocr_data.get('words', [])]
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º bboxes –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ OCR –¥–∞–Ω–Ω—ã—Ö
            image_width = ocr_data.get('width', 0)
            image_height = ocr_data.get('height', 0)
            
            if image_width > 0 and image_height > 0:
                self._log(f"–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è bboxes –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_width}x{image_height}")
                bboxes = [self.normalize_bbox(bbox, image_width, image_height) for bbox in raw_bboxes]
                # –í—ã–≤–æ–¥–∏–º –ø—Ä–∏–º–µ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if len(raw_bboxes) > 0:
                    self._log(f"–ü—Ä–∏–º–µ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {raw_bboxes[0]} -> {bboxes[0]}")
            else:
                self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –†–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ OCR –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ bboxes")
                bboxes = raw_bboxes
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –∏ –±–æ–∫—Å–æ–≤ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
            if len(words) != len(bboxes):
                self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ ({len(words)}) –∏ –±–æ–∫—Å–æ–≤ ({len(bboxes)})")
                # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã
                min_len = min(len(words), len(bboxes))
                words = words[:min_len]
                bboxes = bboxes[:min_len]
                
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ–±—Ä–µ–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
            if not words or not bboxes:
                self._log("–û–®–ò–ë–ö–ê: –ü–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å —Å–ª–æ–≤ –∏–ª–∏ –±–æ–∫—Å–æ–≤")
                return {}
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤ –∫–∞–∫ "O" (outside)
            labels = ["O"] * len(words)
            
            # –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            entities = {}
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
            if has_fields:
                # –í—ã–≤–æ–¥–∏–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –∏–∑ Gemini
                self._log("–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª–µ–π Gemini –≤ —Ñ–æ—Ä–º–∞—Ç–µ fields:")
                for idx, field_data in enumerate(gemini_data.get('fields', [])):
                    if isinstance(field_data, dict) and 'field_name' in field_data and 'field_value' in field_data:
                        self._log(f"  –ü–æ–ª–µ {idx+1}: {field_data['field_name']} = '{field_data['field_value']}'")
                    else:
                        self._log(f"  –ü–æ–ª–µ {idx+1}: {field_data} (–Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ –∏–∑ Gemini –≤ —Ñ–æ—Ä–º–∞—Ç–µ fields
                for field_data in gemini_data.get('fields', []):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ
                    if not isinstance(field_data, dict) or 'field_name' not in field_data or 'field_value' not in field_data:
                        self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ü—Ä–æ–ø—É—Å–∫ –ø–æ–ª—è —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {field_data}")
                        continue
                        
                    field_name = field_data['field_name']
                    field_value = field_data['field_value']
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if not field_value or not field_name:
                        continue
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                    field_value = str(field_value).strip().lower()
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è (–º–µ–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤)
                    if len(field_value) < 2:
                        continue
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ–ª—è –¥–ª—è IOB2 —Ç–µ–≥–æ–≤
                    field_type = self._normalize_field_type(field_name)
                    
                    # –ü–æ–∏—Å–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ OCR
                    if 'currency' in field_name.lower() or field_type == 'CURRENCY':
                        self._log(f"–ü–æ–∏—Å–∫ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—è 'CURRENCY': '{field_value}'")
                    elif 'date' in field_name.lower() or field_type == 'DATE':
                        self._log(f"–ü–æ–∏—Å–∫ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –ø–æ–ª—è 'DATE': '{field_value}'")
                    
                    # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
                    field_matches = self._mark_matching_words(words, field_value, field_type, labels)
                    if field_matches:
                        entities[field_type] = field_value
            elif has_table_data:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                for table_key in table_keys:
                    if table_key in gemini_data and isinstance(gemini_data[table_key], list):
                        table_items = gemini_data[table_key]
                        self._log(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –∫–ª—é—á–µ '{table_key}': {len(table_items)} —Å—Ç—Ä–æ–∫")
                        
                        for item_idx, item in enumerate(table_items):
                            if not isinstance(item, dict):
                                self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –≠–ª–µ–º–µ–Ω—Ç —Ç–∞–±–ª–∏—Ü—ã —Å –∏–Ω–¥–µ–∫—Å–æ–º {item_idx} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º")
                                continue
                                
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Ç–∞–±–ª–∏—Ü—ã
                            self._process_table_item(item, words, labels, item_idx)
            
            if has_direct_keys:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä—è–º—ã–µ –∫–ª—é—á–∏ –∏–∑ —Å–ª–æ–≤–∞—Ä—è (–∫–∞–∫ –∏–∑ GeminiProcessor._convert_to_training_format)
                for key, value in gemini_data.items():
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–ª—é—á–∏ –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if key.startswith('note_') or key in ['source_image', 'processed_at'] or not value:
                        continue
                        
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤—ã—à–µ
                    if key in table_keys and isinstance(value, list):
                        continue
                        
                    # –ï—Å–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ - —Å–ª–æ–≤–∞—Ä—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, supplier, customer), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –µ–≥–æ –ø–æ–ª—è
                    if isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            if sub_value:
                                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–ª—é—á –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –ø–æ–ª—è
                                field_name = f"{key}_{sub_key}"
                                field_type = self._normalize_field_type(field_name)
                                
                                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ
                                field_value = str(sub_value).strip().lower()
                                
                                # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞
                                field_matches = self._mark_matching_words(words, field_value, field_type, labels)
                                if field_matches:
                                    entities[field_type] = field_value
                    else:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ –ø–æ–ª–µ
                        field_type = self._normalize_field_type(key)
                        field_value = str(value).strip().lower()
                        
                        # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞
                        field_matches = self._mark_matching_words(words, field_value, field_type, labels)
                        if field_matches:
                            entities[field_type] = field_value
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π
            tag_counts = {}
            for label in labels:
                if label != "O":
                    tag_type = label.split('-')[1] if '-' in label else label
                    tag_counts[tag_type] = tag_counts.get(tag_type, 0) + 1
            
            entity_count = len(entities)
            b_count = sum(1 for label in labels if label.startswith('B-'))
            
            self._log(f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –ø–æ–ª–µ–π: {entity_count}, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {tag_counts}")
            
            # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
            for field_type, field_value in entities.items():
                b_labels = [i for i, label in enumerate(labels) if label == f'B-{field_type}']
                i_labels = [i for i, label in enumerate(labels) if label == f'I-{field_type}']
                
                if b_labels:
                    matched_words = []
                    for b_idx in b_labels:
                        # –°–æ–±–∏—Ä–∞–µ–º –≥—Ä—É–ø–ø—É —Å–ª–æ–≤, –Ω–∞—á–∏–Ω–∞—è —Å —Ç–µ–∫—É—â–µ–≥–æ B- –∏ –≤–∫–ª—é—á–∞—è –≤—Å–µ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ I-
                        group = [words[b_idx]]
                        i = b_idx + 1
                        while i < len(labels) and labels[i].startswith(f'I-{field_type}'):
                            group.append(words[i])
                            i += 1
                        matched_words.append(' '.join(group))
                    
                    self._log(f"–ü–æ–ª–µ '{field_type}': '{field_value}' => {matched_words}")
                else:
                    self._log(f"–ü–æ–ª–µ '{field_type}': '{field_value}' –Ω–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–æ —Å —Ç–µ–∫—Å—Ç–æ–º OCR")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ 'words', 'bboxes', 'labels' –∏ 'entities'
            return {
                'words': words,
                'bboxes': bboxes,
                'labels': labels,
                'entities': entities
            }
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö Gemini –∏ OCR: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return {}

        # –£–õ–£–ß–®–ï–ù–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø DATA_PREPARATOR.PY
    # –°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏ –≤ –∫–ª–∞—Å—Å TrainingDataPreparator

    def _mark_matching_words(self, words, field_value, field_type, labels):
        """
        –ú–∞—Ä–∫–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—é –ø–æ–ª—è. –£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø.
        """
        if not field_value or not words:
            return False

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è
        field_value = str(field_value).strip().lower()

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è "n/a", "null", "none" –∏ —Ç.–ø.
        skip_values = {'n/a', 'null', 'none', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ', '–Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ', '-', '‚Äî', '–Ω–µ—Ç', '–ø—É—Å—Ç–æ'}
        if field_value in skip_values:
            self._log(f"–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—è '{field_type}': '{field_value}'")
            return False

        # –≠–¢–ê–ü 1: –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if self._is_numeric_field(field_value):
            found_numeric = self._find_numeric_match_improved(words, field_value, field_type, labels)
            if found_numeric:
                return True

        # –≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Ñ—Ä–∞–∑
        field_tokens = field_value.split()
        if len(field_tokens) > 1:
            found_exact = self._find_exact_phrase_match_improved(words, field_value, field_type, labels)
            if found_exact:
                return True

        # –≠–¢–ê–ü 3: –ü–æ–∏—Å–∫ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∑–Ω–∞—á–∏–º—ã–º —Ç–æ–∫–µ–Ω–∞–º
        found_tokens = self._find_token_sequence_match_improved(words, field_tokens, field_type, labels)
        if found_tokens:
            return True

        # –≠–¢–ê–ü 4: –£–ª—É—á—à–µ–Ω–Ω—ã–π —á–∞—Å—Ç–∏—á–Ω—ã–π –ø–æ–∏—Å–∫
        found_partial = self._find_improved_partial_match(words, field_value, field_type, labels)
        if found_partial:
            return True

        # –≠–¢–ê–ü 5: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫
        found_contextual = self._find_contextual_match_improved(words, field_value, field_type, labels)
        if found_contextual:
            return True

        self._log(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –ø–æ–ª—è '{field_type}': '{field_value}'")
        return False

    def _find_numeric_match_improved(self, words, field_value, field_type, labels):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π."""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —á–∏—Å–ª–∞ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–ª—è
        field_numbers = re.findall(r'\d+(?:[.,]\d+)?', field_value)
        if not field_numbers:
            return False

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∏—Å–ª–∞ (–∑–∞–ø—è—Ç—ã–µ –≤ —Ç–æ—á–∫–∏)
        field_numbers_norm = [num.replace(',', '.') for num in field_numbers]

        for i, word in enumerate(words):
            if labels[i] != "O":
                continue

            word_numbers = re.findall(r'\d+(?:[.,]\d+)?', word)
            word_numbers_norm = [num.replace(',', '.') for num in word_numbers]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —á–∏—Å–µ–ª
            for field_num in field_numbers_norm:
                for word_num in word_numbers_norm:
                    if field_num == word_num and len(field_num) >= 2:  # –ó–Ω–∞—á–∏–º—ã–µ —á–∏—Å–ª–∞
                        labels[i] = f"B-{field_type}"
                        self._log(f"‚úÖ –ß–ò–°–õ–û–í–û–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ '{field_type}': '{field_value}' => '{word}' (—á–∏—Å–ª–æ: {field_num})")

                        # –ò—â–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä—è–¥–æ–º
                        self._extend_numeric_field_improved(words, i, field_value, field_type, labels)
                        return True

        return False

    def _find_exact_phrase_match_improved(self, words, field_value, field_type, labels):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑."""
        field_value_clean = self._normalize_text_for_matching(field_value)
        field_tokens = field_value_clean.split()

        if len(field_tokens) < 2:
            return False

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–Ω–∞
        max_window = min(len(field_tokens) + 1, 6)

        for window_size in range(max_window, 1, -1):
            for i in range(len(words) - window_size + 1):
                if any(labels[j] != "O" for j in range(i, i + window_size)):
                    continue

                window_words = [self._normalize_text_for_matching(words[j]) for j in range(i, i + window_size)]
                window_text = " ".join(window_words)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
                if self._is_phrase_match(window_text, field_value_clean):
                    for j in range(i, i + window_size):
                        labels[j] = f"B-{field_type}" if j == i else f"I-{field_type}"

                    matched_words = " ".join(words[i:i+window_size])
                    self._log(f"‚úÖ –§–†–ê–ó–û–í–û–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ '{field_type}': '{field_value}' => '{matched_words}'")
                    return True

        return False

    def _find_token_sequence_match_improved(self, words, field_tokens, field_type, labels):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤."""
        if len(field_tokens) < 2:
            return False

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Ç–æ–∫–µ–Ω—ã
        significant_tokens = [token for token in field_tokens if len(token) >= 2]
        if len(significant_tokens) < 2:
            return False

        found_positions = []

        for token in significant_tokens:
            token_clean = self._normalize_text_for_matching(token)
            best_match = None
            best_score = 0

            for i, word in enumerate(words):
                if labels[i] != "O":
                    continue

                word_clean = self._normalize_text_for_matching(word)
                score = self._calculate_token_match_score(word_clean, token_clean)

                if score > 0.8 and score > best_score:
                    best_match = i
                    best_score = score

            if best_match is not None:
                found_positions.append(best_match)

        # –¢—Ä–µ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ö–æ—Ç—è –±—ã 60% —Ç–æ–∫–µ–Ω–æ–≤
        if len(found_positions) >= len(significant_tokens) * 0.6:
            # –ú–∞—Ä–∫–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
            found_positions.sort()
            for idx, pos in enumerate(found_positions):
                labels[pos] = f"B-{field_type}" if idx == 0 else f"I-{field_type}"

            self._log(f"‚úÖ –¢–û–ö–ï–ù–´ '{field_type}': –Ω–∞–π–¥–µ–Ω–æ {len(found_positions)}/{len(significant_tokens)} —Ç–æ–∫–µ–Ω–æ–≤")
            return True

        return False

    def _find_improved_partial_match(self, words, field_value, field_type, labels):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —á–∞—Å—Ç–∏—á–Ω—ã–π –ø–æ–∏—Å–∫."""
        field_clean = self._normalize_text_for_matching(field_value)

        # –ü–æ–≤—ã—à–µ–Ω–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
        min_length = max(3, len(field_clean) // 3)

        for i, word in enumerate(words):
            if labels[i] != "O":
                continue

            word_clean = self._normalize_text_for_matching(word)

            if len(word_clean) < min_length:
                continue

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∏–¥—ã —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            scores = []

            # –í—Ö–æ–∂–¥–µ–Ω–∏–µ
            if word_clean in field_clean or field_clean in word_clean:
                min_len = min(len(word_clean), len(field_clean))
                max_len = max(len(word_clean), len(field_clean))
                scores.append(min_len / max_len)

            # –°—Ö–æ–∂–µ—Å—Ç—å –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞
            if len(word_clean) >= 4 and len(field_clean) >= 4:
                start_match = word_clean[:4] == field_clean[:4]
                end_match = word_clean[-3:] == field_clean[-3:]
                if start_match or end_match:
                    scores.append(0.7)

            # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫
            if len(word_clean) <= 15 and len(field_clean) <= 15:
                scores.append(self._levenshtein_similarity(word_clean, field_clean))

            best_score = max(scores) if scores else 0

            if best_score >= 0.75:  # –°—Ç—Ä–æ–≥–∏–π –ø–æ—Ä–æ–≥
                labels[i] = f"B-{field_type}"
                self._log(f"‚úÖ –ß–ê–°–¢–ò–ß–ù–û–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ '{field_type}': '{field_value}' => '{word}' (score: {best_score:.2f})")
                return True

        return False

    def _find_contextual_match_improved(self, words, field_value, field_type, labels):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫."""
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –¥–ª—è —Ç–∏–ø–æ–≤ –ø–æ–ª–µ–π
        context_markers = {
            'INVOICE_ID': ['‚Ññ', '–Ω–æ–º–µ—Ä', '—Å—á–µ—Ç', 'invoice', 'number', '–æ—Ç'],
            'DATE': ['–¥–∞—Ç–∞', '–æ—Ç', 'date', '–≥.', '–≥–æ–¥–∞', '—á–∏—Å–ª–æ'],
            'TOTAL': ['–∏—Ç–æ–≥–æ', '—Å—É–º–º–∞', '–≤—Å–µ–≥–æ', 'total', 'sum', '–∫', '–æ–ø–ª–∞—Ç–µ'],
            'TAX_ID': ['–∏–Ω–Ω', 'inn', '–Ω–∞–ª–æ–≥–æ–≤—ã–π', '–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä'],
            'COMPANY': ['–æ–æ–æ', '–æ–∞–æ', '–∑–∞–æ', '–∏–ø', 'ltd', 'llc', 'inc', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '–ø–æ—Å—Ç–∞–≤—â–∏–∫']
        }

        markers = context_markers.get(field_type, [])
        if not markers:
            return False

        field_clean = self._normalize_text_for_matching(field_value)

        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Å–ª–æ–≤–∞ —Ä—è–¥–æ–º —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏
        for i, word in enumerate(words):
            if labels[i] != "O":
                continue

            word_clean = self._normalize_text_for_matching(word)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ª–æ–≤–∞ —Ç–∏–ø—É –ø–æ–ª—è
            if not self._is_suitable_for_field_type_improved(word_clean, field_clean, field_type):
                continue

            # –ò—â–µ–º –º–∞—Ä–∫–µ—Ä—ã –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ ¬±4 —Å–ª–æ–≤–∞
            context_found = False
            for j in range(max(0, i-4), min(len(words), i+5)):
                if j == i:
                    continue
                context_word = self._normalize_text_for_matching(words[j])
                if any(marker in context_word for marker in markers):
                    context_found = True
                    break

            if context_found:
                labels[i] = f"B-{field_type}"
                self._log(f"‚úÖ –ö–û–ù–¢–ï–ö–°–¢–ù–û–ï —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ '{field_type}': '{word}' —Ä—è–¥–æ–º —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏")
                return True

        return False

    def _is_suitable_for_field_type_improved(self, word, field_value, field_type):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Å–ª–æ–≤–∞ —Ç–∏–ø—É –ø–æ–ª—è."""
        if len(word) < 2:
            return False

        if field_type == 'TAX_ID':
            # –ò–ù–ù: 10-12 —Ü–∏—Ñ—Ä
            return bool(re.match(r'^\d{10,12}$', word))
        elif field_type == 'INVOICE_ID':
            # –ù–æ–º–µ—Ä —Å—á–µ—Ç–∞: —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã, –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
            return bool(re.search(r'\d+', word)) and len(word) <= 20
        elif field_type == 'DATE':
            # –î–∞—Ç–∞: —Ü–∏—Ñ—Ä—ã –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –º–µ—Å—è—Ü–µ–≤
            date_patterns = r'\d{1,4}|—è–Ω–≤–∞—Ä|—Ñ–µ–≤—Ä–∞–ª|–º–∞—Ä—Ç|–∞–ø—Ä–µ–ª|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥—É—Å—Ç|—Å–µ–Ω—Ç—è–±—Ä|–æ–∫—Ç—è–±—Ä|–Ω–æ—è–±—Ä|–¥–µ–∫–∞–±—Ä'
            return bool(re.search(date_patterns, word))
        elif field_type == 'TOTAL':
            # –°—É–º–º–∞: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ü–∏—Ñ—Ä—ã
            return bool(re.search(r'\d+', word))
        elif field_type == 'COMPANY':
            # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏: –ª—é–±–æ–π —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤
            return len(word) >= 2

        # –û–±—â–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ö–æ–∂–µ—Å—Ç—å
        return self._calculate_token_match_score(word, field_value) > 0.4

    def _normalize_text_for_matching(self, text):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è."""
        if not text:
            return ""

        text = str(text).lower().strip()

        # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–≤—ã—á–∫–∏ –∏ —Å–∫–æ–±–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ü–∏—Ñ—Ä—ã –∏ –≤–∞–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        remove_chars = ['"', "'", '¬´', '¬ª', '(', ')', '[', ']', '{', '}']
        for char in remove_chars:
            text = text.replace(char, ' ')

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        text = " ".join(text.split())

        return text

    def _is_phrase_match(self, text1, text2):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑."""
        return text1 == text2 or (text1 in text2 and len(text1) / len(text2) > 0.7)

    def _calculate_token_match_score(self, word, token):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞."""
        if word == token:
            return 1.0
        if word in token or token in word:
            return min(len(word), len(token)) / max(len(word), len(token))
        return self._levenshtein_similarity(word, token)

    def _extend_numeric_field_improved(self, words, start_pos, field_value, field_type, labels):
        """–†–∞—Å—à–∏—Ä—è–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –ø–æ–ª–µ –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞."""
        extensions = ['—Ä—É–±', '—Ä—É–±–ª–µ–π', '–∫–æ–ø–µ–µ–∫', '—à—Ç', '—à—Ç—É–∫', '%', '–ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤']

        for offset in [-1, 1]:
            pos = start_pos + offset
            if 0 <= pos < len(words) and labels[pos] == "O":
                word_clean = self._normalize_text_for_matching(words[pos])
                if any(ext in word_clean for ext in extensions):
                    labels[pos] = f"I-{field_type}"
                    self._log(f"  + —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ '{words[pos]}' –¥–ª—è '{field_type}'")

    def _is_numeric_field(self, field_value):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–æ–ª–µ —á–∏—Å–ª–æ–≤—ã–º."""
        return bool(re.search(r'\d', field_value))

    def _levenshtein_similarity(self, s1, s2):
        """–°—Ö–æ–∂–µ—Å—Ç—å –ø–æ –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω—É."""
        if len(s1) < len(s2):
            return self._levenshtein_similarity(s2, s1)
        if len(s2) == 0:
            return 0.0

        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        max_len = max(len(s1), len(s2))
        return 1.0 - (previous_row[-1] / max_len)

    def _is_matching_text(self, text1, text2, threshold=0.5):
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ü–û–ù–ò–ñ–ï–ù–ù–´–ú –ø–æ—Ä–æ–≥–æ–º.
        """
        try:
            if text1 == text2:
                return True

            t1 = self._normalize_text_for_matching(text1)
            t2 = self._normalize_text_for_matching(text2)

            if t1 == t2:
                return True

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            skip_values = {'n/a', 'null', 'none', '–Ω–µ —É–∫–∞–∑–∞–Ω–æ', '–Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ', '-', '‚Äî', '–Ω–µ—Ç'}
            if t1 in skip_values or t2 in skip_values:
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            if len(t1) >= 3 and len(t2) >= 3:
                if t1 in t2 or t2 in t1:
                    min_len = min(len(t1), len(t2))
                    max_len = max(len(t1), len(t2))
                    if min_len / max_len >= threshold:
                        return True

            # –û–±—â–∏–µ —á–∏—Å–ª–∞
            if self._has_common_significant_numbers(t1, t2):
                return True

            # –°—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
            if len(t1) <= 30 and len(t2) <= 30:
                similarity = self._levenshtein_similarity(t1, t2)
                return similarity >= threshold

            return False

        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç–æ–≤: {str(e)}")
            return False

    def _has_common_significant_numbers(self, str1, str2):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö –∑–Ω–∞—á–∏–º—ã—Ö —á–∏—Å–µ–ª."""
        numbers1 = set(re.findall(r'\d+(?:[.,]\d+)?', str1))
        numbers2 = set(re.findall(r'\d+(?:[.,]\d+)?', str2))

        if not numbers1 or not numbers2:
            return False

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∏—Å–ª–∞
        numbers1_norm = {num.replace(',', '.') for num in numbers1}
        numbers2_norm = {num.replace(',', '.') for num in numbers2}

        # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∑–Ω–∞—á–∏–º—ã—Ö —á–∏—Å–µ–ª (–¥–ª–∏–Ω–Ω–µ–µ 1 —Ü–∏—Ñ—Ä—ã)
        common = numbers1_norm.intersection(numbers2_norm)
        significant_common = {num for num in common if len(num.replace('.', '').replace(',', '')) > 1}

        return bool(significant_common)


    def _merge_bboxes(self, bboxes):
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ bounding boxes –≤ –æ–¥–∏–Ω.
        
        Args:
            bboxes (List[List[int]]): –°–ø–∏—Å–æ–∫ bounding boxes –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            
        Returns:
            List[int]: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π bounding box
        """
        if not bboxes:
            return [0, 0, 0, 0]
            
        x_min = min(bbox[0] for bbox in bboxes)
        y_min = min(bbox[1] for bbox in bboxes)
        x_max = max(bbox[0] + bbox[2] for bbox in bboxes)
        y_max = max(bbox[1] + bbox[3] for bbox in bboxes)
        
        return [x_min, y_min, x_max - x_min, y_max - y_min]

    def _assign_iob2_labels(self, matched_entities, ocr_words, entity_types, label_other):
        """
        –ù–∞–∑–Ω–∞—á–∞–µ—Ç IOB2 –º–µ—Ç–∫–∏ –¥–ª—è —Å–ª–æ–≤ OCR –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π.
        
        Args:
            matched_entities (list): –°–ø–∏—Å–æ–∫ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
            ocr_words (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ OCR
            entity_types (list): –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –¥–∞–Ω–Ω—ã—Ö Gemini
            label_other (str): –ú–µ—Ç–∫–∞ –¥–ª—è —Å–ª–æ–≤, –Ω–µ –≤—Ö–æ–¥—è—â–∏—Ö –≤ —Å—É—â–Ω–æ—Å—Ç–∏
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ IOB2 –º–µ—Ç–æ–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞
        """
        num_ocr_words = len(ocr_words)
        iob2_labels = [label_other] * num_ocr_words

        if not matched_entities:
            self._log("[_assign_iob2_labels] –ù–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π.")
            return iob2_labels

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –∏—Ö –ø–µ—Ä–≤–æ–º—É –∏–Ω–¥–µ–∫—Å—É –≤ OCR
        matched_entities.sort(key=lambda x: min(x.get('word_ids', [9999])))

        for entity in matched_entities:
            label = entity['entity_type']
            word_ids = entity.get('word_ids', [])

            if not word_ids:
                self._log(f"[_assign_iob2_labels] –ü—Ä–æ–ø—É—Å–∫ —Å—É—â–Ω–æ—Å—Ç–∏ '{label}' - –Ω–µ—Ç word_ids.")
                continue
            
            if label not in entity_types:
                self._log(f"[_assign_iob2_labels] –ü—Ä–æ–ø—É—Å–∫ —Å—É—â–Ω–æ—Å—Ç–∏ '{label}' - —Ç–∏–ø –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∞–Ω–Ω—ã—Ö Gemini.")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∏–Ω–¥–µ–∫—Å—ã –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
            if any(idx < 0 or idx >= num_ocr_words for idx in word_ids):
                invalid_indices = [idx for idx in word_ids if idx < 0 or idx >= num_ocr_words]
                self._log(f"[_assign_iob2_labels] –û–®–ò–ë–ö–ê: –ò–Ω–¥–µ–∫—Å—ã {invalid_indices} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ [0, {num_ocr_words-1}] –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–∏ '{label}'.")
                continue

            # –ù–∞–∑–Ω–∞—á–∞–µ–º B- –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ –∏ I- –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            for i, idx in enumerate(word_ids):
                if iob2_labels[idx] == label_other:
                    iob2_labels[idx] = f"B-{label}" if i == 0 else f"I-{label}"
                elif not (iob2_labels[idx].startswith(f"B-{label}") or iob2_labels[idx].startswith(f"I-{label}")):
                    self._log(f"[_assign_iob2_labels] –ö–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ—Ç–æ–∫ –¥–ª—è OCR —Å–ª–æ–≤–∞ —Å –∏–Ω–¥–µ–∫—Å–æ–º {idx}. "
                             f"–¢–µ–∫—É—â–∞—è –º–µ—Ç–∫–∞: {iob2_labels[idx]}, –Ω–æ–≤–∞—è: {label}. –û—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—É—é.")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–∫
        for i in range(1, len(iob2_labels)):
            current_label = iob2_labels[i]
            prev_label = iob2_labels[i-1]
            
            # –ï—Å–ª–∏ —Ç–µ–∫—É—â–∞—è –º–µ—Ç–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å I-, –Ω–æ –ø—Ä–µ–¥—ã–¥—É—â–∞—è –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–æ–π –∂–µ —Å—É—â–Ω–æ—Å—Ç–∏
            if current_label.startswith('I-'):
                entity_type = current_label[2:]  # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ 'I-'
                if not (prev_label == f"B-{entity_type}" or prev_label == f"I-{entity_type}"):
                    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ B-, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
                    iob2_labels[i] = f"B-{entity_type}"
                    self._log(f"[_assign_iob2_labels] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ—Ç–æ–∫: –∏–Ω–¥–µ–∫—Å {i}")

        return iob2_labels

    def _create_huggingface_dataset(self, processed_dataset_path, entity_types, label_other):
        """
        –°–æ–∑–¥–∞–µ—Ç Hugging Face Dataset –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º.
        
        Args:
            processed_dataset_path (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å .ocr.json, .iob2.json –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏.
            entity_types (list): –°–ø–∏—Å–æ–∫ –±–∞–∑–æ–≤—ã—Ö —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π.
            label_other (str): –ú–µ—Ç–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –≤–Ω–µ —Å—É—â–Ω–æ—Å—Ç–µ–π.
            
        Returns:
            datasets.Dataset –∏–ª–∏ None: –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        try:
            self._log(f"üöÄ –°–æ–∑–¥–∞–Ω–∏–µ Hugging Face Dataset —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º –∏–∑: {processed_dataset_path}")
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
            all_images = []
            all_words_list = []
            all_bboxes_list = []
            all_labels_list = []
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤
            all_files = os.listdir(processed_dataset_path)
            ocr_files = [f for f in all_files if f.endswith('.ocr.json')]
            
            self._log(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(ocr_files)} —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            
            for i, ocr_file in enumerate(ocr_files):
                try:
                    base_name = ocr_file[:-9]  # –£–±–∏—Ä–∞–µ–º '.ocr.json'
                    image_file = next((f for f in all_files if f.startswith(base_name) and 
                                     any(f.endswith(ext) for ext in ['.jpg', '.jpeg', '.png'])), None)
                    iob2_file = base_name + '.iob2.json'
                    
                    if not image_file or not os.path.exists(os.path.join(processed_dataset_path, iob2_file)):
                        self._log(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {ocr_file}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ IOB2 —Ñ–∞–π–ª")
                        continue
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º OCR –¥–∞–Ω–Ω—ã–µ
                    with open(os.path.join(processed_dataset_path, ocr_file), 'r', encoding='utf-8') as f:
                        ocr_data = json.load(f)
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º IOB2 –º–µ—Ç–∫–∏
                    with open(os.path.join(processed_dataset_path, iob2_file), 'r', encoding='utf-8') as f:
                        iob2_data = json.load(f)
                    
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    image_path = os.path.join(processed_dataset_path, image_file)
                    image = Image.open(image_path).convert("RGB")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                    words = [word['text'] for word in ocr_data.get('words', [])]
                    width, height = image.size
                    bboxes = [self.normalize_bbox(word.get('bbox', [0, 0, 0, 0]), width, height) 
                             for word in ocr_data.get('words', [])]
                    labels = iob2_data.get('labels', [])
                    
                    if not words or not bboxes or not labels or len(words) != len(bboxes) or len(words) != len(labels):
                        self._log(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {ocr_file}: –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω –¥–∞–Ω–Ω—ã—Ö")
                        continue
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ batch —Å–ø–∏—Å–∫–∏
                    all_images.append(image)
                    all_words_list.append(words)
                    all_bboxes_list.append(bboxes)
                    all_labels_list.append(labels)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏
                    if self.progress_callback and i % 5 == 0:
                        progress = int((i / len(ocr_files)) * 50)  # 50% –Ω–∞ –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö
                        self.progress_callback(progress)
                        
                except Exception as file_error:
                    self._log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ñ–∞–π–ª–∞ {ocr_file}: {file_error}")
                    continue
            
            if not all_images:
                self._log("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
                return None
            
            self._log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_images)} –ø—Ä–∏–º–µ—Ä–æ–≤. –ù–∞—á–∏–Ω–∞–µ–º batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º
            tokenized_data = self._batch_tokenize_layoutlm(
                images=all_images,
                words_list=all_words_list,
                bboxes_list=all_bboxes_list,
                labels_list=all_labels_list,
                model_name="microsoft/layoutlmv3-base"
            )
            
            if not tokenized_data:
                self._log("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ batch —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = Dataset.from_dict(tokenized_data)
            
            self._log(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º. –†–∞–∑–º–µ—Ä: {len(dataset)}")
            self._log(f"üíæ –ü–æ–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {list(dataset.column_names)}")
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å
            if self.progress_callback:
                self.progress_callback(100)
            
            return dataset
            
        except Exception as e:
            self._log(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def _convert_labels_to_ids(self, labels, label_other, entity_types):
        """
        –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã–µ ID.
        
        Args:
            labels (list): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–æ–∫
            label_other (str): –ú–µ—Ç–∫–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –≤–Ω–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entity_types (list): –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ —á–∏—Å–ª–æ–≤—ã—Ö ID –º–µ—Ç–æ–∫
        """
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –º–µ—Ç–æ–∫
        label2id = {label_other: 0}
        current_id = 1
        for entity_type in entity_types:
            label2id[f"B-{entity_type}"] = current_id
            current_id += 1
            label2id[f"I-{entity_type}"] = current_id
            current_id += 1
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç–∫–∏ –≤ ID
        return [label2id.get(label, 0) for label in labels]

    def split_and_save_dataset(self, hf_dataset, base_output_folder, split_ratio=0.1):
        """
        –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ train –∏ validation –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ–≥–æ.
        
        Args:
            hf_dataset (Dataset): Hugging Face Dataset –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è 
            base_output_folder (str): –ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            split_ratio (float): –î–æ–ª—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è validation (0.0-1.0)
            
        Returns:
            DatasetDict –∏–ª–∏ None: –†–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            self._log(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞. train_test_split={1-split_ratio}/{split_ratio}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
            dataset_size = len(hf_dataset)
            if dataset_size < 2:
                self._log("–î–∞—Ç–∞—Å–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            train_dir = os.path.join(base_output_folder, 'train')
            val_dir = os.path.join(base_output_folder, 'validation')
            os.makedirs(train_dir, exist_ok=True)
            os.makedirs(val_dir, exist_ok=True)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            split_dataset = hf_dataset.train_test_split(test_size=split_ratio)
            
            # –°–æ–∑–¥–∞–µ–º DatasetDict
            dataset_dict = DatasetDict({
                'train': split_dataset['train'],
                'validation': split_dataset['test']
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset_dict.save_to_disk(base_output_folder)
            
            # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞–ø–∫–∏
            for split_name, split_data in dataset_dict.items():
                output_dir = os.path.join(base_output_folder, split_name, 'images')
                os.makedirs(output_dir, exist_ok=True)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª—è 'image_path' –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ (–±—ã–ª–æ 'image_path')
                if 'image_path' in split_data.column_names:
                    image_paths = split_data['image_path']
                    self._log(f"–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ {output_dir}")
                    
                    for image_path in image_paths:
                        if os.path.exists(image_path):
                            image_name = os.path.basename(image_path)
                            output_path = os.path.join(output_dir, image_name)
                            shutil.copy2(image_path, output_path)
                else:
                    self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í –¥–∞—Ç–∞—Å–µ—Ç–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'image_path', –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            
            self._log(f"–î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–¥–µ–ª–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {base_output_folder}")
            self._log(f"Train size: {len(dataset_dict['train'])}")
            self._log(f"Validation size: {len(dataset_dict['validation'])}")
            
            return dataset_dict
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def validate_image(self, image_path: str) -> Tuple[bool, str, Optional[Image.Image]]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            Tuple[bool, str, Optional[Image.Image]]: (—É—Å–ø–µ—Ö, —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
        """
        try:
            if not os.path.exists(image_path):
                return False, f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {image_path}", None
                
            image = Image.open(image_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
            if image.size[0] < 100 or image.size[1] < 100:
                return False, f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ: {image.size}", None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç
            if image.format not in ['JPEG', 'PNG', 'TIFF']:
                return False, f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {image.format}", None
                
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if image.mode != 'RGB':
                image = image.convert('RGB')
                
            return True, "", image
            
        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}", None

    def normalize_bbox(self, bbox: List[int], width: int, height: int) -> List[int]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –¥–ª—è LayoutLM (–¥–∏–∞–ø–∞–∑–æ–Ω [0, 1023]).
        
        Args:
            bbox: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã [x1, y1, x2, y2]
            width: –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            height: –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            List[int]: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã [x1, y1, x2, y2]
        """
        try:
            if not bbox or len(bbox) != 4:
                return [0, 0, 0, 0]
                
            if width <= 0 or height <= 0:
                return [0, 0, 0, 0]
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            x1, y1, x2, y2 = bbox
            
            # –ö–ª–∞–º–ø–∏–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∫ –≥—Ä–∞–Ω–∏—Ü–∞–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            x1 = max(0, min(width - 1, x1))
            y1 = max(0, min(height - 1, y1))
            x2 = max(0, min(width - 1, x2))
            y2 = max(0, min(height - 1, y2))
                
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0, 1023] –¥–ª—è LayoutLM
            x1_norm = max(0, min(1023, int(1023 * x1 / width)))
            y1_norm = max(0, min(1023, int(1023 * y1 / height)))
            x2_norm = max(0, min(1023, int(1023 * x2 / width)))
            y2_norm = max(0, min(1023, int(1023 * y2 / height)))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
            if x2_norm < x1_norm:
                x1_norm, x2_norm = x2_norm, x1_norm
            if y2_norm < y1_norm:
                y1_norm, y2_norm = y2_norm, y1_norm
                
            # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ bbox –Ω–µ –Ω—É–ª–µ–≤–æ–π
            if x1_norm == x2_norm:
                x2_norm = min(1023, x1_norm + 1)
            if y1_norm == y2_norm:
                y2_norm = min(1023, y1_norm + 1)
                
            return [x1_norm, y1_norm, x2_norm, y2_norm]
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ bbox: {str(e)}")
            return [0, 0, 0, 0]

    def apply_augmentation(self, image: Image.Image) -> Image.Image:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é"""
        augmented = self.augmentation(image=np.array(image))
        return Image.fromarray(augmented['image'])

    def compute_class_weights(self, labels):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º IOB2 –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã
        label_counts = Counter(label for doc_labels in labels for label in doc_labels)
        unique_labels = sorted(label_counts.keys())
        label_to_id = {label: i for i, label in enumerate(unique_labels)}
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        numeric_labels = [label_to_id[label] for doc_labels in labels for label in doc_labels]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞
        class_weights = compute_class_weight(
            class_weight='balanced',
            classes=np.unique(numeric_labels),
            y=numeric_labels
        )
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –≤–µ—Å–æ–≤
        return {label: weight for label, weight in zip(unique_labels, class_weights)}

    def prepare_dataset(self, images, annotations, split_ratio=0.2, augment=True):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π –∏ –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–æ–π"""
        processed_images = []
        processed_annotations = []
        
        for img, ann in zip(images, annotations):
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed_images.append(img)
            processed_annotations.append(ann)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if augment:
                aug_img = self.apply_augmentation(img)
                processed_images.append(aug_img)
                processed_annotations.append(ann)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                
        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
        class_weights = self.compute_class_weights(processed_annotations)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val
        split_idx = int(len(processed_images) * (1 - split_ratio))
        
        train_images = processed_images[:split_idx]
        train_annotations = processed_annotations[:split_idx]
        val_images = processed_images[split_idx:]
        val_annotations = processed_annotations[split_idx:]
        
        return {
            'train': (train_images, train_annotations),
            'val': (val_images, val_annotations),
            'class_weights': class_weights
        }

    def _process_document_page(self, 
                             image: Image.Image,
                             base_name: str,
                             dataset_folder: str,
                             training_prompt: str) -> Optional[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–æ–∫—É–º–µ–Ω—Ç–∞.
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            base_name: –ë–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            dataset_folder: –ü–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            training_prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è Gemini
            
        Returns:
            Optional[Dict]: –î–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_path = os.path.join(dataset_folder, 'images', f"{base_name}.jpg")
            image.save(image_path, 'JPEG')
            
            # –ü–æ–ª—É—á–∞–µ–º OCR –¥–∞–Ω–Ω—ã–µ
            ocr_data = self.ocr_processor.process_file(image_path)
            if not ocr_data or not ocr_data.get('words'):
                self._log(f"[–û–®–ò–ë–ö–ê] OCR –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∞–π–ª–∞: {base_name}")
                return None
                
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ç Gemini
            gemini_result = self.gemini_processor.process_file(image_path, training_prompt)
            if not gemini_result:
                self._log(f"–û—à–∏–±–∫–∞ Gemini –¥–ª—è {base_name}")
                return None
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç Gemini –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞
            self._log(f"–†–µ–∑—É–ª—å—Ç–∞—Ç Gemini –¥–ª—è {base_name}:")
            self._log(f"–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞: {type(gemini_result)}")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ JSON —Å–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π –æ—Ç–≤–µ—Ç–∞
            gemini_json = json.dumps(gemini_result, ensure_ascii=False, indent=2)
            self._log(f"–î–∞–Ω–Ω—ã–µ Gemini API:\n{gemini_json}")
                
            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
            matched_entities = self._match_gemini_and_ocr(gemini_result, ocr_data)
            if not matched_entities:
                self._log(f"–ù–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è {base_name}")
                return None
                
            # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É
            words = ocr_data['words']
            width, height = image.size
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º bbox –∏ —Å–æ–∑–¥–∞–µ–º labels
            bboxes = []
            texts = []
            
            for word in words:
                # –ü–æ–ª—É—á–∞–µ–º bbox –∏–∑ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                bbox = word.get('bbox', None)
                if not bbox:
                    # –ï—Å–ª–∏ bbox –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º —Å–æ–±—Ä–∞—Ç—å –µ–≥–æ –∏–∑ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                    bbox = [
                        word.get('x', 0),
                        word.get('y', 0),
                        word.get('x', 0) + word.get('width', 0),
                        word.get('y', 0) + word.get('height', 0)
                    ]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å bbox
                if not all(isinstance(coord, (int, float)) for coord in bbox) or len(bbox) != 4:
                    self._log(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π bbox –¥–ª—è —Å–ª–æ–≤–∞ '{word.get('text', '')}': {bbox}")
                    bbox = [0, 0, 0, 0]
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º bbox
                try:
                    normalized_bbox = self.normalize_bbox(bbox, width, height)
                except Exception as e:
                    self._log(f"–û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ bbox {bbox}: {str(e)}")
                    normalized_bbox = [0, 0, 0, 0]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π bbox
                if normalized_bbox == [0, 0, 0, 0]:
                    self._log(f"–ü—Ä–æ–ø—É—Å–∫ —Å–ª–æ–≤–∞ '{word.get('text', '')}' —Å –Ω—É–ª–µ–≤—ã–º bbox")
                    continue
                
                if not (0 <= normalized_bbox[0] <= 1000 and 0 <= normalized_bbox[1] <= 1000 and
                       0 <= normalized_bbox[2] <= 1000 and 0 <= normalized_bbox[3] <= 1000):
                    self._log(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π bbox –¥–ª—è —Å–ª–æ–≤–∞ '{word.get('text', '')}': {normalized_bbox}")
                    continue
                
                bboxes.append(normalized_bbox)
                texts.append(word['text'])
            
            if not texts or not bboxes:
                self._log(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–ª–æ–≤ —Å bbox –¥–ª—è {base_name}")
                return None
            
            # –°–æ–∑–¥–∞–µ–º IOB2 —Ä–∞–∑–º–µ—Ç–∫—É
            iob2_labels = self._assign_iob2_labels(matched_entities, words, list(gemini_result.keys()), "O")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω
            if len(texts) != len(bboxes) or len(texts) != len(iob2_labels):
                self._log(f"–ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª–∏–Ω –¥–∞–Ω–Ω—ã—Ö: texts={len(texts)}, bboxes={len(bboxes)}, labels={len(iob2_labels)}")
                return None
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            debug_data = {
                'words': texts,
                'bboxes': bboxes,
                'labels': iob2_labels,
                'entities': matched_entities,
                'gemini_result': gemini_result  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç Gemini –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            }
            debug_path = os.path.join(dataset_folder, f"{base_name}_debug.json")
            with open(debug_path, 'w', encoding='utf-8') as f:
                json.dump(debug_data, f, ensure_ascii=False, indent=2)
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º —Å—É—â–Ω–æ—Å—Ç—è–º –∏ –º–µ—Ç–∫–∞–º
            label_counts = {}
            for label in iob2_labels:
                label_counts[label] = label_counts.get(label, 0) + 1
            
            self._log(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –¥–ª—è {base_name}:")
            self._log(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(texts)}")
            self._log(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫: {len(label_counts)}")
            self._log(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫: {json.dumps(label_counts, ensure_ascii=False, indent=2)}")
            
            # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ–ª–µ–π Gemini –∏ –º–µ—Ç–æ–∫ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
            self._log("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö Gemini –∏ –∑–∞–ø–∏—Å–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:")
            gemini_fields = []
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ Gemini –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞
            if 'fields' in gemini_result and isinstance(gemini_result['fields'], list):
                for field in gemini_result['fields']:
                    if isinstance(field, dict) and 'field_name' in field and 'field_value' in field:
                        field_type = self._normalize_field_type(field['field_name'])
                        field_value = field['field_value']
                        gemini_fields.append((field_type, field_value))
            else:
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä—è–º—ã–µ –∫–ª—é—á–∏
                for key, value in gemini_result.items():
                    if key not in ['source_image', 'processed_at', 'note_gemini']:
                        if isinstance(value, list):
                            # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                            self._log(f"–¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ '{key}' —Å {len(value)} —Å—Ç—Ä–æ–∫–∞–º–∏")
                        elif isinstance(value, dict):
                            # –í–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã (supplier, customer, etc)
                            for sub_key, sub_value in value.items():
                                gemini_fields.append((f"{key}_{sub_key}", sub_value))
                        else:
                            gemini_fields.append((key, value))
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            b_count = sum(1 for label in iob2_labels if label.startswith('B-'))
            i_count = sum(1 for label in iob2_labels if label.startswith('I-'))
            total_fields = len(gemini_fields)
            
            self._log(f"–í—Å–µ–≥–æ –ø–æ–ª–µ–π –≤ Gemini: {total_fields}")
            self._log(f"–ù–∞–π–¥–µ–Ω–æ –Ω–∞—á–∞–ª –ø–æ–ª–µ–π (B-): {b_count}")
            self._log(f"–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–π –ø–æ–ª–µ–π (I-): {i_count}")
            
            if total_fields > 0:
                matching_rate = b_count / total_fields
                self._log(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π: {matching_rate:.2%}")
                
                if matching_rate < 0.5:
                    self._log("–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª–µ–π")
                elif matching_rate > 0.8:
                    self._log("–û–¢–õ–ò–ß–ù–û: –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ–ª–µ–π")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏ 'words', 'bboxes', 'labels' –∏ 'entities'
            return {
                'words': words,
                'bboxes': bboxes,
                'labels': iob2_labels,
                'entities': entities
            }
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {base_name}: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def _create_dataset(self, processed_data, use_augmentation=True):
        """
        –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            processed_data: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            use_augmentation: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            
        Returns:
            Optional[Dataset]: –°–æ–∑–¥–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            if not processed_data:
                self._log("–û–®–ò–ë–ö–ê: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
                return None
                
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            dataset_dict = {
                'image_path': [],
                'words': [],
                'bboxes': [],
                'labels': []
            }
            
            for data_idx, data in enumerate(processed_data):
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π
                    if not isinstance(data, dict):
                        self._log(f"–û–®–ò–ë–ö–ê: –≠–ª–µ–º–µ–Ω—Ç –¥–∞–Ω–Ω—ã—Ö {data_idx} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º: {type(data)}")
                        continue
                        
                    required_fields = ['image_path', 'words', 'bboxes', 'labels']
                    missing_fields = [field for field in required_fields if field not in data]
                    if missing_fields:
                        self._log(f"–û–®–ò–ë–ö–ê: –í —ç–ª–µ–º–µ–Ω—Ç–µ –¥–∞–Ω–Ω—ã—Ö {data_idx} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—è: {missing_fields}")
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –º–∞—Å—Å–∏–≤—ã –¥–∞–Ω–Ω—ã—Ö –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–ª–∏–Ω—É
                    words = data['words']
                    bboxes = data['bboxes']
                    labels = data['labels']
                    
                    if not isinstance(words, list) or not isinstance(bboxes, list) or not isinstance(labels, list):
                        self._log(f"–û–®–ò–ë–ö–ê: words, bboxes –∏–ª–∏ labels –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∞–º–∏ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ {data_idx}")
                        continue
                        
                    if not words or not bboxes or not labels:
                        self._log(f"–û–®–ò–ë–ö–ê: –ü—É—Å—Ç—ã–µ words, bboxes –∏–ª–∏ labels –≤ —ç–ª–µ–º–µ–Ω—Ç–µ {data_idx}")
                        continue
                        
                    if len(words) != len(bboxes) or len(words) != len(labels):
                        self._log(f"–û–®–ò–ë–ö–ê: –†–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞ words ({len(words)}), bboxes ({len(bboxes)}) –∏ labels ({len(labels)}) –≤ —ç–ª–µ–º–µ–Ω—Ç–µ {data_idx}")
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã—Ä–æ–≤–Ω—è—Ç—å –¥–ª–∏–Ω—ã –º–∞—Å—Å–∏–≤–æ–≤
                        min_len = min(len(words), len(bboxes), len(labels))
                        if min_len > 0:
                            self._log(f"–û–±—Ä–µ–∑–∞–µ–º –º–∞—Å—Å–∏–≤—ã –¥–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π –¥–ª–∏–Ω—ã: {min_len}")
                            words = words[:min_len]
                            bboxes = bboxes[:min_len]
                            labels = labels[:min_len]
                        else:
                            self._log(f"–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã—Ä–æ–≤–Ω—è—Ç—å –º–∞—Å—Å–∏–≤—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç {data_idx}")
                            continue
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç
                    dataset_dict['image_path'].append(data['image_path'])
                    dataset_dict['words'].append(words)
                    dataset_dict['bboxes'].append(bboxes)
                    dataset_dict['labels'].append(labels)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    if use_augmentation:
                        try:
                            image = Image.open(data['image_path'])
                            aug_image = self.apply_augmentation(image)
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            aug_path = data['image_path'].replace('.jpg', '_aug.jpg')
                            aug_image.save(aug_path, 'JPEG')
                            
                            dataset_dict['image_path'].append(aug_path)
                            dataset_dict['words'].append(words)
                            dataset_dict['bboxes'].append(bboxes)
                            dataset_dict['labels'].append(labels)
                            
                        except Exception as e:
                            self._log(f"–û—à–∏–±–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞ {data_idx}: {str(e)}")
                            continue
                except Exception as data_error:
                    self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–∞–Ω–Ω—ã—Ö {data_idx}: {str(data_error)}")
                    continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
            if not dataset_dict['image_path']:
                self._log("–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
                return None
                
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            try:
                from datasets import Dataset, Features, Sequence, Value
                
                features = Features({
                    'image_path': Value('string'),
                    'words': Sequence(Value('string')),
                    'bboxes': Sequence(Sequence(Value('int64'), length=4)),
                    'labels': Sequence(Value('string'))
                })
                
                dataset = Dataset.from_dict(dataset_dict, features=features)
                
                self._log(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Ä–∞–∑–º–µ—Ä–æ–º {len(dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                return dataset
            except Exception as dataset_error:
                self._log(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞ Dataset: {str(dataset_error)}")
                return None
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def _create_iob2_labels(self, words, matched_entities):
        """–°–æ–∑–¥–∞–µ—Ç IOB2 —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è —Å–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        labels = ["O"] * len(words)
        for entity in matched_entities:
            label = entity['entity_type']
            indices = entity.get('word_ids', [])
            if not indices:
                continue
            
            for i, idx in enumerate(indices):
                if 0 <= idx < len(labels):
                    labels[idx] = f"{'B' if i == 0 else 'I'}-{label}"
        
        return labels

    def _is_image_file(self, filename):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏–ª–∏ PDF –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è.
        
        Args:
            filename (str): –ò–º—è —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            bool: True, –µ—Å–ª–∏ —ç—Ç–æ —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ PDF, –∏–Ω–∞—á–µ False
        """
        extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.pdf']
        ext = os.path.splitext(filename)[1].lower()
        return ext in extensions

    def _process_single_image(self, image_path, base_name, train_folder, training_prompt):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ PDF —Ñ–∞–π–ª –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            image_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ PDF
            base_name (str): –ë–∞–∑–æ–≤–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            train_folder (str): –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            training_prompt (str): –ü—Ä–æ–º–ø—Ç –¥–ª—è Gemini
            
        Returns:
            Optional[Dict]: –°–ª–æ–≤–∞—Ä—å —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª PDF
            if image_path.lower().endswith('.pdf'):
                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—É—Ç—å –∫ Poppler
                poppler_path = self.app_config.POPPLER_PATH
                if not poppler_path or not os.path.exists(poppler_path):
                    self._log(f"[–û–®–ò–ë–ö–ê] –ü—É—Ç—å –∫ Poppler –Ω–µ –Ω–∞–π–¥–µ–Ω: {poppler_path}")
                    return None
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                images = convert_from_path(image_path, poppler_path=poppler_path)
                if not images:
                    self._log(f"[–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_path}")
                    return None
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                temp_image_path = os.path.join(train_folder, f"{base_name}_temp.jpg")
                images[0].save(temp_image_path, 'JPEG')
                image_path = temp_image_path
            
            # –ü–æ–ª—É—á–∞–µ–º OCR –¥–∞–Ω–Ω—ã–µ
            ocr_data = self.ocr_processor.process_file(image_path)
            if not ocr_data:
                self._log(f"[–û–®–ò–ë–ö–ê] OCR –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∞–π–ª–∞: {base_name}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ ocr_data - —Å–ª–æ–≤–∞—Ä—å
            if not isinstance(ocr_data, dict):
                self._log(f"[–û–®–ò–ë–ö–ê] OCR –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ª–æ–≤–∞—Ä—å: {type(ocr_data)}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤
            if 'words' not in ocr_data or not ocr_data.get('words'):
                self._log(f"[–û–®–ò–ë–ö–ê] OCR –Ω–µ –≤–µ—Ä–Ω—É–ª —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∞–π–ª–∞: {base_name}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ words - —Å–ø–∏—Å–æ–∫
            if not isinstance(ocr_data['words'], list):
                self._log(f"[–û–®–ò–ë–ö–ê] OCR –≤–µ—Ä–Ω—É–ª —Å–ª–æ–≤–∞ –Ω–µ –≤ –≤–∏–¥–µ —Å–ø–∏—Å–∫–∞: {type(ocr_data['words'])}")
                return None
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è Gemini
            words = []
            for word in ocr_data.get('words', []):
                if not isinstance(word, dict):
                    continue
                text = word.get('text', '').strip()
                if text:
                    words.append(text)
            
            if not words:
                self._log(f"[–û–®–ò–ë–ö–ê] –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Ñ–∞–π–ª–µ: {base_name}")
                return None
                
            text_for_gemini = ' '.join(words)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
            if len(text_for_gemini) < 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                self._log(f"[–û–®–ò–ë–ö–ê] –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {text_for_gemini}")
                return None
            
            self._log(f"[–ò–ù–§–û] –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω —Ç–µ–∫—Å—Ç –¥–ª—è Gemini ({len(words)} —Å–ª–æ–≤)")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ Gemini
            gemini_data = self.gemini_processor.process_file(text_for_gemini, custom_prompt=training_prompt)
            if not gemini_data:
                self._log(f"[–û–®–ò–ë–ö–ê] Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∞–π–ª–∞: {base_name}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ gemini_data - —Å–ª–æ–≤–∞—Ä—å
            if not isinstance(gemini_data, dict):
                self._log(f"[–û–®–ò–ë–ö–ê] Gemini –≤–µ—Ä–Ω—É–ª –Ω–µ —Å–ª–æ–≤–∞—Ä—å: {type(gemini_data)}")
                if isinstance(gemini_data, str):
                    try:
                        # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É –≤ JSON
                        gemini_data = json.loads(gemini_data)
                    except json.JSONDecodeError:
                        self._log(f"[–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç Gemini –≤ JSON")
                        return None
                else:
                    return None
            
            # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ Gemini —Å OCR –¥–∞–Ω–Ω—ã–º–∏
            matched_data = self._match_gemini_and_ocr(gemini_data, ocr_data)
            if not matched_data:
                self._log(f"[DataPreparator] [_process_single_image] –ù–µ—Ç —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∞–π–ª–∞: {base_name}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ matched_data - —Å–ª–æ–≤–∞—Ä—å —Å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–º–∏ –ø–æ–ª—è–º–∏
            required_fields = ['words', 'bboxes', 'labels']
            if not isinstance(matched_data, dict) or not all(field in matched_data for field in required_fields):
                self._log(f"[–û–®–ò–ë–ö–ê] _match_gemini_and_ocr –≤–µ—Ä–Ω—É–ª –Ω–µ–ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {type(matched_data)}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–µ—Ç–∫–∞ –Ω–µ "O"
            if all(label == "O" for label in matched_data.get('labels', [])):
                self._log(f"[–û–®–ò–ë–ö–ê] –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ –º–µ—Ç–∫–∏ –∏–º–µ—é—Ç –∑–Ω–∞—á–µ–Ω–∏–µ 'O' (–Ω–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π)")
                return None
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –ø–∞–ø–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            try:
                image = Image.open(image_path)
                output_image_path = os.path.join(train_folder, f"{base_name}.jpg")
                image.save(output_image_path, 'JPEG')
            except Exception as img_error:
                self._log(f"[–û–®–ò–ë–ö–ê] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {image_path}: {str(img_error)}")
                return None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                'image_path': output_image_path,
                'words': matched_data['words'],
                'bboxes': matched_data['bboxes'],
                'labels': matched_data['labels']
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            try:
                debug_info = {
                    'ocr_words_count': len(ocr_data['words']),
                    'matched_words_count': len(matched_data['words']),
                    'labels_count': len(matched_data['labels']),
                    'gemini_entities': list(gemini_data.keys()) if isinstance(gemini_data, dict) else None
                }
                debug_path = os.path.join(train_folder, f"{base_name}_debug.json")
                with open(debug_path, 'w', encoding='utf-8') as f:
                    json.dump(debug_info, f, ensure_ascii=False, indent=2)
            except Exception as debug_error:
                self._log(f"[–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é: {str(debug_error)}")
            
            return result
            
        except Exception as e:
            self._log(f"[–û–®–ò–ë–ö–ê] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {base_name}: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def prepare_for_layoutlm(self, source_folder: str, dataset_name: str, training_prompt: str = None) -> Optional[str]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LayoutLM.
        
        Args:
            source_folder (str): –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            dataset_name (str): –ò–º—è –¥–ª—è —Å–æ–∑–¥–∞–≤–∞–µ–º–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
            training_prompt (str, optional): –ü—Ä–æ–º–ø—Ç –¥–ª—è Gemini (–µ—Å–ª–∏ None, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
            
        Returns:
            Optional[str]: –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            # üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–ª–µ–π —Ç–∞–±–ª–∏—Ü—ã
            if not training_prompt:
                training_prompt = self.get_training_prompt("layoutlm")
                self._log("ü§ñ –ü—Ä–æ–º–ø—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–ª–µ–π —Ç–∞–±–ª–∏—Ü—ã")
            else:
                self._log("üìù –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç")
            
            self._log(f"üìè –ü—Ä–æ–º–ø—Ç ({len(training_prompt)} —Å–∏–º–≤–æ–ª–æ–≤):")
            # –í—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            preview = training_prompt[:300] + "..." if len(training_prompt) > 300 else training_prompt
            self._log(f"üìñ –ü—Ä–µ–≤—å—é –ø—Ä–æ–º–ø—Ç–∞:\n{preview}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            if not os.path.exists(source_folder) or not os.path.isdir(source_folder):
                self._log(f"–û–®–ò–ë–ö–ê: –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {source_folder}")
                return None
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            files = [f for f in os.listdir(source_folder) if self._is_image_file(f)]
            if not files:
                self._log(f"–û–®–ò–ë–ö–ê: –í –ø–∞–ø–∫–µ {source_folder} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ PDF-—Ñ–∞–π–ª–æ–≤")
                return None
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            dataset_folder = os.path.join(self.app_config.TRAINING_DATASETS_PATH, dataset_name)
            os.makedirs(dataset_folder, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ –ø–æ–ª–µ–π
            self._save_dataset_metadata(dataset_folder, {
                'source_folder': os.path.relpath(source_folder, self.app_config.PROJECT_ROOT),
                'dataset_name': dataset_name,
                'task_type': 'layoutlm',
                'training_prompt': training_prompt,
                'fields_source': 'field_manager' if self.field_manager else 'static',
                'active_fields': [f.id for f in self.field_manager.get_enabled_fields()] if self.field_manager else None,
                'entity_types': self.get_entity_types_from_fields()
            })
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–µ
            processed_files = []
            self._log(f"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(files)} —Ñ–∞–π–ª–æ–≤ –∏–∑ {source_folder}")
            
            for idx, filename in enumerate(files):
                self._log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ {idx+1}/{len(files)}: {filename}")
                if self._is_image_file(filename):
                    image_path = os.path.join(source_folder, filename)
                    base_name = os.path.splitext(filename)[0]
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    result = self._process_single_image(image_path, base_name, dataset_folder, training_prompt)
                    if result:
                        processed_files.append(result)
                    else:
                        self._log(f"–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª {filename}")
            
            if not processed_files:
                self._log("–û–®–ò–ë–ö–ê: –ù–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
                return None
                
            self._log(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_files)} —Ñ–∞–π–ª–æ–≤ –∏–∑ {len(files)}")
            
            # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
            dataset = self._create_dataset(processed_files)
            if dataset is None:
                self._log("–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç")
                return None
                
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º
            output_path = os.path.join(dataset_folder, "dataset_dict")
            self._log(f"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/validation –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ {output_path}...")
            self.split_and_save_dataset(dataset, output_path)
            
            self._log(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LayoutLM –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {output_path}")
            return output_path
            
        except Exception as e:
            self._log(f"–û–®–ò–ë–ö–ê –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LayoutLM: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def create_full_dataset(self, processed_records: List[Dict], output_dir: str = None) -> Optional[Dataset]:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π.
        
        Args:
            processed_records: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            Dataset –∏–ª–∏ None: –î–∞—Ç–∞—Å–µ—Ç Hugging Face –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        """
        try:
            self._log(f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ {len(processed_records)} –∑–∞–ø–∏—Å–µ–π")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ records –Ω–µ –ø—É—Å—Ç–æ–π
            if not processed_records:
                self._log("–û–®–ò–ë–ö–ê: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –ø—É—Å—Ç")
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            required_fields = ['image_path', 'words', 'bboxes', 'labels']
            for i, record in enumerate(processed_records):
                missing_fields = [field for field in required_fields if field not in record]
                if missing_fields:
                    self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ó–∞–ø–∏—Å—å {i} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª—è: {missing_fields}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–æ–ª–µ–π
                    for field in missing_fields:
                        if field == 'image_path':
                            record[field] = ""
                        elif field == 'words':
                            record[field] = []
                        elif field == 'bboxes':
                            record[field] = []
                        elif field == 'labels':
                            record[field] = []
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –ø—É—Ç—è–º–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
            valid_records = [record for record in processed_records if record.get('image_path') and os.path.exists(record['image_path'])]
            
            if len(valid_records) == 0:
                self._log("–û–®–ò–ë–ö–ê: –ù–µ—Ç –∑–∞–ø–∏—Å–µ–π —Å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º")
                return None
            
            if len(valid_records) < len(processed_records):
                self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(processed_records) - len(valid_records)} –∑–∞–ø–∏—Å–µ–π —Å –Ω–µ–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º")
            
            # –°–æ–∑–¥–∞–µ–º Hugging Face Dataset
            from datasets import Dataset as HFDataset
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            num_words = [len(record['words']) for record in valid_records]
            num_bboxes = [len(record['bboxes']) for record in valid_records]
            num_labels = [len(record['labels']) for record in valid_records]
            
            if not all(num_words[i] == num_bboxes[i] == num_labels[i] for i in range(len(valid_records))):
                self._log("–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ words, bboxes –∏ labels")
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                for record in valid_records:
                    max_len = max(len(record['words']), len(record['bboxes']), len(record['labels']))
                    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã –≤—Å–µ—Ö —Å–ø–∏—Å–∫–æ–≤
                    if len(record['words']) < max_len:
                        record['words'].extend(["PAD"] * (max_len - len(record['words'])))
                    if len(record['bboxes']) < max_len:
                        record['bboxes'].extend([[0, 0, 0, 0]] * (max_len - len(record['bboxes'])))
                    if len(record['labels']) < max_len:
                        record['labels'].extend(["O"] * (max_len - len(record['labels'])))
                    # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    record['words'] = record['words'][:max_len]
                    record['bboxes'] = record['bboxes'][:max_len]
                    record['labels'] = record['labels'][:max_len]
            
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            try:
                dataset = HFDataset.from_dict({
                    'image_path': [record['image_path'] for record in valid_records],
                    'words': [record['words'] for record in valid_records],
                    'bboxes': [record['bboxes'] for record in valid_records],
                    'labels': [record['labels'] for record in valid_records]
                })
                
                # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
                self._log(f"–°–æ–∑–¥–∞–Ω –¥–∞—Ç–∞—Å–µ—Ç —Å {len(dataset)} –∑–∞–ø–∏—Å—è–º–∏")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
                if output_dir:
                    self._save_dataset(dataset, output_dir)
                
                return dataset
                
            except Exception as e:
                self._log(f"–û–®–ò–ë–ö–ê –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ HFDataset: {str(e)}")
                import traceback
                self._log(traceback.format_exc())
                return None
                
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {str(e)}")
            import traceback
            self._log(traceback.format_exc())
            return None

    def _is_matching_text(self, text1, text2, threshold=0.5):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ª–∏ –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ –¥—Ä—É–≥ –¥—Ä—É–≥—É —Å —É—á–µ—Ç–æ–º —Å—Ö–æ–∂–µ—Å—Ç–∏.
        
        Args:
            text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0-1.0)
            
        Returns:
            bool: True, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥ –¥—Ä—É–≥—É
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            if text1 == text2:
                return True
                
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
            t1 = self._normalize_text(text1)
            t2 = self._normalize_text(text2)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
            if t1 == t2:
                return True
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
            if t1 in t2 or t2 in t1:
                # –ï—Å–ª–∏ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç —è–≤–ª—è–µ—Ç—Å—è –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π –¥—Ä—É–≥–æ–≥–æ, –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é –¥–ª–∏–Ω—É
                min_len = min(len(t1), len(t2))
                max_len = max(len(t1), len(t2))
                if min_len / max_len >= threshold:
                    return True
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —á–∞—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π)
            if self._has_common_numbers(t1, t2):
                return True
                
            # –†–∞—Å—á–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
            if len(t1) <= 20 and len(t2) <= 20:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞ –≤–º–µ—Å—Ç–æ –≤–Ω–µ—à–Ω–µ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
                distance = levenshtein_distance(t1, t2)
                max_len = max(len(t1), len(t2))
                if max_len == 0:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                    return False
                similarity = 1.0 - (distance / max_len)
                return similarity >= threshold
                
            return False
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–∫—Å—Ç–æ–≤: {str(e)}")
            return False
    
    def _normalize_text(self, text):
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è, —É–¥–∞–ª—è—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥—è –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            str: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not text:
            return ""
            
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = str(text).lower()
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = " ".join(text.split())
        
        # –£–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–∏–º–≤–æ–ª—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–∂–Ω—ã–µ –¥–ª—è —á–∏—Å–µ–ª –∏ –¥–∞—Ç
        for char in ['¬´', '¬ª', '"', "'", '(', ')', '[', ']', '{', '}']:
            text = text.replace(char, '')
        # –ù–ï —É–¥–∞–ª—è–µ–º —Ç–æ—á–∫–∏, –∑–∞–ø—è—Ç—ã–µ –∏ —Ç–∏—Ä–µ - –æ–Ω–∏ –≤–∞–∂–Ω—ã –¥–ª—è —á–∏—Å–µ–ª –∏ –¥–∞—Ç
            
        return text
    
    def _has_common_numbers(self, str1, str2):
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–∏ –¥–≤–µ —Å—Ç—Ä–æ–∫–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.
        
        Args:
            str1: –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞
            str2: –í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞
            
        Returns:
            bool: True, –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –æ–±—â–µ–µ —á–∏—Å–ª–æ
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∏—Å–ª–∞ –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
            import re
            numbers1 = set(re.findall(r'\d+(?:[.,]\d+)?', str1))
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∏—Å–ª–∞ –∏–∑ –≤—Ç–æ—Ä–æ–π —Å—Ç—Ä–æ–∫–∏
            numbers2 = set(re.findall(r'\d+(?:[.,]\d+)?', str2))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –æ–±—â–∏–µ —á–∏—Å–ª–∞
            common_numbers = numbers1.intersection(numbers2)
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –æ–±—â–µ–µ —á–∏—Å–ª–æ –∏ –æ–Ω–æ –∑–Ω–∞—á–∏–º–æ (–¥–ª–∏–Ω–Ω–µ–µ 1 —Ü–∏—Ñ—Ä—ã)
            significant_numbers = {num for num in common_numbers if len(num.replace('.', '').replace(',', '')) > 1}
            return bool(significant_numbers)
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —á–∏—Å–µ–ª –≤ —Å—Ç—Ä–æ–∫–∞—Ö: {str(e)}")
            return False
    
    def _normalize_field_type(self, field_name):
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è –ø–æ–ª—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ IOB2 —Ç–µ–≥–∞—Ö.
        
        Args:
            field_name: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–º—è –ø–æ–ª—è
            
        Returns:
            str: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∏–º—è –ø–æ–ª—è
        """
        try:
            if not field_name:
                return "ENTITY"
                
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
            field_name = field_name.upper()
            
            # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
            field_name = field_name.replace(' ', '_')
            
            # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–æ–ª—è –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            field_mapping = {
                # –ö–æ–º–ø–∞–Ω–∏—è/–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
                '–ö–û–ú–ü–ê–ù–ò–Ø': 'COMPANY',
                '–û–†–ì–ê–ù–ò–ó–ê–¶–ò–Ø': 'COMPANY',
                '–ü–û–°–¢–ê–í–©–ò–ö': 'COMPANY',
                '–ü–†–û–î–ê–í–ï–¶': 'COMPANY',
                '–ü–û–ö–£–ü–ê–¢–ï–õ–¨': 'COMPANY',
                'CONTRACTOR': 'COMPANY',
                'SUPPLIER': 'COMPANY',
                'VENDOR': 'COMPANY',
                'CUSTOMER': 'COMPANY',
                'FIRM': 'COMPANY',
                'SELLER': 'COMPANY',
                'BUYER': 'COMPANY',
                'ORGANIZATION': 'COMPANY',
                
                # –î–∞—Ç–∞
                '–î–ê–¢–ê': 'DATE',
                '–î–ï–ù–¨': 'DATE',
                '–î–ê–¢–ê_–í–´–°–¢–ê–í–õ–ï–ù–ò–Ø': 'DATE',
                '–î–ê–¢–ê_–î–û–ö–£–ú–ï–ù–¢–ê': 'DATE',
                'INVOICE_DATE': 'DATE',
                'DOCUMENT_DATE': 'DATE',
                
                # –ù–æ–º–µ—Ä —Å—á–µ—Ç–∞/–∏–Ω–≤–æ–π—Å–∞
                '–ù–û–ú–ï–†': 'INVOICE_ID',
                '–ù–û–ú–ï–†_–°–ß–ï–¢–ê': 'INVOICE_ID',
                '–ù–û–ú–ï–†_–ò–ù–í–û–ô–°–ê': 'INVOICE_ID',
                '–ù–û–ú–ï–†_–î–û–ö–£–ú–ï–ù–¢–ê': 'INVOICE_ID',
                '–î–û–ö–£–ú–ï–ù–¢_–ù–û–ú–ï–†': 'INVOICE_ID',
                '–°–ß–ï–¢': 'INVOICE_ID',
                '–ò–ù–í–û–ô–°': 'INVOICE_ID',
                'INVOICE': 'INVOICE_ID',
                'INVOICE_NUMBER': 'INVOICE_ID',
                'DOCUMENT_NUMBER': 'INVOICE_ID',
                
                # –°—É–º–º–∞
                '–°–£–ú–ú–ê': 'TOTAL',
                '–ò–¢–û–ì–û': 'TOTAL',
                '–í–°–ï–ì–û': 'TOTAL',
                '–ò–¢–û–ì–û–í–ê–Ø_–°–£–ú–ú–ê': 'TOTAL',
                '–°–£–ú–ú–ê_–ö_–û–ü–õ–ê–¢–ï': 'TOTAL',
                'TOTAL_AMOUNT': 'TOTAL',
                'AMOUNT': 'TOTAL',
                'AMOUNT_DUE': 'TOTAL',
                'TOTAL_SUM': 'TOTAL',
                'SUM': 'TOTAL',
                
                # –ê–¥—Ä–µ—Å
                '–ê–î–†–ï–°': 'ADDRESS',
                '–Æ–†–ò–î–ò–ß–ï–°–ö–ò–ô_–ê–î–†–ï–°': 'ADDRESS',
                '–§–ê–ö–¢–ò–ß–ï–°–ö–ò–ô_–ê–î–†–ï–°': 'ADDRESS',
                'ADDRESS': 'ADDRESS',
                'LEGAL_ADDRESS': 'ADDRESS',
                'PHYSICAL_ADDRESS': 'ADDRESS',
                
                # –ò–ù–ù/–û–ì–†–ù
                '–ò–ù–ù': 'TAX_ID',
                '–û–ì–†–ù': 'TAX_ID',
                'TAX_ID': 'TAX_ID',
                'VAT': 'TAX_ID',
                'VAT_NUMBER': 'TAX_ID',
                
                # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞/—É—Å–ª—É–≥–∏
                '–ù–ê–ò–ú–ï–ù–û–í–ê–ù–ò–ï': 'NAME',
                '–¢–û–í–ê–†': 'NAME',
                '–ü–†–û–î–£–ö–¢': 'NAME',
                '–£–°–õ–£–ì–ê': 'NAME',
                '–†–ê–ë–û–¢–ê': 'NAME',
                '–ü–û–ó–ò–¶–ò–Ø': 'NAME',
                'ITEM': 'NAME',
                'PRODUCT': 'NAME',
                'SERVICE': 'NAME',
                'DESCRIPTION': 'NAME',
                'TITLE': 'NAME',
                'NAME': 'NAME',
                
                # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                '–ö–û–õ–ò–ß–ï–°–¢–í–û': 'QUANTITY',
                '–ö–û–õ-–í–û': 'QUANTITY',
                '–ö–û–õ–ò–ß–ï–°–¢–í–û_–ï–î–ò–ù–ò–¶': 'QUANTITY',
                '–û–ë–™–ï–ú': 'QUANTITY',
                'QUANTITY': 'QUANTITY',
                'QTY': 'QUANTITY',
                'AMOUNT': 'QUANTITY',
                'COUNT': 'QUANTITY',
                'NUMBER': 'QUANTITY',
                
                # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
                '–ï–î': 'UNIT',
                '–ï–î–ò–ù–ò–¶–ê': 'UNIT',
                '–ï–î–ò–ù–ò–¶–ê_–ò–ó–ú–ï–†–ï–ù–ò–Ø': 'UNIT',
                '–ï–î_–ò–ó–ú': 'UNIT',
                'UNIT': 'UNIT',
                'MEASURE': 'UNIT',
                
                # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —Ü–µ–Ω–∞
                '–¶–ï–ù–ê': 'PRICE',
                '–°–¢–û–ò–ú–û–°–¢–¨': 'PRICE',
                '–¶–ï–ù–ê_–ó–ê_–ï–î–ò–ù–ò–¶–£': 'PRICE',
                '–¶–ï–ù–ê_–ó–ê_–®–¢': 'PRICE',
                'PRICE': 'PRICE',
                'PRICE_PER_UNIT': 'PRICE',
                'UNIT_PRICE': 'PRICE',
                'COST': 'PRICE',
                
                # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - —Å—É–º–º–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏
                '–°–£–ú–ú–ê_–ü–û–ó–ò–¶–ò–ò': 'ITEM_TOTAL',
                '–°–¢–û–ò–ú–û–°–¢–¨_–ü–û–ó–ò–¶–ò–ò': 'ITEM_TOTAL',
                '–°–£–ú–ú–ê_–¢–û–í–ê–†–ê': 'ITEM_TOTAL',
                'ITEM_TOTAL': 'ITEM_TOTAL',
                'LINE_TOTAL': 'ITEM_TOTAL',
                'SUBTOTAL': 'ITEM_TOTAL',
                'ROW_TOTAL': 'ITEM_TOTAL',
                
                # –¢–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –ù–î–°
                '–ù–î–°': 'VAT',
                '–ù–ê–õ–û–ì': 'VAT',
                '–°–£–ú–ú–ê_–ù–î–°': 'VAT',
                '–°–¢–ê–í–ö–ê_–ù–î–°': 'VAT_RATE',
                'VAT': 'VAT',
                'VAT_AMOUNT': 'VAT',
                'TAX': 'VAT',
                'VAT_RATE': 'VAT_RATE',
                'TAX_RATE': 'VAT_RATE',
                
                # –û–±—â–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
                '–¢–ê–ë–õ–ò–¶–ê': 'TABLE',
                '–ü–û–ó–ò–¶–ò–ò': 'TABLE',
                '–¢–û–í–ê–†–´': 'TABLE',
                '–£–°–õ–£–ì–ò': 'TABLE',
                'TABLE': 'TABLE',
                'ITEMS': 'TABLE',
                'PRODUCTS': 'TABLE',
                'SERVICES': 'TABLE',
                'POSITIONS': 'TABLE',
                'LINE_ITEMS': 'TABLE',
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            if field_name in field_mapping:
                return field_mapping[field_name]
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            for key, value in field_mapping.items():
                if key in field_name or field_name in key:
                    return value
                    
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ–ª—è
            if len(field_name) > 20:
                field_name = field_name[:20]
                
            # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            import re
            field_name = re.sub(r'[^\w_]', '', field_name)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ–ª–µ –∏–ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return field_name if field_name else "ENTITY"
            
        except Exception as e:
            self._log(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–º–µ–Ω–∏ –ø–æ–ª—è: {str(e)}")
            return "ENTITY"

    def _process_table_item(self, item, words, labels, item_idx):
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç —Ç–∞–±–ª–∏—Ü—ã –∏ –º–∞—Ä–∫–∏—Ä—É–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ.
        
        Args:
            item (dict): –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ —Ç–∞–±–ª–∏—Ü—ã
            words (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ OCR
            labels (list): –°–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            item_idx (int): –ò–Ω–¥–µ–∫—Å —ç–ª–µ–º–µ–Ω—Ç–∞ —Ç–∞–±–ª–∏—Ü—ã
        """
        if not isinstance(item, dict):
            self._log(f"–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –≠–ª–µ–º–µ–Ω—Ç —Ç–∞–±–ª–∏—Ü—ã —Å –∏–Ω–¥–µ–∫—Å–æ–º {item_idx} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º")
            return
            
        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        self._log(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ —Ç–∞–±–ª–∏—Ü—ã #{item_idx+1}: {item}")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è —ç–ª–µ–º–µ–Ω—Ç–∞ —Ç–∞–±–ª–∏—Ü—ã
        name_fields = ['name', 'description', 'title', 'product', 'service', 'item', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '—Ç–æ–≤–∞—Ä', '—É—Å–ª—É–≥–∞', '—Ä–∞–±–æ—Ç–∞']
        quantity_fields = ['quantity', 'qty', 'count', 'number', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–∫–æ–ª-–≤–æ']
        unit_fields = ['unit', 'measure', '–µ–¥–∏–Ω–∏—Ü–∞', '–µ–¥_–∏–∑–º', '–µ–¥']
        price_fields = ['price', 'unit_price', 'cost', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞_–∑–∞_–µ–¥–∏–Ω–∏—Ü—É']
        total_fields = ['total', 'amount', 'sum', 'line_total', '—Å—É–º–º–∞', '–∏—Ç–æ–≥–æ']
        vat_fields = ['vat', 'tax', 'vat_amount', '–Ω–∞–ª', '–Ω–¥—Å']
        
        # –ü–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —ç–ª–µ–º–µ–Ω—Ç–µ —Ç–∞–±–ª–∏—Ü—ã
        field_groups = {
            'NAME': name_fields,
            'QUANTITY': quantity_fields,
            'UNIT': unit_fields,
            'PRICE': price_fields,
            'ITEM_TOTAL': total_fields,
            'VAT': vat_fields
        }
        
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –º–∞—Ä–∫–∏—Ä–æ–≤–∫—É —Ç–∞–±–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–æ–∫ —Ä—è–¥–æ–º —Å –Ω–∏–º–∏
        header_indices = []
        header_row_found = False
        
        # –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö —Ç–∞–±–ª–∏—Ü
        header_keywords = ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '—Ç–æ–≤–∞—Ä', '—É—Å–ª—É–≥–∞', '—Ä–∞–±–æ—Ç–∞', '–∫–æ–ª-–≤–æ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', 
                           '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Å—É–º–º–∞', '–∏—Ç–æ–≥–æ', '–µ–¥', '–Ω–¥—Å']
        
        # –ò—â–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        for i, word in enumerate(words):
            word_lower = word.lower().strip()
            if any(keyword in word_lower for keyword in header_keywords):
                header_indices.append(i)
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –ø–æ–¥—Ä—è–¥, —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                if len(header_indices) >= 2 and all(abs(header_indices[j] - header_indices[j-1]) <= 3 
                                                 for j in range(1, len(header_indices))):
                    header_row_found = True
                    break
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É –ø–æ–ª–µ–π
        for field_type, field_names in field_groups.items():
            value = None
            
            # –ò—â–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –≤ —ç–ª–µ–º–µ–Ω—Ç–µ —Ç–∞–±–ª–∏—Ü—ã
            for field_name in field_names:
                if field_name in item and item[field_name]:
                    value = item[field_name]
                    break
            
            if value is not None:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É
                value_str = str(value).strip().lower()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if len(value_str) < 2:
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫—É –¥–ª—è –ø–æ–ª—è —Ç–∞–±–ª–∏—Ü—ã
                table_field_type = f"TABLE_ITEM_{field_type}"
                
                # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
                marked_indices = self._mark_table_field(words, value_str, table_field_type, labels, item_idx, header_indices)
                
                # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ–ª–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∏ –º—ã —É—Å–ø–µ—à–Ω–æ –Ω–∞—à–ª–∏ –µ–≥–æ –≤ —Ç–µ–∫—Å—Ç–µ
                if field_type == 'NAME' and marked_indices:
                    # –ú–∞—Ä–∫–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
                    first_idx = min(marked_indices)
                    labels[first_idx] = f"B-TABLE_ROW"
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
                    self._log(f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª—è –ø–æ–ª—è '{field_type}': '{value_str}' (–∏–Ω–¥–µ–∫—Å—ã: {marked_indices})")

    def _mark_table_field(self, words, field_value, field_type, labels, item_idx, header_indices):
        """
        –ú–∞—Ä–∫–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ–ª—é —Ç–∞–±–ª–∏—Ü—ã.
        
        Args:
            words (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ OCR
            field_value (str): –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            field_type (str): –¢–∏–ø –ø–æ–ª—è –¥–ª—è –º–µ—Ç–æ–∫ IOB2
            labels (list): –°–ø–∏—Å–æ–∫ –º–µ—Ç–æ–∫ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            item_idx (int): –ò–Ω–¥–µ–∫—Å —ç–ª–µ–º–µ–Ω—Ç–∞ —Ç–∞–±–ª–∏—Ü—ã
            header_indices (list): –ò–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤, —è–≤–ª—è—é—â–∏—Ö—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —Ç–∞–±–ª–∏—Ü—ã
            
        Returns:
            list: –ò–Ω–¥–µ–∫—Å—ã —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è
        field_value = field_value.lower().strip()
        
        # –ò–Ω–¥–µ–∫—Å—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
        marked_indices = []
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞: —Å–Ω–∞—á–∞–ª–∞ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ —Ç–∞–±–ª–∏—Ü—ã, –ø–æ—Ç–æ–º –≤–µ–∑–¥–µ
        search_ranges = []
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü—ã, –Ω–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ—Å–ª–µ –Ω–∏—Ö
        if header_indices:
            max_header_idx = max(header_indices)
            start_idx = max_header_idx + 1
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 50 —Å–ª–æ–≤ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            end_idx = min(start_idx + 50, len(words))
            search_ranges.append((start_idx, end_idx))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_ranges.append((0, len(words)))
        
        # –ü–æ–∏—Å–∫ –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
        for start_idx, end_idx in search_ranges:
            # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            for i in range(start_idx, end_idx):
                word = words[i].lower().strip()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                if labels[i] != "O":
                    continue
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if word == field_value:
                    labels[i] = f"B-{field_type}"
                    marked_indices.append(i)
                    return marked_indices
                    
            # –ü–æ–∏—Å–∫ –ø–æ –≥—Ä—É–ø–ø–∞–º —Å–ª–æ–≤
            max_window_size = min(10, len(words) - start_idx)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞
            
            for window_size in range(min(len(field_value.split()), max_window_size), 0, -1):
                for i in range(start_idx, end_idx - window_size + 1):
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ —É–∂–µ —Ä–∞–∑–º–µ—á–µ–Ω–æ
                    if labels[i] != "O":
                        continue
                        
                    # –°–æ–∑–¥–∞–µ–º –æ–∫–Ω–æ –∏–∑ —Å–ª–æ–≤
                    window_words = [words[j].lower().strip() for j in range(i, i + window_size)]
                    window_text = " ".join(window_words)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
                    if self._is_matching_text(window_text, field_value, threshold=0.7):
                        # –ú–∞—Ä–∫–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –∫–∞–∫ B-{field_type}
                        labels[i] = f"B-{field_type}"
                        marked_indices.append(i)
                        
                        # –ú–∞—Ä–∫–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∫–∞–∫ I-{field_type}
                        for j in range(i + 1, i + window_size):
                            labels[j] = f"I-{field_type}"
                            marked_indices.append(j)
                            
                        return marked_indices
            
            # –ü–æ–∏—Å–∫ –ø–æ –Ω–∞–ª–∏—á–∏—é –æ–±—â–∏—Ö —á–∏—Å–µ–ª –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
            for i in range(start_idx, end_idx):
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
                if labels[i] != "O":
                    continue
                    
                word = words[i].lower().strip()
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                if len(word) < 2:
                    continue
                    
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–ª–∏ –Ω–∞–ª–∏—á–∏–µ –æ–±—â–∏—Ö —á–∏—Å–µ–ª
                if (word in field_value or 
                    field_value in word or 
                    self._has_common_numbers(word, field_value)):
                    
                    # –ú–∞—Ä–∫–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ
                    labels[i] = f"B-{field_type}"
                    marked_indices.append(i)
                    
                    # –ò—â–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –≤ —Å–æ—Å–µ–¥–Ω–∏—Ö —Å–ª–æ–≤–∞—Ö
                    j = i + 1
                    while j < end_idx and j < i + 5:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ–∏—Å–∫ 5 —Å–ª–µ–¥—É—é—â–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
                        if labels[j] != "O":
                            j += 1
                            continue
                            
                        next_word = words[j].lower().strip()
                        
                        if (next_word in field_value or 
                            field_value in next_word or 
                            self._has_common_numbers(next_word, field_value)):
                            
                            labels[j] = f"I-{field_type}"
                            marked_indices.append(j)
                            j += 1
                        else:
                            break
                    
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    if marked_indices:
                        return marked_indices
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        return marked_indices

    # ... (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥, –µ—Å–ª–∏ –µ—Å—Ç—å)

    # –ù–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    def set_callbacks(self, log_callback=None, progress_callback=None):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –¥–ª—è GUI"""
        self.log_callback = log_callback
        self.progress_callback = progress_callback
        
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.stop_requested = True
        self._log("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º-–∞—É—Ç –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        import threading
        def force_stop():
            import time
            time.sleep(5)  # –ñ–¥–µ–º 5 —Å–µ–∫—É–Ω–¥
            if self.stop_requested:
                self._log("üî¥ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ 5 —Å–µ–∫—É–Ω–¥")
                import os
                import sys
                # –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –∑–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å
                # os._exit(0)  # –ó–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–æ, —Ç–æ–ª—å–∫–æ –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        timer_thread = threading.Thread(target=force_stop, daemon=True)
        timer_thread.start()
        if self.log_callback:
            self.log_callback("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
            
    def _update_progress(self, progress: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        if self.progress_callback:
            self.progress_callback(progress)
            
    def prepare_dataset_for_donut_modern(self,
                                       source_folder: str,
                                       output_path: str,
                                       task_type: str = "document_parsing",
                                       annotation_method: str = "gemini",
                                       max_files: Optional[int] = None) -> Optional[str]:
        """
        –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Donut
        
        Args:
            source_folder: –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (document_parsing, document_vqa)
            annotation_method: –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
            
        Returns:
            str: –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        import traceback
        import sys
        import psutil
        import os
        
        try:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self._log("=" * 80)
            self._log("üöÄ –ù–ê–ß–ê–õ–û –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–¢–ê–°–ï–¢–ê - –†–ê–°–®–ò–†–ï–ù–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï")
            self._log("=" * 80)
            self._log(f"üîç –í–µ—Ä—Å–∏—è Python: {sys.version}")
            self._log(f"üîç –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"üîç PID –ø—Ä–æ—Ü–µ—Å—Å–∞: {os.getpid()}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
            try:
                process = psutil.Process()
                memory_info = process.memory_info()
                self._log(f"üîç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_info.rss / 1024 / 1024:.1f} MB")
            except Exception as e:
                self._log(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏: {e}")
            
            self._log("üç© –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è Donut...")
            self._log(f"üìÅ –ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {source_folder}")
            self._log(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
            self._log(f"üîß –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {annotation_method}")
            self._log(f"üìä –ú–∞–∫—Å. —Ñ–∞–π–ª–æ–≤: {max_files if max_files else '–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π'}")
            self._log(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {output_path}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if hasattr(self, 'stop_requested') and self.stop_requested:
                self._log("‚èπÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –º–µ—Ç–æ–¥–∞")
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if not source_folder or not os.path.exists(source_folder):
                raise ValueError(f"–ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {source_folder}")
            
            if not output_path:
                raise ValueError("–ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
                
            self._log("‚úÖ –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã")
            
            # –°–æ–∑–¥–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –µ—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
            if not os.path.isabs(output_path):
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                output_path = os.path.join(project_root, output_path)
                self._log(f"üìç –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {output_path}")
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            self._log("üìÇ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            from pathlib import Path
            dataset_dir = Path(output_path)
            dataset_dir.mkdir(parents=True, exist_ok=True)
            self._log(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_dir}")
            
            images_dir = dataset_dir / "images"
            images_dir.mkdir(exist_ok=True)
            self._log(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {images_dir}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
            from datetime import datetime
            dataset_info = {
                "created_at": datetime.now().isoformat(),
                "source_folder": os.path.abspath(source_folder),
                "model_type": "donut",
                "task_type": task_type,
                "annotation_method": annotation_method,
                "max_files": max_files,
                "total_files_processed": 0,
                "successful_files": 0,
                "failed_files": 0
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º info —Ñ–∞–π–ª
            info_path = dataset_dir / "dataset_info.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(dataset_info, f, indent=2, ensure_ascii=False)
            self._log(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {info_path}")
            
            # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã
            self._log("üîç –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            files = self._find_files_modern(source_folder, max_files)
            if not files:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                
            self._log(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
            for i, file_path in enumerate(files[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
                self._log(f"   {i+1}. {file_path.name}")
            if len(files) > 5:
                self._log(f"   ... –∏ –µ—â–µ {len(files) - 5} —Ñ–∞–π–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
            self._log("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤...")
            if annotation_method == "gemini":
                if not self.gemini_processor:
                    self._log("‚ö†Ô∏è Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ OCR")
                    annotation_method = "ocr"
                else:
                    self._log("‚úÖ Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
            
            if annotation_method == "ocr":
                if not self.ocr_processor:
                    self._log("‚ö†Ô∏è OCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                    annotation_method = "manual"
                else:
                    self._log("‚úÖ OCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
            self._log("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤...")
            annotations = []
            total_files = len(files)
            processed_files = 0
            failed_files = 0
            
            for i, file_path in enumerate(files):
                try:
                    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
                    self._log("=" * 60)
                    self._log(f"üîÑ –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–ê {i+1}/{total_files}")
                    self._log("=" * 60)
                    self._log(f"üìÑ –§–∞–π–ª: {file_path.name}")
                    self._log(f"üìç –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path}")
                    self._log(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    try:
                        process = psutil.Process()
                        memory_info = process.memory_info()
                        self._log(f"üîç –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π: {memory_info.rss / 1024 / 1024:.1f} MB")
                    except:
                        pass
                        
                    if hasattr(self, 'stop_requested') and self.stop_requested:
                        self._log("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
                        return None
                    
                    try:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        self._log(f"   üì∑ –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
                        self._log(f"   üì∑ –¢–∏–ø —Ñ–∞–π–ª–∞: {file_path.suffix}")
                        images = self._convert_to_images_modern(file_path)
                        self._log(f"   ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ª—É—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
                        
                        if not images:
                            self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞")
                            failed_files += 1
                            continue
                        
                        for j, image in enumerate(images):
                            self._log(f"   üñºÔ∏è –û–ë–†–ê–ë–û–¢–ö–ê –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø {j+1}/{len(images)}")
                            
                            if hasattr(self, 'stop_requested') and self.stop_requested:
                                self._log("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                                return None
                                
                            try:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                width, height = image.size
                                self._log(f"   üìê –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {width}x{height}")
                                self._log(f"   üìê –†–µ–∂–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.mode}")
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                image_name = f"{file_path.stem}_page_{j+1}.png"
                                image_path = images_dir / image_name
                                self._log(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {image_name}")
                                self._log(f"   üíæ –ü—É—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {image_path}")
                                
                                image.save(image_path)
                                self._log(f"   ‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω
                                if image_path.exists():
                                    file_size = image_path.stat().st_size
                                    self._log(f"   ‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω, —Ä–∞–∑–º–µ—Ä: {file_size / 1024:.1f} KB")
                                else:
                                    self._log(f"   ‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω!")
                                
                                # –°–æ–∑–¥–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è Donut
                                self._log(f"   üè∑Ô∏è –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–æ–º: {annotation_method}")
                                annotation = self._create_donut_annotation_modern(
                                    image,
                                    image_name,
                                    task_type,
                                    annotation_method
                                )
                                
                                if annotation:
                                    annotations.append(annotation)
                                    self._log(f"   ‚úÖ –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∞ (–≤—Å–µ–≥–æ: {len(annotations)})")
                                else:
                                    self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é")
                                    
                            except Exception as img_error:
                                self._log(f"   ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {j+1}: {str(img_error)}")
                                self._log(f"   üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {traceback.format_exc()}")
                                raise img_error
                        
                        processed_files += 1
                        self._log(f"‚úÖ –§–∞–π–ª {file_path.name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ü–û–õ–ù–û–°–¢–¨–Æ —É—Å–ø–µ—à–Ω–æ")
                        
                    except Exception as conv_error:
                        self._log(f"   ‚ùå –û–®–ò–ë–ö–ê –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {str(conv_error)}")
                        self._log(f"   üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {traceback.format_exc()}")
                        failed_files += 1
                        continue
                        
                except Exception as file_error:
                    failed_files += 1
                    error_msg = f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {str(file_error)}"
                    self._log(error_msg)
                    self._log(f"üìã –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏
                    try:
                        process = psutil.Process()
                        memory_info = process.memory_info()
                        self._log(f"üîç –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏: {memory_info.rss / 1024 / 1024:.1f} MB")
                    except:
                        pass
                    continue
                    
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress = int((i + 1) / total_files * 80)
                self._update_progress(progress)
                
            self._log(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
            self._log(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}")
            self._log(f"   ‚ùå –§–∞–π–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏: {failed_files}")
            self._log(f"   üìù –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(annotations)}")
                
            if not annotations:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            self._log("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
            annotations_file = dataset_dir / "annotations.json"
            with open(annotations_file, 'w', encoding='utf-8') as f:
                json.dump(annotations, f, indent=2, ensure_ascii=False)
            self._log(f"‚úÖ –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {annotations_file}")
                
            self._update_progress(90)
            
            # –°–æ–∑–¥–∞–µ–º HuggingFace Dataset –¥–ª—è Donut
            self._log("ü§ó –°–æ–∑–¥–∞–Ω–∏–µ HuggingFace Dataset...")
            dataset = self._create_donut_dataset_modern(dataset_dir, task_type)
            self._log("‚úÖ HuggingFace Dataset —Å–æ–∑–¥–∞–Ω")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            dataset_save_path = str(dataset_dir / "dataset_dict")
            dataset.save_to_disk(dataset_save_path)
            self._log(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {dataset_save_path}")
            
            self._update_progress(100)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            dataset_info.update({
                "total_files_processed": total_files,
                "successful_files": processed_files,
                "failed_files": failed_files,
                "total_annotations": len(annotations),
                "finished_at": datetime.now().isoformat()
            })
            
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(dataset_info, f, indent=2, ensure_ascii=False)
            self._log(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {info_path}")
            
            self._log(f"üéâ –î–∞—Ç–∞—Å–µ—Ç –¥–ª—è Donut –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            self._log(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            self._log(f"   üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}/{total_files}")
            self._log(f"   üìù –°–æ–∑–¥–∞–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(annotations)}")
            self._log(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_path}")
            
            return str(dataset_dir)
            
        except Exception as e:
            error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ Donut: {str(e)}"
            self._log(error_msg)
            import traceback
            full_traceback = traceback.format_exc()
            self._log(f"üîç –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
            for line in full_traceback.split('\n'):
                if line.strip():
                    self._log(f"   {line}")
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
            self._log("üîß –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            self._log(f"   Python –≤–µ—Ä—Å–∏—è: {sys.version}")
            self._log(f"   –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"   –î–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ –ø–∞–ø–∫–∞ –∏—Å—Ç–æ—á–Ω–∏–∫: {os.path.exists(source_folder) if source_folder else False}")
            self._log(f"   Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä: {'–î–æ—Å—Ç—É–ø–µ–Ω' if self.gemini_processor else '–ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}")
            self._log(f"   OCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä: {'–î–æ—Å—Ç—É–ø–µ–Ω' if self.ocr_processor else '–ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}")
            
            return None
    
    def prepare_donut_dataset(self, source_folder: str, output_folder: str, task_type: str = "document_parsing", 
                            annotation_method: str = "gemini", max_files: Optional[int] = None) -> Optional[str]:
        """
        –ê–ª–∏–∞—Å –¥–ª—è prepare_dataset_for_donut_modern –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        
        Args:
            source_folder: –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            output_folder: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (document_parsing, document_vqa)
            annotation_method: –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (gemini, ocr, manual)
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
            
        Returns:
            str: –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        self._log("üîÑ –í—ã–∑–≤–∞–Ω prepare_donut_dataset (–∞–ª–∏–∞—Å –¥–ª—è prepare_dataset_for_donut_modern)")
        return self.prepare_dataset_for_donut_modern(
            source_folder=source_folder,
            output_path=output_folder,
            task_type=task_type,
            annotation_method=annotation_method,
            max_files=max_files
        )
            
    def _find_files_modern(self, source_folder: str, max_files: Optional[int] = None) -> List:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ"""
        from pathlib import Path
        source_path = Path(source_folder)
        files = []
        
        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
        supported_image_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        supported_document_formats = {'.pdf'}
        all_formats = supported_image_formats | supported_document_formats
        
        for file_path in source_path.rglob("*"):
            if file_path.is_file() and file_path.suffix.lower() in all_formats:
                files.append(file_path)
                
                if max_files and len(files) >= max_files:
                    break
                    
        return sorted(files)
        
    def _convert_to_images_modern(self, file_path) -> List[Image.Image]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤:
        - –î–ª—è PDF —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–ª–æ–µ–º: –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é 
        - –î–ª—è PDF –±–µ–∑ —Ç–µ–∫—Å—Ç–∞: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR
        - –î–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞–∫ –µ—Å—Ç—å
        """
        import traceback
        import os
        
        images = []
        
        try:
            self._log(f"         [CONVERT] –ù–ê–ß–ê–õ–û –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞: {file_path.name}")
            self._log(f"         [CONVERT] –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path}")
            self._log(f"         [CONVERT] –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                self._log(f"         [CONVERT] –ü–∞–º—è—Ç—å –¥–æ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {memory_info.rss / 1024 / 1024:.1f} MB")
            except:
                pass
            
            supported_image_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
            
            if file_path.suffix.lower() in supported_image_formats:
                self._log(f"         [CONVERT] –§–∞–π–ª —É–∂–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {file_path.suffix}")
                try:
                    image = Image.open(file_path).convert('RGB')
                    images.append(image)
                    self._log(f"         [CONVERT] –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ: {image.size}")
                except Exception as img_error:
                    self._log(f"         [ERROR] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(img_error)}")
                    self._log(f"         [ERROR] –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
                    raise img_error
                
            elif file_path.suffix.lower() == '.pdf':
                self._log(f"         [CONVERT] üß† –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–∞...")
                
                # –°–Ω–∞—á–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º PDF –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è
                try:
                    analysis = self.pdf_analyzer.analyze_pdf(str(file_path))
                    self._log(f"         [ANALYSIS] {analysis['recommendation']}")
                    self._log(f"         [ANALYSIS] –ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {analysis['processing_method']}")
                    
                    if analysis['processing_method'] == 'text_extraction':
                        # PDF —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π
                        self._log(f"         [TEXT_EXTRACT] ‚úÖ –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é (–∫–∞—á–µ—Å—Ç–≤–æ: {analysis['text_quality']:.2f})")
                        
                        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        strategy = self.pdf_analyzer.get_processing_strategy(str(file_path))
                        
                        if strategy['text_blocks']:
                            self._log(f"         [TEXT_EXTRACT] –ù–∞–π–¥–µ–Ω–æ {len(strategy['text_blocks'])} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤")
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –±–ª–æ–∫–∞—Ö –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                            # –≠—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –≤–º–µ—Å—Ç–æ OCR –¥–∞–Ω–Ω—ã—Ö
                            self._pdf_text_blocks = strategy['text_blocks']
                            self._pdf_has_text_layer = True
                            
                            # –°–æ–∑–¥–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
                            # –ù–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –±–µ—Ä–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è
                            temp_images = self._convert_pdf_to_image_for_layout(file_path)
                            if temp_images:
                                images.extend(temp_images)
                                self._log(f"         [TEXT_EXTRACT] ‚úÖ PDF —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º —Å–ª–æ–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                            else:
                                # Fallback –∫ –æ–±—ã—á–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                self._log(f"         [TEXT_EXTRACT] ‚ö†Ô∏è Fallback –∫ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                                self._pdf_has_text_layer = False
                                images.extend(self._convert_pdf_to_images_ocr(file_path))
                        else:
                            self._log(f"         [TEXT_EXTRACT] ‚ö†Ô∏è –¢–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º OCR")
                            self._pdf_has_text_layer = False
                            images.extend(self._convert_pdf_to_images_ocr(file_path))
                    else:
                        # PDF —Ç—Ä–µ–±—É–µ—Ç OCR
                        self._log(f"         [OCR] üîç PDF —Ç—Ä–µ–±—É–µ—Ç OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                        self._pdf_has_text_layer = False
                        images.extend(self._convert_pdf_to_images_ocr(file_path))
                        
                except Exception as analysis_error:
                    self._log(f"         [ERROR] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ PDF: {analysis_error}")
                    self._log(f"         [FALLBACK] –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é")
                    self._pdf_has_text_layer = False
                    images.extend(self._convert_pdf_to_images_ocr(file_path))
            else:
                self._log(f"         [WARNING] –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path.suffix}")
                
            self._log(f"         [CONVERT] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
            return images
            
        except Exception as general_error:
            self._log(f"         [ERROR] –û–ë–©–ê–Ø –û–®–ò–ë–ö–ê –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {str(general_error)}")
            self._log(f"         [ERROR] –¢–∏–ø –æ—à–∏–±–∫–∏: {type(general_error).__name__}")
            self._log(f"         [ERROR] –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    self._log(f"         [ERROR]   {line}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–µ—Ä—à–∞—Ç—å –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å
            return []
    
    def _convert_pdf_to_image_for_layout(self, file_path) -> List[Image.Image]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è layout (–∫–æ–≥–¥–∞ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Å–ª–æ–π)"""
        try:
            from pdf2image import convert_from_path
            
            poppler_path = self.app_config.POPPLER_PATH if hasattr(self.app_config, 'POPPLER_PATH') else None
            
            if poppler_path and os.path.exists(poppler_path):
                pdf_images = convert_from_path(
                    str(file_path),
                    first_page=1,
                    last_page=1,  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è layout
                    dpi=150,  # –ú–µ–Ω—å—à–µ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ, —Ç–∞–∫ –∫–∞–∫ —Ç–µ–∫—Å—Ç —É–∂–µ –µ—Å—Ç—å
                    fmt='RGB',
                    poppler_path=poppler_path
                )
            else:
                pdf_images = convert_from_path(
                    str(file_path),
                    first_page=1,
                    last_page=1,
                    dpi=150,
                    fmt='RGB'
                )
            
            return pdf_images
            
        except Exception as e:
            self._log(f"         [ERROR] –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF –¥–ª—è layout: {e}")
            return []
    
    def _convert_pdf_to_images_ocr(self, file_path) -> List[Image.Image]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è OCR (–∫–æ–≥–¥–∞ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–ª–æ—è)"""
        images = []
        try:
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ Poppler
            self._log(f"         [CONVERT] –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—É—Ç–∏ –∫ Poppler...")
                    poppler_path = self.app_config.POPPLER_PATH if hasattr(self.app_config, 'POPPLER_PATH') else None
                    self._log(f"         [CONVERT] –ü—É—Ç—å –∫ Poppler: {poppler_path}")
                    
                    if poppler_path and os.path.exists(poppler_path):
                        self._log(f"         [CONVERT] Poppler –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤...")
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∏—Å–ø–æ–ª–Ω—è–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ Poppler
                        required_files = ['pdftoppm.exe', 'pdfinfo.exe']
                        for req_file in required_files:
                            full_path = os.path.join(poppler_path, req_file)
                            if os.path.exists(full_path):
                                self._log(f"         [CONVERT] –ù–∞–π–¥–µ–Ω: {req_file}")
                            else:
                                self._log(f"         [WARNING] –ù–ï –Ω–∞–π–¥–µ–Ω: {req_file} –≤ {full_path}")
                    else:
                        self._log(f"         [WARNING] –ü—É—Ç—å –∫ Poppler –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {poppler_path}")
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º pdf2image –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
                    self._log(f"         [CONVERT] –ò–º–ø–æ—Ä—Ç pdf2image...")
                    from pdf2image import convert_from_path
                    self._log(f"         [CONVERT] pdf2image –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                    
                    self._log(f"         [CONVERT] –í—ã–∑–æ–≤ convert_from_path...")
                    self._log(f"         [CONVERT] –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: file={file_path}, dpi=200, fmt=RGB")
                    
                    if poppler_path and os.path.exists(poppler_path):
                        self._log(f"         [CONVERT] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å —É–∫–∞–∑–∞–Ω–∏–µ–º Poppler...")
                        pdf_images = convert_from_path(
                            str(file_path),
                            dpi=200,  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –ª—É—á—à–µ–≥–æ OCR
                            fmt='RGB',
                            poppler_path=poppler_path
                        )
                    else:
                        self._log(f"         [CONVERT] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è Poppler (–∏—Å–ø–æ–ª—å–∑—É–µ–º PATH)...")
                        pdf_images = convert_from_path(
                            str(file_path),
                            dpi=200,  # –í—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –ª—É—á—à–µ–≥–æ OCR
                            fmt='RGB'
                        )
                    
                    self._log(f"         [CONVERT] convert_from_path –∑–∞–≤–µ—Ä—à–µ–Ω –£–°–ü–ï–®–ù–û!")
                    self._log(f"         [CONVERT] –ü–æ–ª—É—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(pdf_images)}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    for i, img in enumerate(pdf_images):
                        self._log(f"         [CONVERT] –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}: —Ä–∞–∑–º–µ—Ä {img.size}, —Ä–µ–∂–∏–º {img.mode}")
                        
                    images.extend(pdf_images)
                    self._log(f"         [OK] PDF –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {len(pdf_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
                    try:
                        import psutil
                        process = psutil.Process()
                        memory_info = process.memory_info()
                        self._log(f"         [CONVERT] –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {memory_info.rss / 1024 / 1024:.1f} MB")
                    except:
                        pass
                    
                except Exception as pdf_error:
                    self._log(f"         [ERROR] –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ PDF: {str(pdf_error)}")
                    self._log(f"         [ERROR] –¢–∏–ø –æ—à–∏–±–∫–∏: {type(pdf_error).__name__}")
                    self._log(f"         [ERROR] –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞:")
                    for line in traceback.format_exc().split('\n'):
                        if line.strip():
                            self._log(f"         [ERROR]   {line}")
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
                    self._log(f"         [ERROR] –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è:")
                    self._log(f"         [ERROR]   –§–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path.exists()}")
                    self._log(f"         [ERROR]   –§–∞–π–ª —á–∏—Ç–∞–µ–º: {os.access(file_path, os.R_OK)}")
                    self._log(f"         [ERROR]   –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {file_path.suffix}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    try:
                        import psutil
                        process = psutil.Process()
                        memory_info = process.memory_info()
                        self._log(f"         [ERROR] –ü–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ: {memory_info.rss / 1024 / 1024:.1f} MB")
                    except:
                        pass
                        
                    raise pdf_error
            else:
                self._log(f"         [WARNING] –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path.suffix}")
                
            self._log(f"         [CONVERT] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
            return images
            
        except Exception as general_error:
            self._log(f"         [ERROR] –û–ë–©–ê–Ø –û–®–ò–ë–ö–ê –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {str(general_error)}")
            self._log(f"         [ERROR] –¢–∏–ø –æ—à–∏–±–∫–∏: {type(general_error).__name__}")
            self._log(f"         [ERROR] –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    self._log(f"         [ERROR]   {line}")
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–µ, —á—Ç–æ–±—ã –Ω–µ –∑–∞–≤–µ—Ä—à–∞—Ç—å –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å
            return []
        
    def _create_donut_annotation_modern(self,
                                      image: Image.Image,
                                      image_name: str,
                                      task_type: str,
                                      annotation_method: str) -> Optional[Dict]:
        """–°–æ–∑–¥–∞–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è Donut"""
        try:
            self._log(f"      üè∑Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è {image_name}")
            self._log(f"      üìã –ú–µ—Ç–æ–¥: {annotation_method}, –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
            
            fields = {}
            
            if annotation_method == "gemini" and self.gemini_processor:
                self._log(f"      ü§ñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π —Å –ø–æ–º–æ—â—å—é Gemini...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                fields = self._extract_fields_with_gemini_modern(image)
                self._log(f"      ‚úÖ Gemini –∏–∑–≤–ª–µ–∫ –ø–æ–ª–µ–π: {len(fields)}")
                
            elif annotation_method == "ocr":
                self._log(f"      üëÅÔ∏è –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π —Å –ø–æ–º–æ—â—å—é OCR...")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º OCR
                fields = self._extract_fields_with_ocr_modern(image)
                self._log(f"      ‚úÖ OCR –∏–∑–≤–ª–µ–∫ –ø–æ–ª–µ–π: {len(fields)}")
                
            else:
                self._log(f"      üìù –°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏...")
                # –ë–∞–∑–æ–≤–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è
                fields = self._create_basic_annotation_modern()
                self._log(f"      ‚úÖ –°–æ–∑–¥–∞–Ω–∞ –±–∞–∑–æ–≤–∞—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å {len(fields)} –ø–æ–ª—è–º–∏")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—è
            if fields:
                self._log(f"      üìä –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—è:")
                for key, value in fields.items():
                    if value:
                        self._log(f"         {key}: {value}")
                    else:
                        self._log(f"         {key}: (–ø—É—Å—Ç–æ)")
            else:
                self._log(f"      ‚ö†Ô∏è –ü–æ–ª—è –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã")
                
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è Donut –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
            self._log(f"      üîÑ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Donut...")
            if task_type == "document_parsing":
                target_text = self._format_donut_parsing_target_modern(fields)
                self._log(f"      ‚úÖ –°–æ–∑–¥–∞–Ω parsing target: {target_text[:100]}...")
            elif task_type == "document_vqa":
                target_text = self._format_donut_vqa_target_modern(fields)
                self._log(f"      ‚úÖ –°–æ–∑–¥–∞–Ω VQA target: {target_text[:100]}...")
            else:
                target_text = json.dumps(fields, ensure_ascii=False)
                self._log(f"      ‚úÖ –°–æ–∑–¥–∞–Ω JSON target: {target_text[:100]}...")
                
            annotation = {
                'image': image_name,
                'text': target_text,
                'fields': fields,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                'task_type': task_type
            }
            
            self._log(f"      üéâ –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return annotation
            
        except Exception as e:
            error_msg = f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ Donut: {str(e)}"
            self._log(error_msg)
            import traceback
            self._log(f"      –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
            return None
            
    def _extract_fields_with_gemini_modern(self, image: Image.Image) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª—è —Å –ø–æ–º–æ—â—å—é Gemini"""
        import traceback
        import tempfile
        import os
        import sys
        
        try:
            self._log(f"         ü§ñ –ù–ê–ß–ò–ù–ê–ï–ú –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å Gemini...")
            self._log(f"         üìê –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.size}")
            self._log(f"         üìê –†–µ–∂–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.mode}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏
            try:
                import psutil
                process = psutil.Process()
                memory_info = process.memory_info()
                self._log(f"         üîç –ü–∞–º—è—Ç—å –¥–æ Gemini: {memory_info.rss / 1024 / 1024:.1f} MB")
            except:
                pass
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            if not self.gemini_processor:
                self._log(f"         ‚ùå Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                return self._create_basic_annotation_modern()
            else:
                self._log(f"         ‚úÖ Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω: {type(self.gemini_processor)}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            self._log(f"         üíæ –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞...")
            tmp_file_path = None
            
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp_file:
                tmp_file_path = tmp_file.name
                self._log(f"         üíæ –ü—É—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {tmp_file_path}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                self._log(f"         üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ñ–∞–π–ª...")
                image.save(tmp_file_path)
                self._log(f"         ‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size = os.path.getsize(tmp_file_path)
                self._log(f"         üìä –†–∞–∑–º–µ—Ä –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {file_size / 1024:.1f} KB")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gemini –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            self._log(f"         üîÑ –í–´–ó–û–í Gemini API...")
            self._log(f"         üîÑ –§–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {tmp_file_path}")
            
            try:
                result = self.gemini_processor.process(tmp_file_path)
                self._log(f"         ‚úÖ Gemini API –æ—Ç–≤–µ—Ç–∏–ª –£–°–ü–ï–®–ù–û")
                self._log(f"         üìä –¢–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}")
                
                if result:
                    self._log(f"         üìã –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø—É—Å—Ç–æ–π")
                else:
                    self._log(f"         ‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—É—Å—Ç–æ–π!")
                    
            except Exception as gemini_error:
                self._log(f"         ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê Gemini API: {str(gemini_error)}")
                self._log(f"         üîç –¢–∏–ø –æ—à–∏–±–∫–∏ Gemini: {type(gemini_error).__name__}")
                self._log(f"         üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ Gemini API: {traceback.format_exc()}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏
                try:
                    import psutil
                    process = psutil.Process()
                    memory_info = process.memory_info()
                    self._log(f"         üîç –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—à–∏–±–∫–∏ Gemini: {memory_info.rss / 1024 / 1024:.1f} MB")
                except:
                    pass
                
                raise gemini_error
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                try:
                    os.unlink(tmp_file.name)
                    self._log(f"         üóëÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")
                except PermissionError:
                    # –§–∞–π–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω, –ø–æ–ø—Ä–æ–±—É–µ–º –ø–æ–∑–∂–µ
                    self._log(f"         ‚ö†Ô∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω, –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω –ø–æ–∑–∂–µ")
                    import atexit
                    atexit.register(lambda: self._safe_delete_file(tmp_file.name))
                except Exception as e:
                    self._log(f"         ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {e}")
                
            if result:
                self._log(f"         üìä –†–µ–∑—É–ª—å—Ç–∞—Ç Gemini: {type(result)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞ –æ—Ç Gemini
                if 'fields' in result and isinstance(result['fields'], list):
                    # –§–æ—Ä–º–∞—Ç —Å –ø–æ–ª–µ–º 'fields' (—Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π)
                    fields_data = {}
                    for field in result['fields']:
                        if isinstance(field, dict) and 'field_name' in field and 'field_value' in field:
                            field_name = field['field_name'].lower().replace(' ', '_').replace('‚Ññ_—Å—á–µ—Ç–∞', 'invoice_number').replace('–¥–∞—Ç–∞_—Å—á–µ—Ç–∞', 'date').replace('–ø–æ—Å—Ç–∞–≤—â–∏–∫', 'company').replace('—Å—É–º–º–∞_—Å_–Ω–¥—Å', 'total_amount')
                            field_value = field['field_value']
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª—è —Å–æ –∑–Ω–∞—á–µ–Ω–∏–µ–º "N/A"
                            if field_value and field_value != "N/A":
                                fields_data[field_name] = field_value
                    
                    self._log(f"         ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –ø–æ–ª—è –∏–∑ 'fields': {len(fields_data)} –ø–æ–ª–µ–π")
                    self._log(f"         üìã –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø–æ–ª—è: {list(fields_data.keys())}")
                    return fields_data
                    
                elif isinstance(result, dict) and any(key in result for key in ['invoice_number', 'date', 'total_amount', 'company']):
                    # –ü—Ä—è–º–æ–π —Ñ–æ—Ä–º–∞—Ç —Å –∫–ª—é—á–∞–º–∏ –ø–æ–ª–µ–π
                    self._log(f"         ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –ø–æ–ª—è –Ω–∞–ø—Ä—è–º—É—é: {len(result)} –ø–æ–ª–µ–π")
                    return result
                    
                else:
                    self._log(f"         ‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                    self._log(f"         üìã –ö–ª—é—á–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {list(result.keys()) if isinstance(result, dict) else '–Ω–µ —Å–ª–æ–≤–∞—Ä—å'}")
                    # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —á—Ç–æ-—Ç–æ –ø–æ–ª–µ–∑–Ω–æ–µ
                    if isinstance(result, dict):
                        return result
                    else:
                        return self._create_basic_annotation_modern()
            else:
                self._log(f"         ‚ùå Gemini –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                    
        except Exception as e:
            error_msg = f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª–µ–π —Å Gemini: {str(e)}"
            self._log(error_msg)
            import traceback
            self._log(f"         –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {traceback.format_exc()}")
            
        self._log(f"         üîÑ –ü–µ—Ä–µ—Ö–æ–¥ –∫ –±–∞–∑–æ–≤–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏...")
        return self._create_basic_annotation_modern()
        
    def _safe_delete_file(self, file_path):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ —É–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        import time
        for attempt in range(3):
            try:
                if os.path.exists(file_path):
                    os.unlink(file_path)
                    self._log(f"         üóëÔ∏è –û—Ç–ª–æ–∂–µ–Ω–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —É—Å–ø–µ—à–Ω–æ: {file_path}")
                return
            except Exception as e:
                if attempt < 2:
                    time.sleep(0.5)  # –ñ–¥–µ–º –ø–æ–ª—Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π
                else:
                    self._log(f"         ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª –ø–æ—Å–ª–µ 3 –ø–æ–ø—ã—Ç–æ–∫: {file_path}")
        
    def _extract_fields_with_ocr_modern(self, image: Image.Image) -> Dict:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª—è —Å –ø–æ–º–æ—â—å—é OCR"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é Tesseract
            import pytesseract
            text = pytesseract.image_to_string(image, lang='rus+eng')
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            fields = {}
            
            # –ò—â–µ–º –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞
            invoice_patterns = [
                r'(?:—Å—á–µ—Ç|invoice|‚Ññ)\s*:?\s*([A-Za-z0-9\-/]+)',
                r'(?:–Ω–æ–º–µ—Ä|number)\s*:?\s*([A-Za-z0-9\-/]+)'
            ]
            
            for pattern in invoice_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    fields['invoice_number'] = match.group(1).strip()
                    break
                    
            # –ò—â–µ–º –¥–∞—Ç—É
            date_patterns = [
                r'(\d{1,2}[./]\d{1,2}[./]\d{2,4})',
                r'(\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{2,4})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    fields['date'] = match.group(1).strip()
                    break
                    
            # –ò—â–µ–º —Å—É–º–º—É
            amount_patterns = [
                r'(?:–∏—Ç–æ–≥–æ|total|—Å—É–º–º–∞|amount)\s*:?\s*([0-9\s,\.]+)',
                r'([0-9\s,\.]+)\s*(?:—Ä—É–±|rub|‚ÇΩ|usd|eur)'
            ]
            
            for pattern in amount_patterns:
                match = re.search(pattern, text, re.IGNORECASE)
                if match:
                    fields['total_amount'] = match.group(1).strip()
                    break
                    
            return fields
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {str(e)}")
            return self._create_basic_annotation_modern()
            
    def _create_basic_annotation_modern(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é"""
        return {
            'invoice_number': '',
            'date': '',
            'company': '',
            'total_amount': '',
            'currency': 'RUB'
        }
        
    def _format_donut_parsing_target_modern(self, fields: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ–ª—è –¥–ª—è Donut –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        formatted_fields = []
        
        field_mapping = {
            'invoice_number': 'invoice_id',
            'date': 'date',
            'company': 'nm',
            'total_amount': 'total_price'
        }
        
        for key, value in fields.items():
            if value and key in field_mapping:
                donut_key = field_mapping[key]
                formatted_fields.append(f"<s_{donut_key}>{value}</s_{donut_key}>")
                
        return "".join(formatted_fields)
        
    def _format_donut_vqa_target_modern(self, fields: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ–ª—è –¥–ª—è Donut –≤ —Ñ–æ—Ä–º–∞—Ç–µ VQA"""
        qa_pairs = []
        
        questions = {
            'invoice_number': '–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞?',
            'date': '–ö–∞–∫–∞—è –¥–∞—Ç–∞ —Å—á–µ—Ç–∞?',
            'company': '–ö–∞–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è?',
            'total_amount': '–ö–∞–∫–∞—è –æ–±—â–∞—è —Å—É–º–º–∞?'
        }
        
        for key, value in fields.items():
            if value and key in questions:
                question = questions[key]
                qa_pairs.append(f"<s_question>{question}</s_question><s_answer>{value}</s_answer>")
                
        return "".join(qa_pairs)
        
    def _create_donut_dataset_modern(self, dataset_dir, task_type: str) -> DatasetDict:
        """–°–æ–∑–¥–∞–µ—Ç HuggingFace Dataset –¥–ª—è Donut"""
        try:
            self._log(f"ü§ó –°–æ–∑–¥–∞–Ω–∏–µ HuggingFace Dataset...")
            from pathlib import Path
            import json
            from PIL import Image
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã
            try:
                from datasets import Dataset, DatasetDict, Features, Value, Image as DatasetImage
                self._log(f"‚úÖ –ò–º–ø–æ—Ä—Ç datasets —É—Å–ø–µ—à–µ–Ω")
            except ImportError as e:
                self._log(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ datasets: {e}")
                self._log(f"   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ datasets —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: pip install datasets")
                raise ImportError(f"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ datasets –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ dataset_dir —è–≤–ª—è–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç–æ–º Path
            if not isinstance(dataset_dir, Path):
                dataset_dir = Path(dataset_dir)
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏
            if not dataset_dir.exists():
                raise FileNotFoundError(f"–ü–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {dataset_dir}")
                
            self._log(f"üìÅ –†–∞–±–æ—á–∞—è –ø–∞–ø–∫–∞: {dataset_dir}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            annotations_file = dataset_dir / "annotations.json"
            self._log(f"üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–∑: {annotations_file}")
            
            if not annotations_file.exists():
                raise FileNotFoundError(f"–§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {annotations_file}")
            
            try:
                with open(annotations_file, 'r', encoding='utf-8') as f:
                    annotations = json.load(f)
                    
                if not isinstance(annotations, list):
                    raise ValueError(f"–ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º, –ø–æ–ª—É—á–µ–Ω: {type(annotations)}")
                    
                if not annotations:
                    raise ValueError("–°–ø–∏—Å–æ–∫ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –ø—É—Å—Ç–æ–π")
                    
                self._log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {len(annotations)}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –ø–µ—Ä–≤–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                first_ann = annotations[0]
                required_keys = ['image', 'text']
                missing_keys = [key for key in required_keys if key not in first_ann]
                if missing_keys:
                    raise ValueError(f"–í –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª—é—á–∏: {missing_keys}")
                    
                self._log(f"‚úÖ –§–æ—Ä–º–∞—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
                
            except json.JSONDecodeError as e:
                raise ValueError(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {e}")
            except Exception as e:
                raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {e}")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            self._log(f"üîÑ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Dataset...")
            data = []
            images_dir = dataset_dir / "images"
            
            if not images_dir.exists():
                raise FileNotFoundError(f"–ü–∞–ø–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {images_dir}")
            
            processed_annotations = 0
            failed_annotations = 0
            
            for i, ann in enumerate(annotations):
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                    if not isinstance(ann, dict):
                        self._log(f"   ‚ö†Ô∏è –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {i+1} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º: {type(ann)}")
                        failed_annotations += 1
                        continue
                        
                    if 'image' not in ann or 'text' not in ann:
                        self._log(f"   ‚ö†Ô∏è –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {i+1} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π: {list(ann.keys())}")
                        failed_annotations += 1
                        continue
                        
                    image_filename = ann['image']
                    if not image_filename:
                        self._log(f"   ‚ö†Ô∏è –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è {i+1} —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Å—Ç–æ–µ –∏–º—è —Ñ–∞–π–ª–∞")
                        failed_annotations += 1
                        continue
                        
                    image_path = images_dir / image_filename
                    if image_path.exists():
                        self._log(f"   üì∑ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}/{len(annotations)}: {image_filename}")
                        
                        # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        try:
                            with Image.open(image_path) as img:
                                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç–∏
                                image = img.convert('RGB')
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                if image.size[0] < 10 or image.size[1] < 10:
                                    self._log(f"   ‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–æ–µ: {image.size}")
                                    failed_annotations += 1
                                    continue
                                    
                                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–ª—è Dataset
                                image_copy = image.copy()
                                
                        except Exception as img_error:
                            self._log(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_filename}: {img_error}")
                            failed_annotations += 1
                            continue
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
                        text = ann['text']
                        if not isinstance(text, str):
                            self._log(f"   ‚ö†Ô∏è –¢–µ–∫—Å—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å—Ç—Ä–æ–∫–æ–π: {type(text)}")
                            text = str(text)
                            
                        data.append({
                            'image': image_copy,
                            'text': text
                        })
                        processed_annotations += 1
                        self._log(f"   ‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ dataset")
                    else:
                        self._log(f"   ‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {image_path}")
                        failed_annotations += 1
                        
                except Exception as e:
                    self._log(f"   ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ {i+1}: {e}")
                    import traceback
                    self._log(f"   –î–µ—Ç–∞–ª–∏: {traceback.format_exc()}")
                    failed_annotations += 1
                    continue
            
            self._log(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö:")
            self._log(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_annotations}")
            self._log(f"   ‚ùå –û—à–∏–±–æ–∫: {failed_annotations}")
            
            if not data:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è Dataset")
                
            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation
            self._log(f"üîÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation...")
            
            # –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –º–∏–Ω–∏–º—É–º 1 –ø—Ä–∏–º–µ—Ä –≤ –∫–∞–∂–¥–æ–º split
            if len(data) == 1:
                # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä, –¥—É–±–ª–∏—Ä—É–µ–º –µ–≥–æ –¥–ª—è –æ–±–æ–∏—Ö splits
                train_data = data
                val_data = data
                self._log(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–º–∞–ª—ã–π –¥–∞—Ç–∞—Å–µ—Ç):")
                self._log(f"   üéì Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ (–¥—É–±–ª–∏—Ä–æ–≤–∞–Ω)")
                self._log(f"   ‚úÖ Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            else:
                train_size = max(1, int(0.8 * len(data)))  # –ú–∏–Ω–∏–º—É–º 1 –ø—Ä–∏–º–µ—Ä
                train_data = data[:train_size]
                val_data = data[train_size:] if len(data) > train_size else [data[-1]]  # –ú–∏–Ω–∏–º—É–º 1 –¥–ª—è validation
                
                self._log(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
                self._log(f"   üéì Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                self._log(f"   ‚úÖ Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º Dataset
            self._log(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ Dataset –æ–±—ä–µ–∫—Ç–æ–≤...")
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è Dataset
                features = Features({
                    'image': DatasetImage(),
                    'text': Value('string')
                })
                self._log(f"‚úÖ –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞")
                
                # –°–æ–∑–¥–∞–µ–º train dataset —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
                self._log(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ train dataset –∏–∑ {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤...")
                if not train_data:
                    raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è train dataset")
                    
                train_dataset = Dataset.from_list(train_data, features=features)
                self._log(f"‚úÖ Train dataset —Å–æ–∑–¥–∞–Ω: {len(train_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                
                # –°–æ–∑–¥–∞–µ–º validation dataset —Å –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏
                self._log(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ validation dataset –∏–∑ {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤...")
                if not val_data:
                    raise ValueError("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è validation dataset")
                    
                val_dataset = Dataset.from_list(val_data, features=features)
                self._log(f"‚úÖ Validation dataset —Å–æ–∑–¥–∞–Ω: {len(val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                
                # –°–æ–∑–¥–∞–µ–º DatasetDict
                self._log(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ DatasetDict...")
                dataset_dict = DatasetDict({
                    'train': train_dataset,
                    'validation': val_dataset
                })
                self._log(f"‚úÖ DatasetDict —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
                self._log(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Dataset:")
                self._log(f"   üéì Train: {len(dataset_dict['train'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
                self._log(f"   ‚úÖ Validation: {len(dataset_dict['validation'])} –ø—Ä–∏–º–µ—Ä–æ–≤")
                
                return dataset_dict
                
            except Exception as e:
                self._log(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Dataset: {e}")
                import traceback
                self._log(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è Dataset:")
                for line in traceback.format_exc().split('\n'):
                    if line.strip():
                        self._log(f"   {line}")
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å HuggingFace Dataset: {e}")
                
        except Exception as e:
            error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è HuggingFace Dataset: {str(e)}"
            self._log(error_msg)
            import traceback
            self._log(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    self._log(f"   {line}")
            raise

    def prepare_dataset_for_layoutlm_modern(self,
                                           source_folder: str,
                                           output_path: str,
                                           task_type: str = "token_classification",
                                           annotation_method: str = "gemini",
                                           max_files: Optional[int] = None) -> Optional[str]:
        """
        –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LayoutLM
        
        Args:
            source_folder: –ü–∞–ø–∫–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (token_classification)
            annotation_method: –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (gemini, ocr, manual)
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤
            
        Returns:
            str: –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        import traceback
        import sys
        import psutil
        import os
        from pathlib import Path
        
        try:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self._log("=" * 80)
            self._log("üéØ –ù–ê–ß–ê–õ–û –ü–û–î–ì–û–¢–û–í–ö–ò –î–ê–¢–ê–°–ï–¢–ê –î–õ–Ø LAYOUTLM - –†–ê–°–®–ò–†–ï–ù–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï")
            self._log("=" * 80)
            self._log(f"üîç –í–µ—Ä—Å–∏—è Python: {sys.version}")
            self._log(f"üîç –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"üîç PID –ø—Ä–æ—Ü–µ—Å—Å–∞: {os.getpid()}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å
            try:
                process = psutil.Process()
                memory_info = process.memory_info()
                self._log(f"üîç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_info.rss / 1024 / 1024:.1f} MB")
            except Exception as e:
                self._log(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏: {e}")
            
            self._log("üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è LayoutLM...")
            self._log(f"üìÅ –ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {source_folder}")
            self._log(f"üéØ –¢–∏–ø –∑–∞–¥–∞—á–∏: {task_type}")
            self._log(f"üîß –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {annotation_method}")
            self._log(f"üìä –ú–∞–∫—Å. —Ñ–∞–π–ª–æ–≤: {max_files if max_files else '–±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π'}")
            self._log(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {output_path}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
            if hasattr(self, 'stop_requested') and self.stop_requested:
                self._log("‚èπÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–ª–∞–≥ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –≤ –Ω–∞—á–∞–ª–µ –º–µ—Ç–æ–¥–∞")
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            if not source_folder or not os.path.exists(source_folder):
                raise ValueError(f"–ò—Å—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {source_folder}")
            
            if not output_path:
                raise ValueError("–ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞")
                
            self._log("‚úÖ –í—Ö–æ–¥–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–æ–≤–µ—Ä–µ–Ω—ã")
            
            # –°–æ–∑–¥–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –µ—Å–ª–∏ –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π
            if not os.path.isabs(output_path):
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                output_path = os.path.join(project_root, output_path)
                self._log(f"üìç –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å: {output_path}")
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞—Ç–∞—Å–µ—Ç–∞
            self._log("üìÇ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è LayoutLM...")
            dataset_dir = Path(output_path)
            dataset_dir.mkdir(parents=True, exist_ok=True)
            self._log(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {dataset_dir}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ö–æ–¥–Ω–æ–π –ø–∞–ø–∫–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ
            from datetime import datetime
            dataset_info = {
                "created_at": datetime.now().isoformat(),
                "source_folder": os.path.abspath(source_folder),
                "model_type": "layoutlm", 
                "task_type": task_type,
                "annotation_method": annotation_method,
                "max_files": max_files,
                "total_files_processed": 0,
                "successful_files": 0,
                "failed_files": 0
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º info —Ñ–∞–π–ª
            info_path = dataset_dir / "dataset_info.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(dataset_info, f, indent=2, ensure_ascii=False)
            self._log(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {info_path}")
            
            # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã
            self._log("üîç –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
            files = self._find_files_modern(source_folder, max_files)
            if not files:
                raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                
            self._log(f"üìÑ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(files)}")
            for i, file_path in enumerate(files[:5]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 —Ñ–∞–π–ª–æ–≤
                self._log(f"   {i+1}. {file_path.name}")
            if len(files) > 5:
                self._log(f"   ... –∏ –µ—â–µ {len(files) - 5} —Ñ–∞–π–ª–æ–≤")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤
            self._log("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤...")
            if annotation_method == "gemini":
                if not self.gemini_processor:
                    self._log("‚ö†Ô∏è Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ OCR")
                    annotation_method = "ocr"
                else:
                    self._log("‚úÖ Gemini –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
            
            if annotation_method == "ocr":
                if not self.ocr_processor:
                    self._log("‚ö†Ô∏è OCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
                    annotation_method = "manual"
                else:
                    self._log("‚úÖ OCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –¥–ª—è LayoutLM
            self._log("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤ –¥–ª—è LayoutLM...")
            processed_records = []
            total_files = len(files)
            processed_files = 0
            failed_files = 0
            
            for i, file_path in enumerate(files):
                try:
                    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
                    self._log("=" * 60)
                    self._log(f"üîÑ –û–ë–†–ê–ë–û–¢–ö–ê –§–ê–ô–õ–ê {i+1}/{total_files} –î–õ–Ø LAYOUTLM")
                    self._log("=" * 60)
                    self._log(f"üìÑ –§–∞–π–ª: {file_path.name}")
                    self._log(f"üìç –ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path}")
                    self._log(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_path.stat().st_size / 1024:.1f} KB")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    try:
                        process = psutil.Process()
                        memory_info = process.memory_info()
                        self._log(f"üîç –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π: {memory_info.rss / 1024 / 1024:.1f} MB")
                    except:
                        pass
                        
                    if hasattr(self, 'stop_requested') and self.stop_requested:
                        self._log("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
                        return None
                    
                    try:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        self._log(f"   üì∑ –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
                        self._log(f"   üì∑ –¢–∏–ø —Ñ–∞–π–ª–∞: {file_path.suffix}")
                        images = self._convert_to_images_modern(file_path)
                        self._log(f"   ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–æ–ª—É—á–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
                        
                        if not images:
                            self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞")
                            failed_files += 1
                            continue
                        
                        for j, image in enumerate(images):
                            self._log(f"   üñºÔ∏è –û–ë–†–ê–ë–û–¢–ö–ê –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø {j+1}/{len(images)} –î–õ–Ø LAYOUTLM")
                            
                            if hasattr(self, 'stop_requested') and self.stop_requested:
                                self._log("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                                return None
                                
                            try:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                width, height = image.size
                                self._log(f"   üìê –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {width}x{height}")
                                self._log(f"   üìê –†–µ–∂–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.mode}")
                                
                                # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –¥–ª—è LayoutLM
                                image_name = f"{file_path.stem}_page_{j+1}.png"
                                self._log(f"   üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è LayoutLM: {image_name}")
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç
                                image_path = dataset_dir / image_name
                                image.save(image_path)
                                self._log(f"   üíæ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {image_path}")
                                
                                # –°–æ–∑–¥–∞–µ–º LayoutLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
                                layoutlm_record = self._create_layoutlm_annotation_modern(
                                    image,
                                    str(image_path),  # –ü–µ—Ä–µ–¥–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
                                    annotation_method
                                )
                                
                                if layoutlm_record:
                                    processed_records.append(layoutlm_record)
                                    self._log(f"   ‚úÖ LayoutLM –∑–∞–ø–∏—Å—å —Å–æ–∑–¥–∞–Ω–∞ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∞ (–≤—Å–µ–≥–æ: {len(processed_records)})")
                                else:
                                    self._log(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å LayoutLM –∑–∞–ø–∏—Å—å")
                                    
                            except Exception as img_error:
                                self._log(f"   ‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {j+1}: {str(img_error)}")
                                self._log(f"   üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {traceback.format_exc()}")
                                raise img_error
                        
                        processed_files += 1
                        self._log(f"‚úÖ –§–∞–π–ª {file_path.name} –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ü–û–õ–ù–û–°–¢–¨–Æ –¥–ª—è LayoutLM")
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                        progress = int((i + 1) / total_files * 100)
                        self._update_progress(progress)
                        
                    except Exception as conv_error:
                        self._log(f"   ‚ùå –û–®–ò–ë–ö–ê –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ {file_path.name}: {str(conv_error)}")
                        self._log(f"   üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {traceback.format_exc()}")
                        failed_files += 1
                        continue
                        
                except Exception as file_error:
                    self._log(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {str(file_error)}")
                    self._log(f"üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ —Ñ–∞–π–ª–∞: {traceback.format_exc()}")
                    failed_files += 1
                    continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._log("üìä –ò–¢–û–ì–ò –û–ë–†–ê–ë–û–¢–ö–ò –î–õ–Ø LAYOUTLM:")
            self._log(f"   üìÑ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
            self._log(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_files}")
            self._log(f"   ‚ùå –ù–µ—É–¥–∞—á–Ω–æ: {failed_files}")
            self._log(f"   üìù –°–æ–∑–¥–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(processed_records)}")
            
            if not processed_records:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –¥–ª—è LayoutLM")
            
            # –°–æ–∑–¥–∞–µ–º HuggingFace –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è LayoutLM
            self._log("ü§ó –°–æ–∑–¥–∞–Ω–∏–µ HuggingFace –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è LayoutLM...")
            layoutlm_dataset = self.create_full_dataset(processed_records)
            
            if not layoutlm_dataset:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å HuggingFace –¥–∞—Ç–∞—Å–µ—Ç")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset_dict_path = dataset_dir / "dataset_dict"
            dataset_dict_path.mkdir(exist_ok=True)
            self._log(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤: {dataset_dict_path}")
            
            self.split_and_save_dataset(layoutlm_dataset, str(dataset_dict_path))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            dataset_info.update({
                "total_files_processed": total_files,
                "successful_files": processed_files,
                "failed_files": failed_files,
                "total_records": len(processed_records),
                "finished_at": datetime.now().isoformat()
            })
            
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(dataset_info, f, indent=2, ensure_ascii=False)
            self._log(f"üìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∞: {info_path}")
            
            self._log("üéâ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê –î–õ–Ø LAYOUTLM –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
            self._log(f"üíæ –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {dataset_dict_path}")
            self._log("=" * 80)
            
            return str(dataset_dict_path)
            
        except Exception as e:
            self._log(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è LayoutLM: {str(e)}")
            self._log(f"üìã –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
            return None

    def _create_layoutlm_annotation_modern(self,
                                         image: Image.Image,
                                         image_path: str,
                                         annotation_method: str) -> Optional[Dict]:
        """
        –°–æ–∑–¥–∞–µ—Ç –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é –¥–ª—è LayoutLM –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        
        Args:
            image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            image_name: –ò–º—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            annotation_method: –ú–µ—Ç–æ–¥ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            
        Returns:
            Dict: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è LayoutLM (words, bboxes, labels, image_path)
        """
        try:
            image_name = os.path.basename(image_path)
            self._log(f"     üéØ –°–æ–∑–¥–∞–Ω–∏–µ LayoutLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è {image_name}")
            self._log(f"     üîß –ú–µ—Ç–æ–¥: {annotation_method}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ –¥–ª—è OCR
            temp_image_path = f"temp_{image_name}"
            image.save(temp_image_path)
            self._log(f"     üíæ –í—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {temp_image_path}")
            
            try:
                # –ü–æ–ª—É—á–∞–µ–º OCR –¥–∞–Ω–Ω—ã–µ (–¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –∏ —Å–ª–æ–≤)
                self._log(f"     üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ OCR –¥–∞–Ω–Ω—ã—Ö...")
                if self.ocr_processor:
                    ocr_result = self.ocr_processor.process_image(temp_image_path)
                    # OCR –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (text, words_list), –∞ –Ω–µ —Å–ª–æ–≤–∞—Ä—å
                    if not ocr_result or len(ocr_result) < 2 or not ocr_result[1]:
                        self._log(f"     ‚ö†Ô∏è OCR –Ω–µ –≤–µ—Ä–Ω—É–ª —Å–ª–æ–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é")
                        words = ["DOCUMENT"]
                        bboxes = [[0, 0, image.width, image.height]]
                    else:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                        ocr_text, ocr_words = ocr_result
                        self._log(f"     üìù OCR —Ç–µ–∫—Å—Ç: {len(ocr_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                        self._log(f"     üìù OCR —Å–ª–æ–≤–∞: {len(ocr_words)} —Å–ª–æ–≤")
                        
                        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –∏ bbox –∏–∑ OCR
                        words = []
                        bboxes = []
                        for word_data in ocr_words:
                            if word_data.get('text', '').strip():
                                words.append(word_data['text'])
                                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã x,y,width,height –≤ x1,y1,x2,y2
                                x, y, w, h = word_data['x'], word_data['y'], word_data['width'], word_data['height']
                                bbox = [x, y, x + w, y + h]
                                bboxes.append(bbox)
                        
                        self._log(f"     ‚úÖ OCR –ø–æ–ª—É—á–∏–ª {len(words)} —Å–ª–æ–≤ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏")
                        if len(words) > 0:
                            self._log(f"     üìã –ü–µ—Ä–≤—ã–µ 3 —Å–ª–æ–≤–∞: {words[:3]}")
                else:
                    self._log(f"     ‚ö†Ô∏è OCR –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é")
                    words = ["DOCUMENT"]
                    bboxes = [[0, 0, image.width, image.height]]
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ—Ç–æ–∫
                structured_data = {}
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º
                if self.intelligent_mode:
                    self._log(f"     üß† –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –†–ï–ñ–ò–ú: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –ø–æ–ª–µ–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
                    self._init_intelligent_extractor()
                    
                    if self.intelligent_extractor:
                        try:
                            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
                            extracted_data = self.intelligent_extractor.extract_all_data(temp_image_path)
                            
                            if extracted_data and extracted_data.get('fields'):
                                self._log(f"     ‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –∏–∑–≤–ª–µ–∫ {len(extracted_data['fields'])} –ø–æ–ª–µ–π")
                                
                                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç structured_data
                                for field in extracted_data['fields']:
                                    if hasattr(field, 'name') and hasattr(field, 'value'):
                                        structured_data[field.name] = field.value
                                    elif isinstance(field, dict):
                                        structured_data[field.get('name', 'unknown')] = field.get('value', '')
                                
                                self._log(f"     üìä –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: {list(structured_data.keys())}")
                            else:
                                self._log(f"     ‚ö†Ô∏è –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ, –ø–µ—Ä–µ—Ö–æ–¥ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–µ–∂–∏–º—É")
                                
                        except Exception as e:
                            self._log(f"     ‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
                            self._log(f"     üîÑ –ü–µ—Ä–µ—Ö–æ–¥ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ä–µ–∂–∏–º—É...")
                
                # –ï—Å–ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –∏–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç–æ–¥—ã
                if not structured_data:
                    if annotation_method == "gemini" and self.gemini_processor:
                        self._log(f"     ü§ñ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Gemini...")
                        structured_data = self._extract_fields_with_gemini_modern(image)
                        self._log(f"     ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π Gemini –≤–µ—Ä–Ω—É–ª –ø–æ–ª—è: {list(structured_data.keys())}")
                    elif annotation_method == "ocr" and self.ocr_processor:
                        self._log(f"     üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ OCR...")
                        structured_data = self._extract_fields_with_ocr_modern(image)
                        self._log(f"     ‚úÖ OCR –≤–µ—Ä–Ω—É–ª –ø–æ–ª—è: {list(structured_data.keys())}")
                    else:
                        self._log(f"     üìù –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø–æ–ª–µ–π...")
                        structured_data = self._create_basic_annotation_modern()
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –¥–ª—è LayoutLM (IOB2 —Ñ–æ—Ä–º–∞—Ç)
                self._log(f"     üè∑Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ IOB2...")
                labels = self._create_layoutlm_labels(words, structured_data)
                self._log(f"     ‚úÖ –°–æ–∑–¥–∞–Ω–æ –º–µ—Ç–æ–∫: {len(labels)}")
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox
                self._log(f"     üìê –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç...")
                normalized_bboxes = []
                for bbox in bboxes:
                    if len(bbox) == 4:
                        normalized_bbox = self.normalize_bbox(bbox, image.width, image.height)
                        normalized_bboxes.append(normalized_bbox)
                    else:
                        self._log(f"     ‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç bbox: {bbox}")
                        normalized_bboxes.append([0, 0, 0, 0])
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
                min_len = min(len(words), len(normalized_bboxes), len(labels))
                if min_len == 0:
                    self._log(f"     ‚ùå –ü—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ - –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∑–∞–ø–∏—Å—å")
                    return None
                
                if len(words) != len(normalized_bboxes) or len(words) != len(labels):
                    self._log(f"     ‚ö†Ô∏è –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–ª–∏–Ω: words={len(words)}, bboxes={len(normalized_bboxes)}, labels={len(labels)}")
                    self._log(f"     üîß –û–±—Ä–µ–∑–∞–µ–º –¥–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã: {min_len}")
                    words = words[:min_len]
                    normalized_bboxes = normalized_bboxes[:min_len]
                    labels = labels[:min_len]
                
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result = {
                    'image_path': image_path,       # –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
                    'words': words,                 # –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤
                    'bboxes': normalized_bboxes,   # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
                    'labels': labels               # IOB2 –º–µ—Ç–∫–∏
                }
                
                self._log(f"     ‚úÖ LayoutLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ:")
                self._log(f"        üìù –°–ª–æ–≤: {len(result['words'])}")
                self._log(f"        üìê Bbox: {len(result['bboxes'])}")
                self._log(f"        üè∑Ô∏è –ú–µ—Ç–æ–∫: {len(result['labels'])}")
                self._log(f"        üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {result['image_path']}")
                
                return result
                
            finally:
                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                try:
                    if os.path.exists(temp_image_path):
                        os.remove(temp_image_path)
                        self._log(f"     üóëÔ∏è –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω: {temp_image_path}")
                except:
                    pass
                    
        except Exception as e:
            self._log(f"     ‚ùå –û–®–ò–ë–ö–ê —Å–æ–∑–¥–∞–Ω–∏—è LayoutLM –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏: {str(e)}")
            import traceback
            self._log(f"     üìã –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞: {traceback.format_exc()}")
            return None

    def _create_layoutlm_labels(self, words: List[str], structured_data: Dict) -> List[str]:
        """
        –°–æ–∑–¥–∞–µ—Ç IOB2 –º–µ—Ç–∫–∏ –¥–ª—è LayoutLM –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            words: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –∏–∑ OCR
            structured_data: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–æ–ª—è —Å—á–µ—Ç–∞)
            
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ IOB2 –º–µ—Ç–æ–∫
        """
        try:
            self._log(f"       üè∑Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ IOB2 –º–µ—Ç–æ–∫ –¥–ª—è {len(words)} —Å–ª–æ–≤")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç–∫–∏ –∫–∞–∫ "O" (Outside)
            labels = ["O"] * len(words)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –ø–æ–ª–µ–π —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –º–∞–ø–ø–∏–Ω–≥–æ–º
            field_types = {
                'company': 'COMPANY',
                '–ø–æ—Å—Ç–∞–≤—â–∏–∫': 'COMPANY',
                'date': 'DATE',
                '–¥–∞—Ç–∞_—Å—á–µ—Ç–∞': 'DATE', 
                'invoice_number': 'INVOICE_NUMBER',
                '‚Ññ_—Å—á–µ—Ç–∞': 'INVOICE_NUMBER',
                'total_amount': 'AMOUNT',
                '—Å—É–º–º–∞_—Å_–Ω–¥—Å': 'AMOUNT',
                'amount': 'AMOUNT',
                'total': 'AMOUNT',
                '–≤–∞–ª—é—Ç–∞': 'CURRENCY',
                'currency': 'CURRENCY',
                '—Ç–æ–≤–∞—Ä—ã': 'ITEMS',
                '–∫–∞—Ç–µ–≥–æ—Ä–∏—è': 'CATEGORY',
                '–∏–Ω–Ω': 'TAX_ID',
                '–∫–ø–ø': 'TAX_CODE'
            }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥–æ–µ –ø–æ–ª–µ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            for field_name, field_value in structured_data.items():
                if not field_value or not isinstance(field_value, str):
                    continue
                    
                field_value = str(field_value).strip()
                if not field_value:
                    continue
                    
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–µ—Ç–∫–∏
                label_type = field_types.get(field_name.lower(), field_name.upper())
                
                self._log(f"       üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—è '{field_name}': '{field_value}' -> {label_type}")
                
                # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞
                matched_indices = self._find_matching_word_indices(words, field_value)
                
                if matched_indices:
                    self._log(f"       ‚úÖ –ù–∞–π–¥–µ–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ö: {matched_indices}")
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º IOB2 —Ä–∞–∑–º–µ—Ç–∫—É
                    for i, word_idx in enumerate(matched_indices):
                        if i == 0:
                            labels[word_idx] = f"B-{label_type}"  # Beginning
                        else:
                            labels[word_idx] = f"I-{label_type}"  # Inside
                else:
                    self._log(f"       ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–ª—è –ø–æ–ª—è '{field_name}'")
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–∫
            label_counts = {}
            for label in labels:
                label_counts[label] = label_counts.get(label, 0) + 1
            
            self._log(f"       üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–æ–∫: {label_counts}")
            
            return labels
            
        except Exception as e:
            self._log(f"       ‚ùå –û–®–ò–ë–ö–ê —Å–æ–∑–¥–∞–Ω–∏—è IOB2 –º–µ—Ç–æ–∫: {str(e)}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∫–∏
            return ["O"] * len(words)

    def _find_matching_word_indices(self, words: List[str], field_value: str) -> List[int]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –∏–Ω–¥–µ–∫—Å—ã —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –∑–Ω–∞—á–µ–Ω–∏—é –ø–æ–ª—è
        
        Args:
            words: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤
            field_value: –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            
        Returns:
            List[int]: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å–ª–æ–≤
        """
        try:
            field_value = field_value.strip().lower()
            field_words = field_value.split()
            
            if not field_words:
                return []
            
            matched_indices = []
            
            # –ü—Ä–æ–±—É–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
            for start_idx in range(len(words) - len(field_words) + 1):
                match = True
                for i, field_word in enumerate(field_words):
                    word_idx = start_idx + i
                    if word_idx >= len(words):
                        match = False
                        break
                    
                    word = words[word_idx].lower().strip()
                    if not self._is_similar_word(word, field_word):
                        match = False
                        break
                
                if match:
                    matched_indices = list(range(start_idx, start_idx + len(field_words)))
                    break
            
            # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if not matched_indices:
                for i, word in enumerate(words):
                    word_clean = word.lower().strip()
                    for field_word in field_words:
                        if self._is_similar_word(word_clean, field_word):
                            if i not in matched_indices:
                                matched_indices.append(i)
            
            return sorted(matched_indices)
            
        except Exception as e:
            self._log(f"         ‚ùå –û–®–ò–ë–ö–ê –ø–æ–∏—Å–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: {str(e)}")
            return []

    def _is_similar_word(self, word1: str, word2: str, threshold: float = 0.7) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Å–ª–æ–≤
        
        Args:
            word1: –ü–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
            word2: –í—Ç–æ—Ä–æ–µ —Å–ª–æ–≤–æ
            threshold: –ü–æ—Ä–æ–≥ –ø–æ—Ö–æ–∂–µ—Å—Ç–∏
            
        Returns:
            bool: True –µ—Å–ª–∏ —Å–ª–æ–≤–∞ –ø–æ—Ö–æ–∂–∏
        """
        try:
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if word1 == word2:
                return True
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
            if word1 in word2 or word2 in word1:
                return True
            
            # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ - –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
            if len(word1) <= 3 or len(word2) <= 3:
                return word1 == word2
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = calculate_text_similarity(word1, word2)
            return similarity >= threshold
            
        except Exception:
            return False

    def get_training_prompt(self, task_type: str = "layoutlm") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–ª–µ–π —Ç–∞–±–ª–∏—Ü—ã
        
        Args:
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ ("layoutlm", "donut", "gemini")
            
        Returns:
            str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        """
        if not self.field_manager:
            # Fallback –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ FieldManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            return self._get_fallback_training_prompt(task_type)
        
        try:
            enabled_fields = self.field_manager.get_enabled_fields()
            
            if not enabled_fields:
                self._log("‚ö†Ô∏è –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π –≤ FieldManager, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –ø—Ä–æ–º–ø—Ç")
                return self._get_fallback_training_prompt(task_type)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π
            if task_type == "gemini":
                prompt = self._generate_gemini_training_prompt(enabled_fields)
            elif task_type == "layoutlm":
                prompt = self._generate_layoutlm_training_prompt(enabled_fields)
            elif task_type == "donut":
                prompt = self._generate_donut_training_prompt(enabled_fields)
            else:
                prompt = self._generate_generic_training_prompt(enabled_fields)
            
            self._log(f"üéØ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –ø—Ä–æ–º–ø—Ç –¥–ª—è {task_type} –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(enabled_fields)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π")
            self._log(f"üìè –î–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return prompt
            
        except Exception as e:
            self._log(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–º–ø—Ç–∞: {e}")
            return self._get_fallback_training_prompt(task_type)

    def _generate_gemini_training_prompt(self, enabled_fields) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Gemini –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π"""
        prompt_parts = [
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:",
            "",
            "–ü–û–õ–Ø –î–õ–Ø –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞):"
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–ª—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –ø–æ–∑–∏—Ü–∏–∏
        sorted_fields = sorted(enabled_fields, key=lambda f: (f.priority, f.position))
        
        for field in sorted_fields:
            # –°–æ–∑–¥–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ–ª—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            keywords_str = ", ".join(field.gemini_keywords[:3])
            required_marker = " [–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ï]" if field.required else ""
            
            prompt_parts.append(
                f"- {field.gemini_keywords[0]}: {field.description}{required_marker}"
            )
            prompt_parts.append(f"  –í–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞: {keywords_str}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –¥–∞–Ω–Ω—ã—Ö
            if field.data_type == "date":
                prompt_parts.append("  –§–æ—Ä–º–∞—Ç: –¥–∞—Ç–∞ –≤ –≤–∏–¥–µ DD.MM.YYYY –∏–ª–∏ —Ç–µ–∫—Å—Ç–æ–º")
            elif field.data_type == "currency":
                prompt_parts.append("  –§–æ—Ä–º–∞—Ç: —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –≤–∞–ª—é—Ç–æ–π")
            elif field.data_type == "number":
                prompt_parts.append("  –§–æ—Ä–º–∞—Ç: —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ")
            
            prompt_parts.append("")
        
        prompt_parts.extend([
            "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:",
            "–í–æ–∑–≤—Ä–∞—â–∞–π –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.",
            "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤—ã—à–µ (–ø–µ—Ä–≤–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ).",
            "–ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–π \"N/A\".",
            "–ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏–∫–∞–∫–∏—Ö –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON.",
            "",
            "–ü–†–ò–û–†–ò–¢–ï–¢:",
            "–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–ø–æ–ª–Ω–µ–Ω—ã –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ.",
            "–ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∑–Ω–∞—á–µ–Ω–∏—è, –≤—ã–±–∏—Ä–∞–π –Ω–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π."
        ])
        
        return "\n".join(prompt_parts)

    def _generate_layoutlm_training_prompt(self, enabled_fields) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è LayoutLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π"""
        # LayoutLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—Ç –∂–µ –ø—Ä–æ–º–ø—Ç —á—Ç–æ –∏ Gemini, –Ω–æ —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        base_prompt = self._generate_gemini_training_prompt(enabled_fields)
        
        layoutlm_addition = [
            "",
            "–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û –î–õ–Ø LAYOUTLM:",
            "–£—á–∏—Ç—ã–≤–∞–π –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ.",
            "–°–≤—è–∑—ã–≤–∞–π –ª–æ–≥–∏—á–µ—Å–∫–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–ª—è –ø–æ –∏—Ö —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—é.",
            f"–¶–µ–ª–µ–≤—ã–µ –ª–µ–π–±–ª—ã –¥–ª—è —Ä–∞–∑–º–µ—Ç–∫–∏: {', '.join(self._get_layoutlm_labels_from_fields(enabled_fields))}"
        ]
        
        return base_prompt + "\n".join(layoutlm_addition)

    def _generate_donut_training_prompt(self, enabled_fields) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Donut –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π"""
        prompt_parts = [
            "–ò–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞.",
            "–°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–ª—è—Ö:",
            ""
        ]
        
        # –î–ª—è Donut –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –∫–æ–º–ø–∞–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        sorted_fields = sorted(enabled_fields, key=lambda f: (f.priority, f.position))
        
        for field in sorted_fields:
            if field.required:
                prompt_parts.append(f"‚Ä¢ {field.gemini_keywords[0]}: {field.description} [–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û]")
            else:
                prompt_parts.append(f"‚Ä¢ {field.gemini_keywords[0]}: {field.description}")
        
        prompt_parts.extend([
            "",
            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.",
            "–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –ø–æ–ª—è –∑–∞–ø–æ–ª–Ω—è–π \"N/A\"."
        ])
        
        return "\n".join(prompt_parts)

    def _generate_generic_training_prompt(self, enabled_fields) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π"""
        return self._generate_gemini_training_prompt(enabled_fields)

    def _get_layoutlm_labels_from_fields(self, enabled_fields) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç LayoutLM –ª–µ–π–±–ª—ã –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π"""
        labels = []
        for field in enabled_fields:
            labels.extend(field.layoutlm_labels)
        return list(set(labels))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

    def _get_fallback_training_prompt(self, task_type: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ FieldManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        fallback_prompts = {
            "gemini": """
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—á–µ—Ç–∞ –∏–ª–∏ —Ñ–∞–∫—Ç—É—Ä—ã –∏ –∏–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:

- –ü–æ—Å—Ç–∞–≤—â–∏–∫: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏-–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
- ‚Ññ –°—á–µ—Ç–∞: –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞/–∏–Ω–≤–æ–π—Å–∞
- –î–∞—Ç–∞ —Å—á–µ—Ç–∞: –¥–∞—Ç–∞ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—á–µ—Ç–∞
- –ö–∞—Ç–µ–≥–æ—Ä–∏—è: —Ç–∏–ø —Ç–æ–≤–∞—Ä–æ–≤/—É—Å–ª—É–≥
- –¢–æ–≤–∞—Ä—ã: –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤/—É—Å–ª—É–≥
- –°—É–º–º–∞ –±–µ–∑ –ù–î–°: —Å—É–º–º–∞ –±–µ–∑ –Ω–∞–ª–æ–≥–∞
- –ù–î–° %: —Å—Ç–∞–≤–∫–∞ –ù–î–° –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
- –°—É–º–º–∞ —Å –ù–î–°: –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞ —Å –Ω–∞–ª–æ–≥–æ–º
- –í–∞–ª—é—Ç–∞: –≤–∞–ª—é—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
- –ò–ù–ù: –ò–ù–ù –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
- –ö–ü–ü: –ö–ü–ü –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞
- –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è

–í–ê–ñ–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–π –æ—Ç–≤–µ—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON. –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π "N/A".
            """.strip(),
            
            "layoutlm": """
–ò–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —É—á–∏—Ç—ã–≤–∞—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —ç–ª–µ–º–µ–Ω—Ç–æ–≤.
–°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø–æ–ª—è—Ö: –ø–æ—Å—Ç–∞–≤—â–∏–∫, –Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞, –¥–∞—Ç–∞, —Å—É–º–º–∞.
–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: JSON —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.
            """.strip(),
            
            "donut": """
–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è: –ø–æ—Å—Ç–∞–≤—â–∏–∫, –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞, –¥–∞—Ç–∞, –∏—Ç–æ–≥–æ–≤–∞—è —Å—É–º–º–∞.
–û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.
            """.strip()
        }
        
        return fallback_prompts.get(task_type, fallback_prompts["gemini"])

    def get_entity_types_from_fields(self) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π
        
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è IOB2 —Ä–∞–∑–º–µ—Ç–∫–∏
        """
        if not self.field_manager:
            # Fallback —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
            return [
                "INVOICE_NUMBER", "DATE", "TOTAL_AMOUNT", "VENDOR", "CUSTOMER", 
                "INN", "KPP", "DESCRIPTION", "CATEGORY", "CURRENCY"
            ]
        
        try:
            enabled_fields = self.field_manager.get_enabled_fields()
            entity_types = []
            
            for field in enabled_fields:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º LayoutLM –ª–µ–π–±–ª—ã –∫–∞–∫ —Ç–∏–ø—ã —Å—É—â–Ω–æ—Å—Ç–µ–π
                entity_types.extend(field.layoutlm_labels)
            
            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–π —Ç–∏–ø "O" (Outside)
            unique_types = list(set(entity_types))
            if "O" not in unique_types:
                unique_types.append("O")
                
            self._log(f"üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(unique_types)} —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª–µ–π")
            return unique_types
            
        except Exception as e:
            self._log(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∏–ø–æ–≤ —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
            return ["INVOICE_NUMBER", "DATE", "TOTAL_AMOUNT", "VENDOR", "O"]

    def _save_dataset_metadata(self, dataset_folder: str, metadata: Dict):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤–∫–ª—é—á–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–ª—è—Ö –∏ –ø—Ä–æ–º–ø—Ç–∞—Ö
        
        Args:
            dataset_folder: –ü–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            metadata: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            import json
            from datetime import datetime
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            metadata['created_at'] = datetime.now().isoformat()
            metadata['version'] = '1.1'  # –í–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π FieldManager
            
            # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            metadata_path = os.path.join(dataset_folder, 'dataset_metadata.json')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self._log(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")

    def load_dataset_metadata(self, dataset_folder: str) -> Optional[Dict]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            dataset_folder: –ü–∞–ø–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            Optional[Dict]: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            import json
            
            metadata_path = os.path.join(dataset_folder, 'dataset_metadata.json')
            
            if not os.path.exists(metadata_path):
                return None
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            self._log(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {metadata_path}")
            return metadata
            
        except Exception as e:
            self._log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return None