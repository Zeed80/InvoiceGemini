"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Donut
–ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö
"""

import os
import json
import torch
import logging
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import numpy as np
from collections import defaultdict
from datetime import datetime
import time

from transformers import DonutProcessor, VisionEncoderDecoderModel
from PIL import Image
import pandas as pd

logger = logging.getLogger(__name__)


class DonutModelTester:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ Donut"""
    
    def __init__(self, model_path: str, device: str = None):
        """
        Args:
            model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
            device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (cuda/cpu)
        """
        self.model_path = model_path
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        self.processor = None
        self.model = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.test_results = {
            'total_documents': 0,
            'perfect_documents': 0,
            'field_metrics': defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0}),
            'processing_times': [],
            'errors': []
        }
        
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"""
        logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}")
        
        try:
            self.processor = DonutProcessor.from_pretrained(self.model_path)
            self.model = VisionEncoderDecoderModel.from_pretrained(self.model_path)
            self.model.to(self.device)
            self.model.eval()
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
            metadata_path = os.path.join(self.model_path, "training_metadata.json")
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                logger.info(f"üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:")
                logger.info(f"   –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {metadata.get('base_model', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                logger.info(f"   –î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: {metadata.get('created_at', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                logger.info(f"   –¶–µ–ª–µ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {metadata.get('target_accuracy', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
            
    def extract_fields_from_image(self, image_path: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª—è –∏–∑ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            
            # –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è
            start_time = time.time()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            pixel_values = self.processor(image, return_tensors="pt").pixel_values.to(self.device)
            
            # Task prompt
            task_prompt = "<s_docvqa><s_question>Extract all fields from the document</s_question><s_answer>"
            decoder_input_ids = self.processor.tokenizer(
                task_prompt, 
                add_special_tokens=False, 
                return_tensors="pt"
            ).input_ids.to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            with torch.no_grad():
                outputs = self.model.generate(
                    pixel_values,
                    decoder_input_ids=decoder_input_ids,
                    max_length=768,
                    num_beams=4,
                    temperature=0.1,
                    do_sample=False,
                    pad_token_id=self.processor.tokenizer.pad_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            prediction = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]
            prediction = prediction.replace(task_prompt, "").strip()
            
            # –ó–∞–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è
            processing_time = time.time() - start_time
            self.test_results['processing_times'].append(processing_time)
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            fields = self._parse_donut_output(prediction)
            
            return fields
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
            self.test_results['errors'].append({
                'file': image_path,
                'error': str(e)
            })
            return {}
            
    def _parse_donut_output(self, text: str) -> Dict[str, str]:
        """–ü–∞—Ä—Å–∏—Ç –≤—ã—Ö–æ–¥ Donut"""
        fields = {}
        
        # JSON –ø–∞—Ä—Å–∏–Ω–≥
        try:
            if text.strip().startswith('{'):
                return json.loads(text)
        except:
            pass
            
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–≥–æ–≤
        import re
        pattern = r'<s_([^>]+)>([^<]+)</s_\1>'
        matches = re.findall(pattern, text)
        
        for field_name, value in matches:
            fields[field_name] = value.strip()
            
        return fields
        
    def test_on_dataset(self, test_data_path: str, ground_truth_path: str = None):
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ
        
        Args:
            test_data_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
            ground_truth_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å ground truth (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        logger.info(f"üß™ –ù–∞—á–∏–Ω–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ: {test_data_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
        test_files = []
        for ext in ['.png', '.jpg', '.jpeg', '.pdf']:
            test_files.extend(Path(test_data_path).glob(f'*{ext}'))
            
        logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(test_files)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º ground truth –µ—Å–ª–∏ –µ—Å—Ç—å
        ground_truth = {}
        if ground_truth_path and os.path.exists(ground_truth_path):
            with open(ground_truth_path, 'r', encoding='utf-8') as f:
                ground_truth = json.load(f)
            logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω ground truth –¥–ª—è {len(ground_truth)} —Ñ–∞–π–ª–æ–≤")
            
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
        results = []
        
        for i, file_path in enumerate(test_files):
            logger.info(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ {i+1}/{len(test_files)}: {file_path.name}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª—è
            predicted_fields = self.extract_fields_from_image(str(file_path))
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å ground truth –µ—Å–ª–∏ –µ—Å—Ç—å
            if file_path.name in ground_truth:
                true_fields = ground_truth[file_path.name]
                accuracy = self._calculate_accuracy(predicted_fields, true_fields)
                
                results.append({
                    'file': file_path.name,
                    'predicted': predicted_fields,
                    'ground_truth': true_fields,
                    'accuracy': accuracy
                })
                
                logger.info(f"   ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy:.1%}")
            else:
                results.append({
                    'file': file_path.name,
                    'predicted': predicted_fields,
                    'ground_truth': None,
                    'accuracy': None
                })
                
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self._calculate_overall_metrics(results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._save_test_results(results)
        
        return results
        
    def _calculate_accuracy(self, predicted: Dict, ground_truth: Dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not ground_truth:
            return 0.0
            
        self.test_results['total_documents'] += 1
        
        all_fields = set(predicted.keys()) | set(ground_truth.keys())
        correct_fields = 0
        total_fields = len(all_fields)
        
        document_perfect = True
        
        for field in all_fields:
            pred_value = predicted.get(field, "").strip().lower()
            true_value = ground_truth.get(field, "").strip().lower()
            
            if pred_value and true_value:
                if pred_value == true_value:
                    correct_fields += 1
                    self.test_results['field_metrics'][field]['tp'] += 1
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                    if self._is_partial_match(pred_value, true_value):
                        correct_fields += 0.5
                        self.test_results['field_metrics'][field]['tp'] += 1
                    else:
                        self.test_results['field_metrics'][field]['fp'] += 1
                        document_perfect = False
            elif pred_value and not true_value:
                self.test_results['field_metrics'][field]['fp'] += 1
                document_perfect = False
            elif not pred_value and true_value:
                self.test_results['field_metrics'][field]['fn'] += 1
                document_perfect = False
                
        if document_perfect and total_fields > 0:
            self.test_results['perfect_documents'] += 1
            
        return correct_fields / total_fields if total_fields > 0 else 0.0
        
    def _is_partial_match(self, pred: str, true: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —á–∏—Å–µ–ª
        pred_digits = ''.join(filter(str.isdigit, pred))
        true_digits = ''.join(filter(str.isdigit, true))
        
        if pred_digits and pred_digits == true_digits:
            return True
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏—è
        if len(pred) > 3 and len(true) > 3:
            if pred in true or true in pred:
                return True
                
        return False
        
    def _calculate_overall_metrics(self, results: List[Dict]):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏"""
        logger.info("\nüìä –û–ë–©–ò–ï –ú–ï–¢–†–ò–ö–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:")
        
        # –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        accuracies = [r['accuracy'] for r in results if r['accuracy'] is not None]
        if accuracies:
            avg_accuracy = np.mean(accuracies)
            logger.info(f"üéØ –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {avg_accuracy:.1%}")
            logger.info(f"üìÑ –ò–¥–µ–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {self.test_results['perfect_documents']}/{self.test_results['total_documents']} ({self.test_results['perfect_documents']/self.test_results['total_documents']*100:.1f}%)")
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
            acc_90_plus = sum(1 for a in accuracies if a >= 0.9)
            acc_95_plus = sum(1 for a in accuracies if a >= 0.95)
            acc_98_plus = sum(1 for a in accuracies if a >= 0.98)
            
            logger.info(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏:")
            logger.info(f"   ‚â• 98%: {acc_98_plus} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({acc_98_plus/len(accuracies)*100:.1f}%)")
            logger.info(f"   ‚â• 95%: {acc_95_plus} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({acc_95_plus/len(accuracies)*100:.1f}%)")
            logger.info(f"   ‚â• 90%: {acc_90_plus} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({acc_90_plus/len(accuracies)*100:.1f}%)")
            
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ–ª—è–º
        logger.info("\nüìã –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ–ª—è–º:")
        field_f1_scores = []
        
        for field, metrics in self.test_results['field_metrics'].items():
            tp = metrics['tp']
            fp = metrics['fp']
            fn = metrics['fn']
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            field_f1_scores.append(f1)
            
            if f1 < 0.95:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–ª—è
                logger.info(f"   {field}: P={precision:.2f}, R={recall:.2f}, F1={f1:.2f}")
                
        # –û–±—â–∏–π F1
        if field_f1_scores:
            overall_f1 = np.mean(field_f1_scores)
            logger.info(f"\nüèÜ –û–±—â–∏–π F1-score: {overall_f1:.3f} ({overall_f1*100:.1f}%)")
            
            if overall_f1 >= 0.98:
                logger.info("üéâ –ü–û–ó–î–†–ê–í–õ–Ø–ï–ú! –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª–µ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å > 98%!")
            elif overall_f1 >= 0.95:
                logger.info("üî• –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! –ë–ª–∏–∑–∫–æ –∫ —Ü–µ–ª–∏.")
            elif overall_f1 >= 0.90:
                logger.info("‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –Ω–æ –µ—Å—Ç—å —á—Ç–æ —É–ª—É—á—à–∏—Ç—å.")
            else:
                logger.info("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è.")
                
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if self.test_results['processing_times']:
            avg_time = np.mean(self.test_results['processing_times'])
            logger.info(f"\n‚ö° –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {avg_time:.2f} —Å–µ–∫/–¥–æ–∫—É–º–µ–Ω—Ç")
            
        # –û—à–∏–±–∫–∏
        if self.test_results['errors']:
            logger.info(f"\n‚ùå –û—à–∏–±–æ–∫ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(self.test_results['errors'])}")
            
    def _save_test_results(self, results: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results_path = f"test_results_{timestamp}.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump({
                'model_path': self.model_path,
                'test_date': timestamp,
                'summary': {
                    'total_documents': self.test_results['total_documents'],
                    'perfect_documents': self.test_results['perfect_documents'],
                    'average_processing_time': np.mean(self.test_results['processing_times']) if self.test_results['processing_times'] else 0
                },
                'detailed_results': results,
                'field_metrics': dict(self.test_results['field_metrics']),
                'errors': self.test_results['errors']
            }, f, ensure_ascii=False, indent=2)
            
        logger.info(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_path}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        if results:
            df_data = []
            for r in results:
                if r['accuracy'] is not None:
                    df_data.append({
                        'file': r['file'],
                        'accuracy': r['accuracy'],
                        'predicted_fields': len(r['predicted']),
                        'ground_truth_fields': len(r['ground_truth']) if r['ground_truth'] else 0
                    })
                    
            if df_data:
                df = pd.DataFrame(df_data)
                csv_path = f"test_results_{timestamp}.csv"
                df.to_csv(csv_path, index=False)
                logger.info(f"üìä CSV —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {csv_path}")
                
    def validate_model_quality(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞
        
        Returns:
            True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ > 98%
        """
        if not self.test_results['field_metrics']:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")
            return False
            
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π F1
        f1_scores = []
        for field, metrics in self.test_results['field_metrics'].items():
            tp = metrics['tp']
            fp = metrics['fp']
            fn = metrics['fn']
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            f1_scores.append(f1)
            
        overall_f1 = np.mean(f1_scores) if f1_scores else 0
        
        logger.info(f"\nüéØ –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏:")
        logger.info(f"   –û–±—â–∏–π F1-score: {overall_f1:.3f}")
        logger.info(f"   –¶–µ–ª–µ–≤–æ–π –ø–æ—Ä–æ–≥: 0.98")
        
        if overall_f1 >= 0.98:
            logger.info("   ‚úÖ –ú–æ–¥–µ–ª—å –ü–†–û–®–õ–ê –≤–∞–ª–∏–¥–∞—Ü–∏—é!")
            return True
        else:
            logger.info("   ‚ùå –ú–æ–¥–µ–ª—å –ù–ï –ø—Ä–æ—à–ª–∞ –≤–∞–ª–∏–¥–∞—Ü–∏—é")
            logger.info(f"   –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–ª—É—á—à–∏—Ç—å –Ω–∞: {(0.98 - overall_f1)*100:.1f}%")
            return False


def test_donut_model(model_path: str, test_data_path: str, ground_truth_path: str = None):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    
    Args:
        model_path: –ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        test_data_path: –ü—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
        ground_truth_path: –ü—É—Ç—å –∫ ground truth (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    """
    tester = DonutModelTester(model_path)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    tester.load_model()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    results = tester.test_on_dataset(test_data_path, ground_truth_path)
    
    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
    passed = tester.validate_model_quality()
    
    return results, passed


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python donut_model_tester.py <model_path> <test_data_path> [ground_truth_path]")
        sys.exit(1)
        
    model_path = sys.argv[1]
    test_data_path = sys.argv[2]
    ground_truth_path = sys.argv[3] if len(sys.argv) > 3 else None
    
    test_donut_model(model_path, test_data_path, ground_truth_path) 