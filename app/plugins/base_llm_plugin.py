"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö LLM –ø–ª–∞–≥–∏–Ω–æ–≤
"""
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Union
import os
import json
import re
import logging
from PIL import Image

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å torch, –Ω–æ –¥–µ–ª–∞–µ–º —ç—Ç–æ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    logger.warning("PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. LLM –ø–ª–∞–≥–∏–Ω—ã –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.")

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å SecretsManager –¥–ª—è –∑–∞—â–∏—Ç—ã API –∫–ª—é—á–µ–π
try:
    from ..security.secrets_manager import get_secrets_manager
    SECRETS_MANAGER_AVAILABLE = True
except ImportError:
    SECRETS_MANAGER_AVAILABLE = False
    logger.warning("SecretsManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. API –∫–ª—é—á–∏ –±—É–¥—É—Ç —Ö—Ä–∞–Ω–∏—Ç—å—Å—è –≤ –Ω–µ–∑–∞—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω–æ–º –≤–∏–¥–µ.")

from ..base_processor import BaseProcessor

class LLMProviderConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM"""
    def __init__(self, name: str, display_name: str, models: List[str], 
                 requires_api_key: bool = True, api_key_name: str = None,
                 default_model: str = None, supports_vision: bool = True,
                 supports_files: bool = False):
        self.name = name
        self.display_name = display_name
        self.models = models
        self.requires_api_key = requires_api_key
        self.api_key_name = api_key_name or f"{name.upper()}_API_KEY"
        self.default_model = default_model or (models[0] if models else None)
        self.supports_vision = supports_vision
        self.supports_files = supports_files

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
LLM_PROVIDERS = {
    "openai": LLMProviderConfig(
        name="openai",
        display_name="OpenAI (ChatGPT)",
        models=[
            "gpt-4o",
            "gpt-4o-mini", 
            "gpt-4-turbo",
            "gpt-4",
            "gpt-3.5-turbo"
        ],
        default_model="gpt-4o",
        supports_vision=True,
        supports_files=True  # GPT-4o, GPT-4o-mini, GPT-4-turbo –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —Ñ–∞–π–ª—ã
    ),
    "anthropic": LLMProviderConfig(
        name="anthropic", 
        display_name="Anthropic (Claude)",
        models=[
            "claude-3-5-sonnet-20241022",
            "claude-3-5-haiku-20241022",
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "claude-3-haiku-20240307"
        ],
        default_model="claude-3-5-sonnet-20241022",
        supports_vision=True,
        supports_files=True  # Claude 3.5 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    ),
    "google": LLMProviderConfig(
        name="google",
        display_name="Google (Gemini)",
        models=[
            "models/gemini-2.0-flash-exp",
            "models/gemini-1.5-pro-latest",
            "models/gemini-1.5-flash-latest",
            "models/gemini-1.5-pro-002",
            "models/gemini-1.5-flash-002"
        ],
        default_model="models/gemini-2.0-flash-exp",
        supports_vision=True,
        supports_files=True  # Gemini –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–æ–≤
    ),
    "mistral": LLMProviderConfig(
        name="mistral",
        display_name="Mistral AI",
        models=[
            "mistral-large-latest",
            "mistral-medium-latest", 
            "mistral-small-latest",
            "pixtral-12b-2409"
        ],
        default_model="mistral-large-latest",
        supports_vision=True,
        supports_files=False  # Mistral –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø—Ä—è–º—É—é –∑–∞–≥—Ä—É–∑–∫—É —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ API
    ),
    "deepseek": LLMProviderConfig(
        name="deepseek",
        display_name="DeepSeek",
        models=[
            "deepseek-chat",
            "deepseek-coder"
        ],
        default_model="deepseek-chat", 
        supports_vision=False,
        supports_files=False  # DeepSeek –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã
    ),
    "xai": LLMProviderConfig(
        name="xai",
        display_name="xAI (Grok)",
        models=[
            "grok-beta",
            "grok-vision-beta"
        ],
        default_model="grok-vision-beta",
        supports_vision=True,
        supports_files=False  # xAI Grok –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã
    ),
    "ollama": LLMProviderConfig(
        name="ollama",
        display_name="Ollama (–õ–æ–∫–∞–ª—å–Ω–æ)",
        models=[
            # Vision –º–æ–¥–µ–ª–∏ (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
            "llama3.2-vision:11b",
            "qwen2.5vl:7b",
            "gemma3:12b",
            "gemma3:4b",
            
            # Text-only –º–æ–¥–µ–ª–∏
            "llama3.2:3b",
            "llama3.1:8b",
            "llama3.1:70b",
            "mistral:7b",
            "qwen2.5:7b"
        ],
        default_model="llama3.2-vision:11b",
        requires_api_key=False,
        supports_vision=True,
        supports_files=True  # Ollama –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã —á–µ—Ä–µ–∑ vision –º–æ–¥–µ–ª–∏
    )
}

class BaseLLMPlugin(BaseProcessor):
    """
    –ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö LLM –ø–ª–∞–≥–∏–Ω–æ–≤.
    –†–∞—Å—à–∏—Ä—è–µ—Ç BaseProcessor —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é –¥–ª—è LLM.
    """
    
    def __init__(self, provider_name: str, model_name: str = None, api_key: str = None, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –ø–ª–∞–≥–∏–Ω–∞.
        
        Args:
            provider_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (openai, anthropic, google, etc.)
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default)
            api_key: API –∫–ª—é—á –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–ª–∞–≥–∏–Ω–∞
        """
        self.provider_name = provider_name
        self.provider_config = LLM_PROVIDERS.get(provider_name)
        
        if not self.provider_config:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä LLM: {provider_name}")
        
        self.model_name = model_name or self.provider_config.default_model
        self.api_key = self._get_secure_api_key(api_key)
        self.client = None
        self.is_loaded = False
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.generation_config = {
            "max_tokens": 4096,
            "temperature": 0.1,
            "top_p": 0.9,
        }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.generation_config.update(kwargs.get("generation_config", {}))
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω LLM –ø–ª–∞–≥–∏–Ω: {self.provider_config.display_name} - {self.model_name}")
    
    def _get_secure_api_key(self, api_key: str = None) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç API –∫–ª—é—á –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —Å–ø–æ—Å–æ–±–æ–º
        
        Args:
            api_key: –ü–µ—Ä–µ–¥–∞–Ω–Ω—ã–π API –∫–ª—é—á
            
        Returns:
            Optional[str]: API –∫–ª—é—á –∏–ª–∏ None
        """
        if not self.provider_config.requires_api_key:
            return None
        
        # –ï—Å–ª–∏ –∫–ª—é—á –ø–µ—Ä–µ–¥–∞–Ω –Ω–∞–ø—Ä—è–º—É—é, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if api_key:
            return api_key
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ SecretsManager
        if SECRETS_MANAGER_AVAILABLE:
            try:
                secrets_manager = get_secrets_manager()
                secret_key = f"{self.provider_name}_api_key"
                stored_key = secrets_manager.get_secret(secret_key)
                if stored_key:
                    logger.debug(f"API –∫–ª—é—á –¥–ª—è {self.provider_name} –ø–æ–ª—É—á–µ–Ω –∏–∑ SecretsManager")
                    return stored_key
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–ª—é—á–∞ –∏–∑ SecretsManager: {e}")
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        env_key = self.provider_config.api_key_name
        api_key = os.environ.get(env_key)
        if api_key:
            logger.debug(f"API –∫–ª—é—á –¥–ª—è {self.provider_name} –ø–æ–ª—É—á–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è {env_key}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SecretsManager –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            if SECRETS_MANAGER_AVAILABLE:
                try:
                    secrets_manager = get_secrets_manager()
                    secrets_manager.store_secret(f"{self.provider_name}_api_key", api_key)
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–ª—é—á –≤ SecretsManager: {e}")
        
        return api_key
    
    def validate_api_key(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å API –∫–ª—é—á–∞
        
        Returns:
            bool: True –µ—Å–ª–∏ –∫–ª—é—á –µ—Å—Ç—å –∏ –≤–∞–ª–∏–¥–µ–Ω
        """
        if not self.provider_config.requires_api_key:
            return True
        
        if not self.api_key:
            logger.error(f"API –∫–ª—é—á –¥–ª—è {self.provider_name} –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return False
        
        # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –∫–ª—é—á–∞
        if len(self.api_key) < 10:
            logger.error(f"API –∫–ª—é—á –¥–ª—è {self.provider_name} —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π")
            return False
        
        return True
    
    # –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ—Ç BaseProcessor
    @abstractmethod
    def process_image(self, image_path, ocr_lang=None, custom_prompt=None):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        pass
    
    def supports_training(self) -> bool:
        """LLM –ø–ª–∞–≥–∏–Ω—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –æ–±—É—á–µ–Ω–∏–µ."""
        return True
    
    def get_trainer_class(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª–∞—Å—Å LLMTrainer."""
        try:
            from .llm_trainer import LLMTrainer
            return LLMTrainer
        except ImportError:
            logger.warning("LLMTrainer –µ—â–µ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω")
            return None
    
    def get_model_type(self) -> str:
        return f"llm_{self.provider_name}"
    
    def get_full_prompt(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω–≤–æ–π—Å–æ–≤."""
        return self.create_invoice_prompt()
    
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è LLM –º–µ—Ç–æ–¥—ã
    @abstractmethod
    def load_model(self) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç.
        
        Returns:
            bool: True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        pass
    
    @abstractmethod
    def generate_response(self, prompt: str, image_path: str = None, image_context: str = "") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è/–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
            image_context: –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ/OCR —Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            str: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        pass
    
    def get_training_config(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.
        –î–ª—è –≤–Ω–µ—à–Ω–∏—Ö API –æ–±—ã—á–Ω–æ –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ.
        
        Returns:
            dict: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        return {
            "provider": self.provider_name,
            "model": self.model_name,
            "supports_fine_tuning": False,
            "generation_config": self.generation_config
        }
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def preprocess_image_for_llm(self, image_path: str) -> Image.Image:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è LLM.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            
        Returns:
            PIL.Image: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        try:
            image = Image.open(image_path)
            if image.mode != 'RGB':
                image = image.convert('RGB')
            return image
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            return Image.new('RGB', (100, 100), color='white')
    
    def extract_text_from_image(self, image_path: str, ocr_lang: str = "rus+eng") -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OCR.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            ocr_lang: –Ø–∑—ã–∫–∏ –¥–ª—è OCR
            
        Returns:
            str: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            import pytesseract
            image = self.preprocess_image_for_llm(image_path)
            text = pytesseract.image_to_string(image, lang=ocr_lang)
            return text.strip()
        except ImportError:
            logger.warning("pytesseract –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
            return "OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ pytesseract"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ OCR: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    def create_invoice_prompt(self, custom_prompt: Optional[str] = None, include_context_fields: bool = True, 
                             use_adaptive: bool = True, ocr_text: Optional[str] = None, image_available: bool = False) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω–≤–æ–π—Å–∞.
        
        Args:
            custom_prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            include_context_fields: –í–∫–ª—é—á–∞—Ç—å –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–æ–ª—è –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            use_adaptive: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–º–ø—Ç–æ–≤ (–¥–ª—è Ollama)
            ocr_text: –¢–µ–∫—Å—Ç –∏–∑ OCR (–¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã)
            image_available: –î–æ—Å—Ç—É–ø–Ω–æ –ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            
        Returns:
            str: –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
        """
        if custom_prompt:
            return custom_prompt
        
        # –î–ª—è Ollama –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–º–ø—Ç–æ–≤
        if self.provider_name == 'ollama' and use_adaptive:
            try:
                from .models.adaptive_prompt_manager import create_adaptive_invoice_prompt
                from ..field_manager import FieldManager
                
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
                field_manager = FieldManager()
                fields = {}
                for field in field_manager.get_enabled_fields():
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º display_name –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—è
                    fields[field.id] = field.display_name
                
                # –°–æ–∑–¥–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
                adaptive_prompt = create_adaptive_invoice_prompt(
                    model_name=self.model_name,
                    fields=fields,
                    image_available=image_available,
                    ocr_text=ocr_text
                )
                
                print(f"[ADAPTIVE] –°–æ–∑–¥–∞–Ω –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è {self.model_name}")
                return adaptive_prompt
                
            except Exception as e:
                print(f"[WARNING] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ–±—ã—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞
        try:
            from pathlib import Path
            import os
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –ø—Ä–æ–º–ø—Ç–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            if self.provider_name in ['openai', 'anthropic', 'google', 'mistral', 'deepseek', 'xai']:
                prompt_filename = f"cloud_llm_{self.provider_name}_prompt.txt"
            elif self.provider_name == 'ollama':
                prompt_filename = f"local_llm_{self.provider_name}_prompt.txt"
            else:
                prompt_filename = f"{self.provider_name}_prompt.txt"
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
            current_dir = Path(__file__).parent.parent.parent  # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
            prompt_path = current_dir / "data" / "prompts" / prompt_filename
            
            logger.debug(f"–ü–æ–∏—Å–∫ –ø—Ä–æ–º–ø—Ç–∞ –ø–æ –ø—É—Ç–∏: {prompt_path}")
            
            if prompt_path.exists():
                with open(prompt_path, 'r', encoding='utf-8') as f:
                    file_prompt = f.read().strip()
                    if file_prompt:
                        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞: {prompt_filename}")
                        return file_prompt
            else:
                logger.warning(f"‚ùå –§–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_path}")
                    
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞: {e}")
            logger.debug(f"Traceback: ", exc_info=True)
        
        # Fallback - —Å–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –ø—Ä–æ–º–ø—Ç –¥–ª—è {self.provider_name}")
        
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        base_prompt = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã –∏–ª–∏ –∏–Ω–≤–æ–π—Å–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–∑ –Ω–µ–≥–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.

–ò–∑–≤–ª–µ–∫–∏ —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–ª—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:"""

        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—è –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Ç–∞–±–ª–∏—Ü—ã –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        fields_json = {}
        if include_context_fields:
            try:
                from ..settings_manager import settings_manager
                table_fields = settings_manager.get_table_fields()
                if table_fields:
                    fields_json = {field['name']: field.get('description', '') for field in table_fields}
            except (ImportError, AttributeError, KeyError, TypeError) as e:
                # –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–æ–ª–µ–π —Ç–∞–±–ª–∏—Ü—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
                pass
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö –ø–æ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ
        if not fields_json:
            fields_json = {
                "sender": "–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏-–ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ –∏–ª–∏ –ø—Ä–æ–¥–∞–≤—Ü–∞",
                "invoice_number": "–ù–æ–º–µ—Ä —Å—á–µ—Ç–∞, –∏–Ω–≤–æ–π—Å–∞ –∏–ª–∏ —Ñ–∞–∫—Ç—É—Ä—ã", 
                "invoice_date": "–î–∞—Ç–∞ –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å—á–µ—Ç–∞ –∏–ª–∏ –∏–Ω–≤–æ–π—Å–∞",
                "total": "–û–±—â–∞—è —Å—É–º–º–∞ –∫ –æ–ø–ª–∞—Ç–µ —Å —É—á–µ—Ç–æ–º –ù–î–°",
                "amount_no_vat": "–°—É–º–º–∞ –±–µ–∑ –ù–î–°",
                "vat_percent": "–°—Ç–∞–≤–∫–∞ –ù–î–° –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö",
                "currency": "–í–∞–ª—é—Ç–∞ –ø–ª–∞—Ç–µ–∂–∞",
                "category": "–ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –∏–ª–∏ —É—Å–ª—É–≥",
                "description": "–û–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤, —É—Å–ª—É–≥ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "note": "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏—è –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏"
            }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        fields_text = ""
        for field_id, description in fields_json.items():
            fields_text += f"- {field_id}: {description}\n"
        
        instructions = """
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ—á–Ω—ã–µ ID –ø–æ–ª–µ–π –∫–∞–∫ –∫–ª—é—á–∏
3. –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π –∑–Ω–∞—á–µ–Ω–∏–µ "N/A"
4. –í—Å–µ —Å—É–º–º—ã —É–∫–∞–∑—ã–≤–∞–π —á–∏—Å–ª–∞–º–∏ –±–µ–∑ —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç
5. –î–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
6. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–º –∫ –¥–µ—Ç–∞–ª—è–º

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤–µ—Ä–Ω–∏ JSON —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏:"""

        return base_prompt + "\n\n" + fields_text + instructions
    
    def parse_llm_response(self, response: str) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON –¥–∞–Ω–Ω—ã–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞.
        
        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç LLM
            
        Returns:
            dict: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        try:
            logger.debug(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –æ—Ç {self.provider_name}, –¥–ª–∏–Ω–∞: {len(response) if response else 0}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ API
            if self._is_error_response(response):
                error_msg = self._extract_error_message(response)
                logger.error(f"‚ùå –û—Ç–≤–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—à–∏–±–∫—É API: {error_msg}")
                return {"error": error_msg, "note_gemini": f"–û—à–∏–±–∫–∞ API {self.provider_name}: {error_msg}"}
            
            # –ü—Ä–æ–±—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä (–¥–ª—è Ollama)
            if self.provider_name == 'ollama':
                try:
                    from .models.response_parser import parse_llm_invoice_response
                    from ..field_manager import FieldManager
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
                    field_manager = FieldManager()
                    required_fields = [field.id for field in field_manager.get_enabled_fields() if hasattr(field, 'id')]
                    
                    # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
                    parsed_data = parse_llm_invoice_response(
                        response,
                        required_fields,
                        model_name=self.model_name
                    )
                    
                    if parsed_data and any(v != "N/A" for v in parsed_data.values()):
                        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞")
                        return parsed_data
                    
                except Exception as e:
                    logger.warning(f"[WARNING] –û—à–∏–±–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞, fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π: {e}")
            
            # Fallback –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø–∞—Ä—Å–∏–Ω–≥–∞
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            cleaned_response = self._clean_json_string(response)
            logger.debug(f"üßπ –û—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤): {cleaned_response[:200]}...")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
            json_patterns = [
                r'\{[\s\S]*\}',  # –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω
                r'```json\s*(\{[\s\S]*?\})\s*```',  # JSON –≤ markdown –±–ª–æ–∫–µ
                r'```\s*(\{[\s\S]*?\})\s*```',  # JSON –≤ –æ–±—ã—á–Ω–æ–º –±–ª–æ–∫–µ –∫–æ–¥–∞
            ]
            
            for pattern in json_patterns:
                json_match = re.search(pattern, cleaned_response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1) if json_match.lastindex else json_match.group()
                    logger.debug(f"üìÑ –ù–∞–π–¥–µ–Ω JSON (–ø–∞—Ç—Ç–µ—Ä–Ω {pattern}): {json_str[:100]}...")
                    
                    try:
                        data = json.loads(json_str)
                        logger.info(f"‚úÖ JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω, –ø–æ–ª–µ–π: {len(data)}")
                        return self._normalize_invoice_data(data)
                    except json.JSONDecodeError as parse_error:
                        logger.debug(f"üîß –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º {pattern}: {parse_error}")
                        continue
            
            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            logger.warning("‚ùå JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM")
            logger.debug(f"üìù –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç LLM:\n{response}")
            return {"error": "JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ", "raw_response": response[:500]}
                
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
            logger.debug(f"üìù –û—Ç–≤–µ—Ç LLM: {response[:500]}...")
            return {"error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}", "raw_response": response[:500]}
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ LLM: {e}")
            logger.debug(f"üìù Traceback: ", exc_info=True)
            return {"error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}", "raw_response": response[:500] if response else "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç"}
    
    def _is_error_response(self, response: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –æ—à–∏–±–∫—É API."""
        if not response:
            return True
        
        error_indicators = [
            "error code:",
            "error:",
            "insufficient_quota",
            "rate_limit_exceeded", 
            "invalid_api_key",
            "user location is not supported",
            "authentication failed",
            "permission denied",
            "service unavailable",
            "internal server error"
        ]
        
        response_lower = response.lower()
        return any(indicator in response_lower for indicator in error_indicators)
    
    def _extract_error_message(self, response: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞."""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON —Å –æ—à–∏–±–∫–æ–π
            import json
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                error_data = json.loads(json_match.group())
                if 'error' in error_data:
                    if isinstance(error_data['error'], dict):
                        return error_data['error'].get('message', str(error_data['error']))
                    else:
                        return str(error_data['error'])
            
            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
            return response[:200] + "..." if len(response) > 200 else response
            
        except (json.JSONDecodeError, ValueError, TypeError) as e:
            # –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
            return response[:200] + "..." if len(response) > 200 else response
    
    def _clean_json_string(self, json_str: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É JSON –æ—Ç –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        # –£–¥–∞–ª—è–µ–º markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        json_str = re.sub(r'```json\s*', '', json_str)
        json_str = re.sub(r'```\s*$', '', json_str)
        return json_str.strip()
    
    def _normalize_invoice_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–Ω–≤–æ–π—Å–∞."""
        normalized = {}
        
        for key, value in data.items():
            if value is None or value == "":
                continue
                
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
            if any(word in key.lower() for word in ['—Å—É–º–º–∞', '–Ω–¥—Å', '–∏–Ω–Ω', '–∫–ø–ø', '–±–∏–∫']):
                if isinstance(value, str):
                    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –∑–∞–ø—è—Ç—ã–µ, –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ —Ç–æ—á–∫–∏
                    cleaned_value = re.sub(r'[^\d,.]', '', value)
                    cleaned_value = cleaned_value.replace(',', '.')
                    try:
                        if '.' in cleaned_value:
                            normalized[key] = float(cleaned_value)
                        else:
                            normalized[key] = int(cleaned_value) if cleaned_value else 0
                    except ValueError:
                        normalized[key] = value
                else:
                    normalized[key] = value
            else:
                normalized[key] = value
                
        return normalized
    
    def _safe_float(self, value) -> float:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float."""
        if isinstance(value, (int, float)):
            return float(value)
        if isinstance(value, str):
            try:
                # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ü–∏—Ñ—Ä, —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
                cleaned = re.sub(r'[^\d,.]', '', value)
                cleaned = cleaned.replace(',', '.')
                return float(cleaned) if cleaned else 0.0
            except ValueError:
                return 0.0
        return 0.0
    
    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏."""
        return {
            "provider": self.provider_config.display_name,
            "model": self.model_name,
            "supports_vision": self.provider_config.supports_vision,
            "requires_api_key": self.provider_config.requires_api_key,
            "is_loaded": self.is_loaded,
            "has_api_key": bool(self.api_key) if self.provider_config.requires_api_key else True
        }
    
    @staticmethod
    def get_available_providers() -> Dict[str, LLMProviderConfig]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM."""
        return LLM_PROVIDERS
    
    @staticmethod
    def get_provider_models(provider_name: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
        config = LLM_PROVIDERS.get(provider_name)
        return config.models if config else []
    
    @staticmethod
    def provider_supports_files(provider_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Ñ–∞–π–ª—ã."""
        config = LLM_PROVIDERS.get(provider_name)
        return config.supports_files if config else False
    
    @staticmethod
    def get_file_capable_providers() -> Dict[str, LLMProviderConfig]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —Ñ–∞–π–ª—ã."""
        return {name: config for name, config in LLM_PROVIDERS.items() 
                if config.supports_files}
    
    @staticmethod
    def update_provider_models(provider_name: str, models: List[str]) -> bool:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–∑ API.
        
        Args:
            provider_name: –ò–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            models: –ù–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            
        Returns:
            bool: True –µ—Å–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        if provider_name in LLM_PROVIDERS:
            LLM_PROVIDERS[provider_name].models = models
            logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è {provider_name}: {len(models)} –º–æ–¥–µ–ª–µ–π")
            return True
        return False
    
    @staticmethod
    def refresh_provider_models(provider_name: str, api_key: str = None) -> List[str]:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ API –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Args:
            provider_name: –ò–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            api_key: API –∫–ª—é—á –¥–ª—è –¥–æ—Å—Ç—É–ø–∞
            
        Returns:
            List[str]: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        """
        if provider_name == "openai":
            return BaseLLMPlugin._refresh_openai_models(api_key)
        elif provider_name == "google":
            return BaseLLMPlugin._refresh_google_models(api_key)
        elif provider_name == "anthropic":
            return BaseLLMPlugin._refresh_anthropic_models(api_key)
        else:
            logger.warning(f"–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {provider_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
            return LLM_PROVIDERS.get(provider_name, LLMProviderConfig("", "", [])).models
    
    @staticmethod
    def _refresh_openai_models(api_key: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π OpenAI —á–µ—Ä–µ–∑ API."""
        try:
            import openai
            client = openai.OpenAI(api_key=api_key)
            models = client.models.list()
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏
            relevant_models = []
            for model in models.data:
                if any(prefix in model.id for prefix in ['gpt-', 'dall-e', 'whisper']):
                    if not BaseLLMPlugin._is_openai_model_deprecated(model.id):
                        relevant_models.append(model.id)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
            relevant_models.sort(reverse=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
            if relevant_models:
                BaseLLMPlugin.update_provider_models("openai", relevant_models)
            
            return relevant_models
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π OpenAI: {e}")
            return LLM_PROVIDERS["openai"].models
    
    @staticmethod
    def _refresh_google_models(api_key: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Google —á–µ—Ä–µ–∑ API."""
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            
            models = []
            for model in genai.list_models():
                if 'generateContent' in model.supported_generation_methods:
                    models.append(model.name)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
            models.sort(reverse=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫
            if models:
                BaseLLMPlugin.update_provider_models("google", models)
            
            return models
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Google: {e}")
            return LLM_PROVIDERS["google"].models
    
    @staticmethod
    def _refresh_anthropic_models(api_key: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Anthropic."""
        # Anthropic –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        return LLM_PROVIDERS["anthropic"].models
    
    @staticmethod
    def _is_openai_model_deprecated(model_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å OpenAI —É—Å—Ç–∞—Ä–µ–≤—à–µ–π."""
        deprecated_patterns = [
            'davinci', 'curie', 'babbage', 'ada',
            'text-', 'code-', 'edit-', 'if-',
            '-001', '-002', '-003'
        ]
        return any(pattern in model_id for pattern in deprecated_patterns) 