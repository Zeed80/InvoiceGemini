"""
LLM Trainer –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π –≤ InvoiceGemini
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç LoRA/QLoRA fine-tuning —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
"""
import os
import json
import torch
from typing import Dict, Any, Optional, List, Callable
from dataclasses import dataclass
import time
from datetime import datetime

try:
    from transformers import (
        AutoTokenizer, AutoModelForCausalLM, TrainingArguments, 
        Trainer, DataCollatorForLanguageModeling
    )
    from datasets import Dataset, DatasetDict
    from peft import LoraConfig, get_peft_model, TaskType, PeftModel
    import accelerate
    TRAINING_AVAILABLE = True
except ImportError:
    TRAINING_AVAILABLE = False
    print("[WARN] –û–±—É—á–∞—é—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers datasets peft accelerate")

@dataclass
class TrainingMetrics:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
    epoch: int = 0
    step: int = 0
    loss: float = 0.0
    eval_loss: float = 0.0
    learning_rate: float = 0.0
    epoch_time: float = 0.0
    gpu_memory_used: float = 0.0
    samples_per_second: float = 0.0

class LLMTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM –º–æ–¥–µ–ª–µ–π —Å LoRA/QLoRA
    """
    
    def __init__(self, plugin_instance, progress_callback: Optional[Callable] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        
        Args:
            plugin_instance: –≠–∫–∑–µ–º–ø–ª—è—Ä LLM –ø–ª–∞–≥–∏–Ω–∞
            progress_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        """
        self.plugin = plugin_instance
        self.progress_callback = progress_callback
        self.training_active = False
        self.should_stop = False
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è
        self.trainer = None
        self.tokenizer = None
        self.model = None
        self.peft_model = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.current_metrics = TrainingMetrics()
        self.training_history = []
        
        if not TRAINING_AVAILABLE:
            raise ImportError("–¢—Ä–µ–±—É—é—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: transformers, datasets, peft, accelerate")
    
    def prepare_dataset_with_gemini(self, image_paths: List[str], gemini_processor, output_path: str) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Gemini API
        
        Args:
            image_paths: –ü—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å—á–µ—Ç–æ–≤
            gemini_processor: –≠–∫–∑–µ–º–ø–ª—è—Ä GeminiProcessor –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
            
        Returns:
            str: –ü—É—Ç—å –∫ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        """
        from datetime import datetime
        import time
        
        start_time = datetime.now()
        self._log("=" * 70)
        self._log("üìä –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê –° GEMINI")
        self._log("=" * 70)
        self._log(f"üìÖ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self._log(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_path}")
        self._log(f"üñºÔ∏è –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(image_paths)}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if not gemini_processor:
            self._log("‚ùå –û–®–ò–ë–ö–ê: GeminiProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
            raise ValueError("GeminiProcessor –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
        
        if not image_paths:
            self._log("‚ùå –û–®–ò–ë–ö–ê: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—É—Å—Ç")
            raise ValueError("–°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        valid_images = []
        invalid_images = []
        total_size = 0
        
        self._log(f"\nüîç –ü–†–û–í–ï–†–ö–ê –í–•–û–î–ù–´–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
        for image_path in image_paths:
            if os.path.exists(image_path):
                try:
                    size = os.path.getsize(image_path)
                    total_size += size
                    valid_images.append(image_path)
                    self._log(f"  ‚úÖ {os.path.basename(image_path)} ({size/1024:.1f} KB)")
                except Exception as e:
                    invalid_images.append((image_path, str(e)))
                    self._log(f"  ‚ùå {os.path.basename(image_path)} - –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è: {e}")
            else:
                invalid_images.append((image_path, "—Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω"))
                self._log(f"  ‚ùå {os.path.basename(image_path)} - —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        self._log(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –í–•–û–î–ù–´–• –î–ê–ù–ù–´–•:")
        self._log(f"  ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(valid_images)}")
        self._log(f"  ‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(invalid_images)}")
        self._log(f"  üìè –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_size/1024/1024:.1f} MB")
        
        if len(valid_images) == 0:
            self._log("‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            raise ValueError("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        os.makedirs(output_path, exist_ok=True)
        self._log(f"‚úÖ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å–æ–∑–¥–∞–Ω–∞/–ø—Ä–æ–≤–µ—Ä–µ–Ω–∞: {output_path}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫–∏
        training_data = []
        successful_count = 0
        failed_count = 0
        processing_times = []
        gemini_errors = []
        
        self._log(f"\nüîÑ –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô:")
        
        for i, image_path in enumerate(valid_images, 1):
            try:
                if self.should_stop:
                    self._log(f"üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ {i}")
                    break
                
                process_start = time.time()
                
                self._log(f"\nüì∑ [{i}/{len(valid_images)}] {os.path.basename(image_path)}")
                self._log(f"   üìÅ –ü—É—Ç—å: {image_path}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress = (i / len(valid_images)) * 100
                self._update_progress(i, len(valid_images), f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {os.path.basename(image_path)}")
                self._log(f"   üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {progress:.1f}%")
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
                file_size = os.path.getsize(image_path) / 1024  # KB
                self._log(f"   üìè –†–∞–∑–º–µ—Ä: {file_size:.1f} KB")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Gemini
                self._log(f"   ü§ñ –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Gemini...")
                result = gemini_processor.process_image(image_path)
                
                process_time = time.time() - process_start
                processing_times.append(process_time)
                
                if result and isinstance(result, dict):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    if 'extracted_data' in result or len(result) > 0:
                        # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä
                        training_example = self._create_training_example(image_path, result)
                        training_data.append(training_example)
                        successful_count += 1
                        
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        if isinstance(result, dict):
                            fields_count = len(result.get('extracted_data', result))
                            self._log(f"   üìã –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {fields_count}")
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è
                            key_fields = ['invoice_number', 'total_amount', 'invoice_date', 'supplier_name']
                            if 'extracted_data' in result:
                                extracted = result['extracted_data']
                            else:
                                extracted = result
                                
                            found_keys = [k for k in key_fields if k in extracted and extracted[k]]
                            if found_keys:
                                self._log(f"   üîë –ù–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è: {', '.join(found_keys)}")
                            else:
                                self._log(f"   ‚ö†Ô∏è –ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                        
                        self._log(f"   ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {process_time:.2f}—Å")
                        self._log(f"   ‚úÖ –£–°–ü–ï–®–ù–û –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ!")
                    else:
                        self._log(f"   ‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç Gemini")
                        failed_count += 1
                        gemini_errors.append((image_path, "–ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"))
                else:
                    self._log(f"   ‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç Gemini")
                    self._log(f"   üìù –¢–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}")
                    failed_count += 1
                    gemini_errors.append((image_path, f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {type(result)}"))
                
            except Exception as e:
                process_time = time.time() - process_start
                processing_times.append(process_time)
                
                error_msg = str(e)
                self._log(f"   üí• –û–®–ò–ë–ö–ê: {error_msg}")
                self._log(f"   ‚è±Ô∏è –í—Ä–µ–º—è –¥–æ –æ—à–∏–±–∫–∏: {process_time:.2f}—Å")
                
                failed_count += 1
                gemini_errors.append((image_path, error_msg))
                continue
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = datetime.now()
        total_duration = end_time - start_time
        
        self._log(f"\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        self._log(f"  ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_count}")
        self._log(f"  ‚ùå –û—à–∏–±–æ–∫: {failed_count}")
        self._log(f"  üìä –í—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {successful_count + failed_count}")
        
        if successful_count + failed_count > 0:
            success_rate = (successful_count / (successful_count + failed_count)) * 100
            self._log(f"  üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
            
            if success_rate < 50:
                self._log(f"  ‚ö†Ô∏è –ù–ò–ó–ö–ò–ô –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ Gemini API –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            elif success_rate < 80:
                self._log(f"  üü° –°—Ä–µ–¥–Ω–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞")
            else:
                self._log(f"  üü¢ –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞")
        
        self._log(f"  ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_duration}")
        
        if processing_times:
            avg_time = sum(processing_times) / len(processing_times)
            min_time = min(processing_times)
            max_time = max(processing_times)
            self._log(f"  ‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {avg_time:.2f}—Å")
            self._log(f"  üèÉ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {min_time:.2f}—Å")
            self._log(f"  üêå –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: {max_time:.2f}—Å")
            
            if avg_time > 30:
                self._log(f"  ‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (>{avg_time:.1f}—Å –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        self._log(f"\nüíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê:")
        dataset_file = os.path.join(output_path, "training_data.json")
        
        try:
            with open(dataset_file, 'w', encoding='utf-8') as f:
                json.dump(training_data, f, ensure_ascii=False, indent=2)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª
            file_size = os.path.getsize(dataset_file) / 1024  # KB
            self._log(f"  ‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {dataset_file}")
            self._log(f"  üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.1f} KB")
            self._log(f"  üìã –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(training_data)}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            if len(training_data) > 0:
                # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                avg_input_length = sum(len(ex.get('input', '')) for ex in training_data) / len(training_data)
                avg_output_length = sum(len(ex.get('output', '')) for ex in training_data) / len(training_data)
                
                self._log(f"  üìù –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ input: {avg_input_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                self._log(f"  üìù –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ output: {avg_output_length:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
                
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –∫–∞—á–µ—Å—Ç–≤–µ
                if avg_input_length < 50:
                    self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                elif avg_input_length > 5000:
                    self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                else:
                    self._log(f"  ‚úÖ –î–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞")
                
                if avg_output_length < 20:
                    self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                elif avg_output_length > 2000:
                    self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                else:
                    self._log(f"  ‚úÖ –î–ª–∏–Ω–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–∞—Ö –µ—Å–ª–∏ –µ—Å—Ç—å
            if gemini_errors:
                error_report_file = os.path.join(output_path, "processing_errors.json")
                error_report = {
                    "timestamp": datetime.now().isoformat(),
                    "total_errors": len(gemini_errors),
                    "errors": [{"image": img, "error": err} for img, err in gemini_errors]
                }
                
                with open(error_report_file, 'w', encoding='utf-8') as f:
                    json.dump(error_report, f, ensure_ascii=False, indent=2)
                    
                self._log(f"  üìã –û—Ç—á–µ—Ç –æ–± –æ—à–∏–±–∫–∞—Ö: {error_report_file}")
            
        except Exception as e:
            self._log(f"  üí• –û–®–ò–ë–ö–ê —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {str(e)}")
            raise
        
        self._log(f"\nüéâ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        self._log("=" * 70)
        
        return dataset_file
    
    def _create_training_example(self, image_path: str, gemini_result: Dict) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Gemini"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        try:
            image_text = self.plugin.extract_text_from_image(image_path)
        except (AttributeError, IOError, OSError, Exception) as e:
            # –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É
            image_text = "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç"
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = self.plugin.create_invoice_prompt()
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Gemini
        target_response = json.dumps(gemini_result, ensure_ascii=False, indent=2)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –¥–∏–∞–ª–æ–≥
        full_prompt = f"{prompt}\n\n–¢–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{image_text}"
        
        return {
            "input": full_prompt,
            "output": target_response,
            "image_path": image_path,
            "source": "gemini_generated"
        }
    
    def prepare_training_dataset(self, training_data_path: str):
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ JSON —Ñ–∞–π–ª–∞
        
        Args:
            training_data_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –æ–±—É—á–∞—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            
        Returns:
            DatasetDict: –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        """
        from datetime import datetime
        
        start_time = datetime.now()
        self._log("=" * 60)
        self._log("üìã –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø")
        self._log("=" * 60)
        self._log(f"üìÖ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self._log(f"üìÅ –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É: {training_data_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not os.path.exists(training_data_path):
            self._log(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {training_data_path}")
            raise FileNotFoundError(f"–§–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {training_data_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        file_size = os.path.getsize(training_data_path) / 1024  # KB
        self._log(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size:.1f} KB")
        
        if file_size > 100000:  # > 100MB
            self._log(f"‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª ({file_size:.1f} KB)")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self._log(f"\nüìñ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•...")
        try:
            with open(training_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            self._log(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON: {e}")
            raise
        except Exception as e:
            self._log(f"‚ùå –û–®–ò–ë–ö–ê –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {e}")
            raise
        
        self._log(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        self._log(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(data)}")
        
        if len(data) == 0:
            self._log(f"‚ùå –û–®–ò–ë–ö–ê: –ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç")
            raise ValueError("–î–∞—Ç–∞—Å–µ—Ç –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
        self._log(f"\nüîç –ê–ù–ê–õ–ò–ó –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•:")
        if isinstance(data, list) and len(data) > 0:
            sample = data[0]
            if isinstance(sample, dict):
                keys = list(sample.keys())
                self._log(f"  üìã –ö–ª—é—á–∏ –≤ –ø—Ä–∏–º–µ—Ä–∞—Ö: {keys}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                required_fields = ['input', 'output']
                missing_fields = [field for field in required_fields if field not in keys]
                
                if missing_fields:
                    self._log(f"  ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {missing_fields}")
                    raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è: {missing_fields}")
                else:
                    self._log(f"  ‚úÖ –í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
                input_lengths = [len(ex.get('input', '')) for ex in data if isinstance(ex, dict)]
                output_lengths = [len(ex.get('output', '')) for ex in data if isinstance(ex, dict)]
                
                if input_lengths:
                    avg_input = sum(input_lengths) / len(input_lengths)
                    min_input = min(input_lengths)
                    max_input = max(input_lengths)
                    self._log(f"  üìù Input –¥–ª–∏–Ω—ã - —Å—Ä–µ–¥–Ω: {avg_input:.0f}, –º–∏–Ω: {min_input}, –º–∞–∫—Å: {max_input}")
                
                if output_lengths:
                    avg_output = sum(output_lengths) / len(output_lengths)
                    min_output = min(output_lengths)
                    max_output = max(output_lengths)
                    self._log(f"  üìù Output –¥–ª–∏–Ω—ã - —Å—Ä–µ–¥–Ω: {avg_output:.0f}, –º–∏–Ω: {min_output}, –º–∞–∫—Å: {max_output}")
            else:
                self._log(f"  ‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: –ø—Ä–∏–º–µ—Ä—ã –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–ª–æ–≤–∞—Ä—è–º–∏")
                raise ValueError("–ü—Ä–∏–º–µ—Ä—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä—è–º–∏")
        else:
            self._log(f"  ‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: –¥–∞–Ω–Ω—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–æ–º")
            raise ValueError("–î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –∏ —Ñ–æ—Ä–º–∞—Ç
        model_family = getattr(self.plugin, 'model_family', 'unknown')
        model_name = getattr(self.plugin, 'model_name', 'unknown')
        self._log(f"\nü§ñ –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–ò:")
        self._log(f"  üîß –°–µ–º–µ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {model_family}")
        self._log(f"  üìõ –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self._log(f"\nüîÑ –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï –î–ê–ù–ù–´–•...")
        formatted_data = []
        skipped_count = 0
        
        for i, example in enumerate(data):
            try:
                if not isinstance(example, dict):
                    self._log(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä {i}: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö")
                    skipped_count += 1
                    continue
                
                if 'input' not in example or 'output' not in example:
                    self._log(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä {i}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                    skipped_count += 1
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                if model_family == "llama":
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat template –¥–ª—è Llama
                    text = self._format_llama_training_text(example["input"], example["output"])
                    self._log(f"  ü¶ô –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ñ–æ—Ä–º–∞—Ç Llama –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ {i+1}")
                else:
                    # –û–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç
                    text = f"{example['input']}\n\n{example['output']}<|endoftext|>"
                    self._log(f"  üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ–±—â–∏–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ {i+1}")
                
                formatted_data.append({"text": text})
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π 100-–π –ø—Ä–∏–º–µ—Ä
                if (i + 1) % 100 == 0:
                    self._log(f"  üìà –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i+1}/{len(data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                    
            except Exception as e:
                self._log(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–º–µ—Ä–∞ {i}: {e}")
                skipped_count += 1
                continue
        
        self._log(f"‚úÖ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        self._log(f"  üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(formatted_data)}")
        self._log(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {skipped_count}")
        
        if len(formatted_data) == 0:
            self._log(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            raise ValueError("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/validation (90/10)
        self._log(f"\nüìä –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/VALIDATION:")
        split_ratio = 0.9
        split_idx = int(len(formatted_data) * split_ratio)
        
        # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–¥ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º
        import random
        random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        random.shuffle(formatted_data)
        self._log(f"  üîÄ –î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ—à–∞–Ω—ã (seed=42)")
        
        train_data = formatted_data[:split_idx]
        val_data = formatted_data[split_idx:]
        
        self._log(f"  üìö Training set: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({len(train_data)/len(formatted_data)*100:.1f}%)")
        self._log(f"  üß™ Validation set: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ ({len(val_data)/len(formatted_data)*100:.1f}%)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        if len(train_data) < 10:
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")
        elif len(train_data) < 100:
            self._log(f"  üü° –ù–µ–±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ({len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")
        else:
            self._log(f"  ‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        if len(val_data) < 2:
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –û—á–µ–Ω—å –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ({len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")
        
        # –°–æ–∑–¥–∞–µ–º DatasetDict
        self._log(f"\nüèóÔ∏è –°–û–ó–î–ê–ù–ò–ï DATASETS...")
        try:
            dataset_dict = DatasetDict({
                "train": Dataset.from_list(train_data),
                "validation": Dataset.from_list(val_data)
            })
            self._log(f"‚úÖ DatasetDict —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞—Ç–∞—Å–µ—Ç–µ
            train_dataset = dataset_dict["train"]
            val_dataset = dataset_dict["validation"]
            
            self._log(f"  üìã Train dataset features: {list(train_dataset.features.keys())}")
            self._log(f"  üìã Validation dataset features: {list(val_dataset.features.keys())}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ
            train_text_lengths = [len(ex['text']) for ex in train_dataset]
            if train_text_lengths:
                avg_length = sum(train_text_lengths) / len(train_text_lengths)
                min_length = min(train_text_lengths)
                max_length = max(train_text_lengths)
                
                self._log(f"  üìè –î–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ - —Å—Ä–µ–¥–Ω: {avg_length:.0f}, –º–∏–Ω: {min_length}, –º–∞–∫—Å: {max_length}")
                
                if avg_length > 4000:
                    self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –î–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –º–æ–≥—É—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏")
                elif avg_length < 100:
                    self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ö–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã –º–æ–≥—É—Ç –ø–ª–æ—Ö–æ –æ–±—É—á–∞—Ç—å—Å—è")
                else:
                    self._log(f"  ‚úÖ –î–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã")
                    
        except Exception as e:
            self._log(f"‚ùå –û–®–ò–ë–ö–ê —Å–æ–∑–¥–∞–Ω–∏—è DatasetDict: {e}")
            raise
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        self._log(f"\nüéâ –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
        self._log(f"  ‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration}")
        self._log(f"  üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        self._log(f"    üìö Training: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        self._log(f"    üß™ Validation: {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        self._log(f"    üìà –ö–∞—á–µ—Å—Ç–≤–æ: {(len(formatted_data)/(len(data)) * 100):.1f}% –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
        self._log("=" * 60)
        
        return dataset_dict
    
    def _format_llama_training_text(self, input_text: str, output_text: str) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Llama –º–æ–¥–µ–ª–∏"""
        if "llama-3" in self.plugin.model_name.lower():
            return f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{input_text}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n{output_text}<|eot_id|>"
        else:
            # Llama 2 format
            return f"<s>[INST] {input_text} [/INST] {output_text} </s>"
    
    def setup_lora_config(self, training_config: Dict[str, Any]):
        """
        –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é LoRA
        
        Args:
            training_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            LoraConfig: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA
        """
        self._log("üîß –ù–ê–°–¢–†–û–ô–ö–ê LORA –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò:")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        rank = training_config.get("default_lora_rank", 16)
        alpha = training_config.get("default_lora_alpha", 32)
        dropout = training_config.get("lora_dropout", 0.1)
        target_modules = training_config.get("target_modules", ["q_proj", "v_proj"])
        
        self._log(f"  üìä LoRA —Ä–∞–Ω–≥ (r): {rank}")
        self._log(f"  üìä LoRA –∞–ª—å—Ñ–∞: {alpha}")
        self._log(f"  üìä Dropout: {dropout}")
        self._log(f"  üéØ –¶–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏: {target_modules}")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if rank <= 0:
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞–Ω–≥ ({rank}), –∏—Å–ø–æ–ª—å–∑—É–µ–º 16")
            rank = 16
        elif rank > 128:
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –í—ã—Å–æ–∫–∏–π —Ä–∞–Ω–≥ ({rank}), –º–æ–∂–µ—Ç –ø–æ—Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏")
        
        if alpha <= 0:
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∞–ª—å—Ñ–∞ ({alpha}), –∏—Å–ø–æ–ª—å–∑—É–µ–º 32")
            alpha = 32
            
        if not (0 <= dropout <= 1):
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π dropout ({dropout}), –∏—Å–ø–æ–ª—å–∑—É–µ–º 0.1")
            dropout = 0.1
        
        if not target_modules or not isinstance(target_modules, list):
            self._log(f"  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ü–µ–ª–µ–≤—ã–µ –º–æ–¥—É–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            target_modules = ["q_proj", "v_proj"]
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        try:
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=rank,
                lora_alpha=alpha,
                lora_dropout=dropout,
                target_modules=target_modules,
                bias="none"
            )
            
            self._log(f"  ‚úÖ LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            
            # –†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            param_efficiency = (rank * 2) / (4096 * 4096)  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            self._log(f"  üìà –ü—Ä–∏–º–µ—Ä–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {param_efficiency:.4f}")
            
            if rank < 8:
                self._log(f"  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ù–∏–∑–∫–∏–π —Ä–∞–Ω–≥ –º–æ–∂–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –æ–±—É—á–∞–µ–º–æ—Å—Ç—å")
            elif rank > 64:
                self._log(f"  üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –í—ã—Å–æ–∫–∏–π —Ä–∞–Ω–≥ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤")
            else:
                self._log(f"  ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á")
                
        except Exception as e:
            self._log(f"  ‚ùå –û–®–ò–ë–ö–ê —Å–æ–∑–¥–∞–Ω–∏—è LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise
        
        return lora_config
    
    def train_model(self, 
                   dataset_path: str, 
                   output_dir: str,
                   training_config: Dict[str, Any]) -> bool:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        
        Args:
            dataset_path: –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
            training_config: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            
        Returns:
            bool: True –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            import traceback
            import sys
            from datetime import datetime
            
            self.training_active = True
            self.should_stop = False
            start_time = datetime.now()
            
            self._log("=" * 80)
            self._log("üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø LLM –ú–û–î–ï–õ–ò")
            self._log("=" * 80)
            self._log(f"üìÖ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
            self._log(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
            self._log(f"üíæ –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            self._log(f"ü§ñ –ü–ª–∞–≥–∏–Ω: {self.plugin.__class__.__name__}")
            
            # üñ•Ô∏è –°–ò–°–¢–ï–ú–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê
            self._log("\nüñ•Ô∏è –°–ò–°–¢–ï–ú–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê:")
            self._log(f"  üêç Python –≤–µ—Ä—Å–∏—è: {sys.version.split()[0]}")
            if hasattr(torch, '__version__'):
                self._log(f"  üî• PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
            self._log(f"  üî• CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
            
            if torch.cuda.is_available():
                self._log(f"  üéÆ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
                self._log(f"  üìç –¢–µ–∫—É—â–µ–µ CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.current_device()}")
                try:
                    device_props = torch.cuda.get_device_properties(0)
                    self._log(f"  üéÆ GPU: {device_props.name}")
                    self._log(f"  üíæ –ü–∞–º—è—Ç—å GPU: {device_props.total_memory / 1024**3:.1f} GB")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
                    memory_allocated = torch.cuda.memory_allocated() / 1024**3
                    memory_cached = torch.cuda.memory_reserved() / 1024**3
                    memory_free = (device_props.total_memory / 1024**3) - memory_cached
                    
                    self._log(f"  üíæ –ü–∞–º—è—Ç—å –≤—ã–¥–µ–ª–µ–Ω–∞: {memory_allocated:.2f} GB")
                    self._log(f"  üíæ –ü–∞–º—è—Ç—å –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∞: {memory_cached:.2f} GB")
                    self._log(f"  üíæ –ü–∞–º—è—Ç—å —Å–≤–æ–±–æ–¥–Ω–∞: {memory_free:.2f} GB")
                    
                    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏
                    if memory_free < 2.0:
                        self._log("  ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–∏–∑–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–æ–±–æ–¥–Ω–æ–π GPU –ø–∞–º—è—Ç–∏!")
                        
                except Exception as gpu_error:
                    self._log(f"  ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU: {gpu_error}")
            else:
                self._log("  ‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU (–º–µ–¥–ª–µ–Ω–Ω–æ)")
            
            # üìã –î–ï–¢–ê–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø
            self._log("\nüìã –î–ï–¢–ê–õ–¨–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø:")
            for section, params in training_config.items():
                self._log(f"  üìÇ {section}:")
                if isinstance(params, dict):
                    for key, value in params.items():
                        self._log(f"    {key}: {value}")
                else:
                    self._log(f"    {params}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
            self._log("\nüîç –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ò–°–ò–ú–û–°–¢–ï–ô:")
            try:
                import transformers
                self._log(f"  ‚úÖ transformers: {transformers.__version__}")
            except ImportError:
                self._log("  ‚ùå transformers: –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–û")
                
            try:
                import datasets
                self._log(f"  ‚úÖ datasets: {datasets.__version__}")
            except ImportError:
                self._log("  ‚ùå datasets: –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–û")
                
            try:
                import peft
                self._log(f"  ‚úÖ peft: {peft.__version__}")
            except ImportError:
                self._log("  ‚ùå peft: –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–û")
                
            try:
                import accelerate
                self._log(f"  ‚úÖ accelerate: {accelerate.__version__}")
            except ImportError:
                self._log("  ‚ùå accelerate: –ù–ï –£–°–¢–ê–ù–û–í–õ–ï–ù–û")
            
            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            self._log("\nü§ñ ===== –≠–¢–ê–ü 1: –ó–ê–ì–†–£–ó–ö–ê –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò =====")
            if not self.plugin.is_loaded:
                self._log("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
                model_name = getattr(self.plugin, 'model_name', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å')
                self._log(f"   –ú–æ–¥–µ–ª—å: {model_name}")
                
                if not self.plugin.load_model():
                    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
                self._log("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            else:
                self._log("‚úÖ –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            self.model = self.plugin.model
            self.tokenizer = self.plugin.tokenizer
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
            if hasattr(self.model, 'num_parameters'):
                total_params = self.model.num_parameters()
                self._log(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {total_params:,}")
            
            # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("\nüìä ===== –≠–¢–ê–ü 2: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–¢–ê–°–ï–¢–ê =====")
            self._log(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑: {dataset_path}")
            if not os.path.exists(dataset_path):
                raise FileNotFoundError(f"–î–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {dataset_path}")
                
            dataset = self.prepare_training_dataset(dataset_path)
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
            self._log("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:")
            for split_name, split_data in dataset.items():
                self._log(f"   {split_name}: {len(split_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                if len(split_data) > 0:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
                    example = split_data[0]
                    if 'text' in example:
                        text_len = len(example['text'])
                        text_preview = example['text'][:100] + "..." if text_len > 100 else example['text']
                        self._log(f"     –ü—Ä–∏–º–µ—Ä –¥–ª–∏–Ω–∞: {text_len} —Å–∏–º–≤–æ–ª–æ–≤")
                        self._log(f"     –ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç: {text_preview}")
            
            # 3. –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self._log("\nüî§ ===== –≠–¢–ê–ü 3: –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –î–ê–¢–ê–°–ï–¢–ê =====")
            max_length = training_config.get("max_sequence_length", 2048)
            self._log(f"üìè –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {max_length}")
            self._log(f"üî§ –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {len(self.tokenizer)}")
            
            tokenized_dataset = self._tokenize_dataset(dataset, training_config)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
            self._log("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏:")
            for split_name, split_data in tokenized_dataset.items():
                self._log(f"   {split_name}: {len(split_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
                if len(split_data) > 0:
                    example_tokens = split_data[0]['input_ids']
                    self._log(f"     –ü—Ä–∏–º–µ—Ä –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤: {len(example_tokens)}")
            
            # 4. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
            self._log("\nüîß ===== –≠–¢–ê–ü 4: –ù–ê–°–¢–†–û–ô–ö–ê LORA =====")
            lora_config = self.setup_lora_config(training_config)
            self._log(f"üéØ –°–æ–∑–¥–∞–Ω–∏–µ PEFT –º–æ–¥–µ–ª–∏...")
            
            self.peft_model = get_peft_model(self.model, lora_config)
            
            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
            trainable_params = 0
            all_params = 0
            for _, param in self.peft_model.named_parameters():
                all_params += param.numel()
                if param.requires_grad:
                    trainable_params += param.numel()
                    
            self._log(f"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA:")
            self._log(f"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {all_params:,}")
            self._log(f"   –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
            self._log(f"   –ü—Ä–æ—Ü–µ–Ω—Ç –æ–±—É—á–∞–µ–º—ã—Ö: {100 * trainable_params / all_params:.2f}%")
            
            # 5. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            self._log("\nüìã ===== –≠–¢–ê–ü 5: –ù–ê–°–¢–†–û–ô–ö–ê –ü–ê–†–ê–ú–ï–¢–†–û–í –û–ë–£–ß–ï–ù–ò–Ø =====")
            self._log(f"üìÅ –°–æ–∑–¥–∞–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {output_dir}")
            os.makedirs(output_dir, exist_ok=True)
            
            training_args = self._setup_training_args(output_dir, training_config)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self._log("üìä –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
            self._log(f"   –≠–ø–æ—Ö: {training_args.num_train_epochs}")
            self._log(f"   Batch size (train): {training_args.per_device_train_batch_size}")
            self._log(f"   Batch size (eval): {training_args.per_device_eval_batch_size}")
            self._log(f"   Learning rate: {training_args.learning_rate}")
            self._log(f"   Gradient accumulation: {training_args.gradient_accumulation_steps}")
            self._log(f"   FP16: {getattr(training_args, 'fp16', False)}")
            self._log(f"   Gradient checkpointing: {getattr(training_args, 'gradient_checkpointing', False)}")
            
            # –†–∞—Å—á–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ –∏ –≤—Ä–µ–º–µ–Ω–∏
            train_dataset_size = len(tokenized_dataset["train"])
            effective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps
            steps_per_epoch = train_dataset_size // effective_batch_size
            total_steps = steps_per_epoch * training_args.num_train_epochs
            
            self._log(f"üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –æ–±—É—á–µ–Ω–∏–∏:")
            self._log(f"   üìÑ –†–∞–∑–º–µ—Ä —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞: {train_dataset_size}")
            self._log(f"   üìà –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {effective_batch_size}")
            self._log(f"   üî¢ –®–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É: {steps_per_epoch}")
            self._log(f"   üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {total_steps}")
            
            # 6. –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä–∞
            self._log("\nüèÉ ===== –≠–¢–ê–ü 6: –°–û–ó–î–ê–ù–ò–ï TRAINER =====")
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            self._log("‚úÖ Data collator —Å–æ–∑–¥–∞–Ω")
            
            self.trainer = Trainer(
                model=self.peft_model,
                args=training_args,
                train_dataset=tokenized_dataset["train"],
                eval_dataset=tokenized_dataset["validation"],
                data_collator=data_collator,
                callbacks=[TrainingProgressCallback(self)]
            )
            self._log("‚úÖ Trainer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # 7. –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            self._log("\nüöÄ ===== –≠–¢–ê–ü 7: –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø =====")
            self._log("üéØ –ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É –º–æ–¥–µ–ª–∏...")
            self._log(f"‚è∞ –í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è: {datetime.now().strftime('%H:%M:%S')}")
            
            train_result = self.trainer.train()
            
            if self.should_stop:
                self._log("‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                return False
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
            end_time = datetime.now()
            training_duration = end_time - start_time
            self._log(f"‚è∞ –í—Ä–µ–º—è –æ–∫–æ–Ω—á–∞–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è: {end_time.strftime('%H:%M:%S')}")
            self._log(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_duration}")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
            if hasattr(train_result, 'training_loss'):
                final_loss = train_result.training_loss
                self._log(f"üìâ –§–∏–Ω–∞–ª—å–Ω—ã–π training loss: {final_loss:.4f}")
                
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ loss
                if final_loss < 0.5:
                    loss_quality = "üü¢ –û—Ç–ª–∏—á–Ω—ã–π"
                elif final_loss < 1.0:
                    loss_quality = "üü° –•–æ—Ä–æ—à–∏–π"
                elif final_loss < 2.0:
                    loss_quality = "üü† –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω—ã–π"
                else:
                    loss_quality = "üî¥ –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è"
                    
                self._log(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è: {loss_quality}")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if hasattr(train_result, 'global_step') and train_result.global_step > 0:
                total_seconds = training_duration.total_seconds()
                steps_per_second = train_result.global_step / total_seconds
                seconds_per_step = total_seconds / train_result.global_step
                
                self._log(f"‚ö° –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
                self._log(f"   üèÉ –®–∞–≥–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {steps_per_second:.2f}")
                self._log(f"   ‚è±Ô∏è –°–µ–∫—É–Ω–¥ –Ω–∞ —à–∞–≥: {seconds_per_step:.2f}")
                
                if seconds_per_step < 2:
                    perf_rating = "üöÄ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ"
                elif seconds_per_step < 10:
                    perf_rating = "‚ö° –ë—ã—Å—Ç—Ä–æ"
                elif seconds_per_step < 30:
                    perf_rating = "üêé –ù–æ—Ä–º–∞–ª—å–Ω–æ"
                else:
                    perf_rating = "üêå –ú–µ–¥–ª–µ–Ω–Ω–æ"
                    
                self._log(f"   üìà –û—Ü–µ–Ω–∫–∞: {perf_rating}")
            
            # 8. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self._log("\nüíæ ===== –≠–¢–ê–ü 8: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====")
            self._log(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ –≤: {output_dir}")
            
            self.trainer.save_model()
            self._log("‚úÖ LoRA –∞–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            
            self.tokenizer.save_pretrained(output_dir)
            self._log("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            
            # 9. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self._log("\nüìä ===== –≠–¢–ê–ü 9: –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–ï–¢–†–ò–ö =====")
            
            # –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
            metrics_file = os.path.join(output_dir, "training_metrics.json")
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False)
            self._log(f"‚úÖ –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metrics_file}")
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
            metadata = {
                'base_model': getattr(self.plugin, 'model_name', 'unknown'),
                'plugin_class': self.plugin.__class__.__name__,
                'training_config': training_config,
                'created_at': datetime.now().isoformat(),
                'dataset_path': dataset_path,
                'training_duration': str(training_duration),
                'final_loss': getattr(train_result, 'training_loss', None),
                'total_steps': getattr(train_result, 'global_step', None),
                'dataset_stats': {
                    'train_size': len(tokenized_dataset["train"]),
                    'validation_size': len(tokenized_dataset["validation"]) if "validation" in tokenized_dataset else 0
                },
                'model_stats': {
                    'total_params': all_params,
                    'trainable_params': trainable_params,
                    'trainable_percent': 100 * trainable_params / all_params
                }
            }
            
            metadata_path = os.path.join(output_dir, 'training_metadata.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            self._log(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
            
            self._log(f"\nüéâ ========== –û–ë–£–ß–ï–ù–ò–ï LLM –ú–û–î–ï–õ–ò –ó–ê–í–ï–†–®–ï–ù–û ==========")
            self._log(f"üìÅ LoRA –∞–¥–∞–ø—Ç–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_dir}")
            self._log(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_duration}")
            self._log(f"üìä –í—Å–µ–≥–æ —à–∞–≥–æ–≤: {getattr(train_result, 'global_step', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            
            return True
            
        except Exception as e:
            self._log(f"\nüí• ========== –û–®–ò–ë–ö–ê –û–ë–£–ß–ï–ù–ò–Ø LLM –ú–û–î–ï–õ–ò ==========")
            error_msg = f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}"
            self._log(error_msg)
            
            # –ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—à–∏–±–∫–∏
            self._log("üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:")
            self._log(f"   Python –≤–µ—Ä—Å–∏—è: {sys.version}")
            if hasattr(torch, '__version__'):
                self._log(f"   PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
            self._log(f"   CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
            if torch.cuda.is_available():
                self._log(f"   CUDA —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
                try:
                    self._log(f"   –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
                except (RuntimeError, AttributeError, Exception) as e:
                    # –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU - –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
                    pass
            
            self._log(f"   –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
            self._log(f"   –î–∞—Ç–∞—Å–µ—Ç —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(dataset_path) if 'dataset_path' in locals() else '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'}")
            self._log(f"   –í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {output_dir}")
            
            # –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏
            self._log("\nüîç –ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
            full_traceback = traceback.format_exc()
            for line in full_traceback.split('\n'):
                if line.strip():
                    self._log(f"   {line}")
            
            return False
        finally:
            self.training_active = False
    
    def _tokenize_dataset(self, dataset, training_config: Dict):
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç"""
        def tokenize_function(examples):
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω–æ–π
            max_length = training_config.get("max_sequence_length", 2048)
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=max_length,
                return_tensors=None
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º labels —Ä–∞–≤–Ω—ã–º–∏ input_ids –¥–ª—è causal LM
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset["train"].column_names,
            desc="–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
        )
        
        return tokenized_dataset
    
    def _setup_training_args(self, output_dir: str, training_config: Dict):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∞—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
        args_dict = training_config.get("training_args", {})
        
        return TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=args_dict.get("num_epochs", 3),
            per_device_train_batch_size=args_dict.get("batch_size", 4),
            per_device_eval_batch_size=args_dict.get("batch_size", 4),
            gradient_accumulation_steps=args_dict.get("gradient_accumulation_steps", 4),
            learning_rate=args_dict.get("learning_rate", 2e-4),
            warmup_steps=args_dict.get("warmup_steps", 100),
            logging_steps=args_dict.get("logging_steps", 10),
            eval_steps=args_dict.get("eval_steps", 500),
            save_steps=args_dict.get("save_steps", 500),
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º wandb/tensorboard
            remove_unused_columns=False,
            dataloader_drop_last=args_dict.get("dataloader_drop_last", True),
            fp16=args_dict.get("fp16", True) and torch.cuda.is_available(),
            gradient_checkpointing=args_dict.get("gradient_checkpointing", True),
            logging_dir=os.path.join(output_dir, "logs"),
        )
    
    def stop_training(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ"""
        self.should_stop = True
        if self.trainer:
            self.trainer.control.should_training_stop = True
        self._log("üõë –ó–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫—É –æ–±—É—á–µ–Ω–∏—è...")
    
    def get_training_metrics(self) -> TrainingMetrics:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        return self.current_metrics
    
    def get_training_history(self) -> List[Dict]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        return self.training_history
    
    def estimate_training_time(self, dataset_size: int, training_config: Dict) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è"""
        epochs = training_config.get("training_args", {}).get("num_epochs", 3)
        batch_size = training_config.get("training_args", {}).get("batch_size", 4)
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        if "70b" in self.plugin.model_name.lower():
            time_per_sample = 2.0  # —Å–µ–∫—É–Ω–¥—ã
        elif "13b" in self.plugin.model_name.lower():
            time_per_sample = 0.5
        else:
            time_per_sample = 0.2
        
        steps_per_epoch = dataset_size // batch_size
        total_steps = steps_per_epoch * epochs
        estimated_time = total_steps * time_per_sample
        
        return {
            "estimated_time_seconds": estimated_time,
            "estimated_time_hours": estimated_time / 3600,
            "total_steps": total_steps,
            "steps_per_epoch": steps_per_epoch
        }
    
    def _update_progress(self, current: int, total: int, message: str = ""):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å"""
        if self.progress_callback:
            progress = int((current / total) * 100) if total > 0 else 0
            self.progress_callback(progress, message)
    
    def _log(self, message: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ callback –µ—Å–ª–∏ –µ—Å—Ç—å
        if hasattr(self.progress_callback, '__call__'):
            try:
                self.progress_callback(-1, log_message)  # -1 –æ–∑–Ω–∞—á–∞–µ—Ç –ª–æ–≥ —Å–æ–æ–±—â–µ–Ω–∏–µ
            except (TypeError, AttributeError, Exception) as e:
                # –û—à–∏–±–∫–∞ –≤—ã–∑–æ–≤–∞ callback - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
                pass


class TrainingProgressCallback:
    """Callback –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, trainer_instance):
        self.trainer_instance = trainer_instance
        self.start_time = time.time()
    
    def on_train_begin(self, args, state, control, **kwargs):
        """–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è"""
        self.start_time = time.time()
        self.trainer_instance._log("üöÄ –û–±—É—á–µ–Ω–∏–µ –Ω–∞—á–∞—Ç–æ")
    
    def on_epoch_begin(self, args, state, control, **kwargs):
        """–ù–∞—á–∞–ª–æ —ç–ø–æ—Ö–∏"""
        epoch = state.epoch
        self.trainer_instance.current_metrics.epoch = int(epoch)
        self.trainer_instance._log(f"üìö –≠–ø–æ—Ö–∞ {int(epoch) + 1}/{args.num_train_epochs}")
    
    def on_step_end(self, args, state, control, **kwargs):
        """–ö–æ–Ω–µ—Ü —à–∞–≥–∞"""
        self.trainer_instance.current_metrics.step = state.global_step
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        progress = (state.global_step / state.max_steps) * 100
        message = f"–®–∞–≥ {state.global_step}/{state.max_steps} (–≠–ø–æ—Ö–∞ {int(state.epoch) + 1})"
        
        if self.trainer_instance.progress_callback:
            self.trainer_instance.progress_callback(int(progress), message)
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        if logs:
            metrics = TrainingMetrics(
                epoch=int(state.epoch),
                step=state.global_step,
                loss=logs.get("train_loss", 0.0),
                eval_loss=logs.get("eval_loss", 0.0),
                learning_rate=logs.get("learning_rate", 0.0),
                samples_per_second=logs.get("train_samples_per_second", 0.0)
            )
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            self.trainer_instance.current_metrics = metrics
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            metrics_dict = {
                "epoch": metrics.epoch,
                "step": metrics.step,
                "loss": metrics.loss,
                "eval_loss": metrics.eval_loss,
                "learning_rate": metrics.learning_rate,
                "timestamp": datetime.now().isoformat()
            }
            self.trainer_instance.training_history.append(metrics_dict)
            
            # –õ–æ–≥–∏—Ä—É–µ–º
            if "train_loss" in logs:
                self.trainer_instance._log(f"üìä –®–∞–≥ {metrics.step}: loss={metrics.loss:.4f}, lr={metrics.learning_rate:.2e}")
    
    def on_train_end(self, args, state, control, **kwargs):
        """–ö–æ–Ω–µ—Ü –æ–±—É—á–µ–Ω–∏—è"""
        total_time = time.time() - self.start_time
        self.trainer_instance._log(f"[OK] –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {total_time/3600:.1f} —á–∞—Å–æ–≤") 