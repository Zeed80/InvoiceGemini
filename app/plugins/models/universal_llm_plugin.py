"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM –ø–ª–∞–≥–∏–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ API.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OpenAI, Anthropic, Google, Mistral, DeepSeek, xAI, Ollama.
"""
import os
import json
import base64
import tempfile
import re
from datetime import datetime
from typing import Dict, Optional, Any
from pathlib import Path
from PIL import Image

from ..base_llm_plugin import BaseLLMPlugin, LLM_PROVIDERS
from .adaptive_prompt_manager import (
    create_adaptive_invoice_prompt,
    get_model_generation_params,
    AdaptivePromptManager
)

class UniversalLLMPlugin(BaseLLMPlugin):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ LLM.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä—É –∏ –º–æ–¥–µ–ª–∏.
    """
    
    def __init__(self, provider_name: str, model_name: str = None, api_key: str = None, **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ LLM –ø–ª–∞–≥–∏–Ω–∞.
        
        Args:
            provider_name: –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (openai, anthropic, google, etc.)
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)
            api_key: API –∫–ª—é—á –¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
            **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        super().__init__(provider_name, model_name, api_key, **kwargs)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        self.base_url = kwargs.get('base_url', None)
        self.temp_files = []
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Ollama
        if provider_name == "ollama":
            self.base_url = kwargs.get('base_url', 'http://localhost:11434')
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π LLM –ø–ª–∞–≥–∏–Ω –¥–ª—è {self.provider_config.display_name}")
    
    def load_model(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ.
        
        Returns:
            bool: True –µ—Å–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è —É—Å–ø–µ—à–Ω—ã
        """
        try:
            success = False
            if self.provider_name == "openai":
                success = self._load_openai_client()
            elif self.provider_name == "anthropic":
                success = self._load_anthropic_client()
            elif self.provider_name == "google":
                success = self._load_google_client()
            elif self.provider_name == "mistral":
                success = self._load_mistral_client()
            elif self.provider_name == "deepseek":
                success = self._load_deepseek_client()
            elif self.provider_name == "xai":
                success = self._load_xai_client()
            elif self.provider_name == "ollama":
                success = self._load_ollama_client()
            else:
                print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider_name}")
                return False
            
            # –ï—Å–ª–∏ –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
            if success:
                print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ {self.provider_name}...")
                test_success = self._test_connection()
                if test_success:
                    print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ {self.provider_name} –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
                    self.is_loaded = True
                    return True
                else:
                    print(f"‚ùå –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ {self.provider_name} –Ω–µ—É–¥–∞—á–Ω–∞")
                    self.is_loaded = False
                    return False
            else:
                return False
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {self.provider_name}: {e}")
            self.is_loaded = False
            return False
    
    def _load_openai_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞."""
        try:
            import openai
            if not self.api_key:
                print("‚ùå API –∫–ª—é—á OpenAI –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
                return False
            
            self.client = openai.OpenAI(api_key=self.api_key)
            print(f"üîß OpenAI –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model_name}")
            return True
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            return False
    
    def _load_anthropic_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Anthropic –∫–ª–∏–µ–Ω—Ç–∞."""
        try:
            import anthropic
            if not self.api_key:
                print("‚ùå API –∫–ª—é—á Anthropic –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
                return False
            
            self.client = anthropic.Anthropic(api_key=self.api_key)
            print(f"üîß Anthropic –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model_name}")
            return True
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ anthropic –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install anthropic")
            return False
    
    def _load_google_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Google Gemini –∫–ª–∏–µ–Ω—Ç–∞."""
        try:
            import google.generativeai as genai
            if not self.api_key:
                print("‚ùå API –∫–ª—é—á Google –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
                return False
            
            genai.configure(api_key=self.api_key)
            self.client = genai.GenerativeModel(self.model_name)
            print(f"üîß Google Gemini –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model_name}")
            return True
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ google-generativeai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install google-generativeai")
            return False
    
    def _load_mistral_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Mistral –∫–ª–∏–µ–Ω—Ç–∞."""
        try:
            from mistralai.client import MistralClient
            if not self.api_key:
                print("‚ùå API –∫–ª—é—á Mistral –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
                return False
            
            self.client = MistralClient(api_key=self.api_key)
            self.is_loaded = True
            print(f"‚úÖ Mistral –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é {self.model_name}")
            return True
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ mistralai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install mistralai")
            return False
    
    def _load_deepseek_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DeepSeek –∫–ª–∏–µ–Ω—Ç–∞ (—á–µ—Ä–µ–∑ OpenAI API)."""
        try:
            import openai
            if not self.api_key:
                print("‚ùå API –∫–ª—é—á DeepSeek –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
                return False
            
            self.client = openai.OpenAI(
                api_key=self.api_key,
                base_url="https://api.deepseek.com"
            )
            self.is_loaded = True
            print(f"‚úÖ DeepSeek –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é {self.model_name}")
            return True
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            return False
    
    def _load_xai_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è xAI –∫–ª–∏–µ–Ω—Ç–∞ (—á–µ—Ä–µ–∑ OpenAI API)."""
        try:
            import openai
            if not self.api_key:
                print("‚ùå API –∫–ª—é—á xAI –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω")
                return False
            
            self.client = openai.OpenAI(
                api_key=self.api_key,
                base_url="https://api.x.ai/v1"
            )
            self.is_loaded = True
            print(f"‚úÖ xAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é {self.model_name}")
            return True
        except ImportError:
            print("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ openai –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openai")
            return False
    
    def _load_ollama_client(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞."""
        try:
            import requests
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä–∞
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞
                models_data = response.json()
                available_models = [model['name'] for model in models_data.get('models', [])]
                
                if self.model_name in available_models:
                    self.client = True  # –î–ª—è Ollama –∏—Å–ø–æ–ª—å–∑—É–µ–º requests –Ω–∞–ø—Ä—è–º—É—é
                    print(f"üîß Ollama –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ {self.model_name}")
                    return True
                else:
                    print(f"‚ùå –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama")
                    print(f"üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {', '.join(available_models[:5])}{'...' if len(available_models) > 5 else ''}")
                    return False
            else:
                print(f"‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –Ω–∞ {self.base_url} (–∫–æ–¥: {response.status_code})")
                return False
        except requests.exceptions.ConnectionError:
            print(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É –Ω–∞ {self.base_url}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω: ollama serve")
            return False
        except requests.exceptions.Timeout:
            print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama –Ω–∞ {self.base_url}")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama: {e}")
            return False
    
    def _test_connection(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ API –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Returns:
            bool: True –µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
        """
        try:
            # –î–ª—è Ollama –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
            if self.provider_name == "ollama":
                return self._test_ollama_connection()
            
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            test_response = self.generate_response("Test", timeout=10)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –≤–∞–ª–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
            if test_response and len(test_response.strip()) > 0:
                return True
            else:
                print(f"‚ùå {self.provider_name}: –ü–æ–ª—É—á–µ–Ω –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç")
                return False
                
        except Exception as e:
            error_msg = str(e).lower()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏
            if "timeout" in error_msg or "timed out" in error_msg:
                print(f"‚ùå {self.provider_name}: –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è")
            elif "unauthorized" in error_msg or "invalid api key" in error_msg:
                print(f"‚ùå {self.provider_name}: –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á")
            elif "credit balance" in error_msg or "insufficient funds" in error_msg:
                print(f"‚ùå {self.provider_name}: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤ –Ω–∞ –±–∞–ª–∞–Ω—Å–µ")
            elif "rate limit" in error_msg:
                print(f"‚ùå {self.provider_name}: –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤")
            elif "model not found" in error_msg:
                print(f"‚ùå {self.provider_name}: –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            elif "connection" in error_msg:
                print(f"‚ùå {self.provider_name}: –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è")
            else:
                print(f"‚ùå {self.provider_name}: –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è - {e}")
            
            return False
    
    def _test_ollama_connection(self) -> bool:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è Ollama - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏."""
        try:
            import requests
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–µ—Ä–≤–µ—Ä –æ—Ç–≤–µ—á–∞–µ—Ç
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code != 200:
                print(f"‚ùå Ollama —Å–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–∫–æ–¥: {response.status_code})")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            models_data = response.json()
            available_models = [model['name'] for model in models_data.get('models', [])]
            
            if self.model_name not in available_models:
                print(f"‚ùå –ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã
            test_data = {
                "model": self.model_name,
                "prompt": "Hi",
                "stream": False,
                "options": {"num_predict": 10}
            }
            
            test_response = requests.post(
                f"{self.base_url}/api/generate",
                json=test_data,
                timeout=15
            )
            
            if test_response.status_code == 200:
                result = test_response.json()
                if result.get("response"):
                    return True
                else:
                    print(f"‚ùå Ollama –º–æ–¥–µ–ª—å {self.model_name} –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç")
                    return False
            else:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Ollama –º–æ–¥–µ–ª–∏: {test_response.status_code}")
                return False
                
        except requests.exceptions.ConnectionError:
            print(f"‚ùå –ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É")
            return False
        except requests.exceptions.Timeout:
            print(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ Ollama")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è Ollama: {e}")
            return False
    
    def generate_response(self, prompt: str, image_path: str = None, image_context: str = "", timeout: int = 30) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
            image_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–¥–ª—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –±–µ–∑ vision)
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            str: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        if not self.client:
            raise ValueError(f"–ö–ª–∏–µ–Ω—Ç {self.provider_name} –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        try:
            if self.provider_name == "openai":
                return self._generate_openai_response(prompt, image_path, image_context, timeout)
            elif self.provider_name == "anthropic":
                return self._generate_anthropic_response(prompt, image_path, image_context, timeout)
            elif self.provider_name == "google":
                return self._generate_google_response(prompt, image_path, image_context, timeout)
            elif self.provider_name == "mistral":
                return self._generate_mistral_response(prompt, image_path, image_context, timeout)
            elif self.provider_name == "deepseek":
                return self._generate_deepseek_response(prompt, image_path, image_context, timeout)
            elif self.provider_name == "xai":
                return self._generate_xai_response(prompt, image_path, image_context, timeout)
            elif self.provider_name == "ollama":
                return self._generate_ollama_response(prompt, image_path, image_context, timeout)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {self.provider_name}")
                
        except Exception as e:
            # –ü–æ–¥–Ω–∏–º–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ _test_connection
            raise e
    
    def _encode_image_base64(self, image_path: str) -> str:
        """–ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ base64."""
        try:
            with open(image_path, "rb") as image_file:
                return base64.b64encode(image_file.read()).decode('utf-8')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return ""
    
    def _generate_openai_response(self, prompt: str, image_path: str = None, image_context: str = "", timeout: int = 30) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI API."""
        messages = []
        
        if image_path and self.provider_config.supports_vision:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision
            base64_image = self._encode_image_base64(image_path)
            if base64_image:
                messages.append({
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                })
            else:
                messages.append({"role": "user", "content": f"{prompt}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"})
        else:
            # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
            content = prompt
            if image_context:
                content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"
            messages.append({"role": "user", "content": content})
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=self.generation_config.get("max_tokens", 4096),
            temperature=self.generation_config.get("temperature", 0.1),
            timeout=timeout
        )
        
        return response.choices[0].message.content
    
    def _generate_anthropic_response(self, prompt: str, image_path: str = None, image_context: str = "", timeout: int = 30) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Anthropic API."""
        content = []
        
        if image_path and self.provider_config.supports_vision:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision
            try:
                import base64
                with open(image_path, "rb") as image_file:
                    image_data = base64.b64encode(image_file.read()).decode()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image_type = "image/jpeg"
                if image_path.lower().endswith('.png'):
                    image_type = "image/png"
                
                content.append({
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": image_type,
                        "data": image_data
                    }
                })
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Claude: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
        text_content = prompt
        if image_context and not (image_path and self.provider_config.supports_vision):
            text_content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"
        
        content.append({"type": "text", "text": text_content})
        
        response = self.client.messages.create(
            model=self.model_name,
            max_tokens=self.generation_config.get("max_tokens", 4096),
            temperature=self.generation_config.get("temperature", 0.1),
            messages=[{"role": "user", "content": content}],
            timeout=timeout
        )
        
        return response.content[0].text
    
    def _generate_google_response(self, prompt: str, image_path: str = None, image_context: str = "", timeout: int = 30) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Google Gemini API."""
        content = []
        
        if image_path and self.provider_config.supports_vision:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision
            try:
                image = Image.open(image_path)
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                content.append(image)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è Gemini: {e}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
        text_content = prompt
        if image_context and not (image_path and self.provider_config.supports_vision):
            text_content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"
        
        content.append(text_content)
        
        generation_config = {
            "temperature": self.generation_config.get("temperature", 0.1),
            "max_output_tokens": self.generation_config.get("max_tokens", 4096),
        }
        
        response = self.client.generate_content(
            content,
            generation_config=generation_config
        )
        
        return response.text
    
    def _generate_mistral_response(self, prompt: str, image_path: str = None, image_context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Mistral API."""
        from mistralai.models.chat_completion import ChatMessage, ImageURLChunk, TextChunk
        
        content = []
        
        if image_path and self.provider_config.supports_vision and "pixtral" in self.model_name.lower():
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision –¥–ª—è Pixtral
            base64_image = self._encode_image_base64(image_path)
            if base64_image:
                content.extend([
                    TextChunk(text=prompt),
                    ImageURLChunk(image_url=f"data:image/jpeg;base64,{base64_image}")
                ])
            else:
                content.append(TextChunk(text=f"{prompt}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"))
        else:
            # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
            text_content = prompt
            if image_context:
                text_content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"
            content.append(TextChunk(text=text_content))
        
        messages = [ChatMessage(role="user", content=content)]
        
        response = self.client.chat(
            model=self.model_name,
            messages=messages,
            max_tokens=self.generation_config.get("max_tokens", 4096),
            temperature=self.generation_config.get("temperature", 0.1)
        )
        
        return response.choices[0].message.content
    
    def _generate_deepseek_response(self, prompt: str, image_path: str = None, image_context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ DeepSeek API (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)."""
        # DeepSeek –ø–æ–∫–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision
        content = prompt
        if image_context:
            content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"
        
        messages = [{"role": "user", "content": content}]
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=self.generation_config.get("max_tokens", 4096),
            temperature=self.generation_config.get("temperature", 0.1)
        )
        
        return response.choices[0].message.content
    
    def _generate_xai_response(self, prompt: str, image_path: str = None, image_context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ xAI API (OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)."""
        messages = []
        
        if image_path and self.provider_config.supports_vision and "vision" in self.model_name.lower():
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ vision –¥–ª—è Grok Vision
            base64_image = self._encode_image_base64(image_path)
            if base64_image:
                messages.append({
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                })
            else:
                messages.append({"role": "user", "content": f"{prompt}\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"})
        else:
            # –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º
            content = prompt
            if image_context:
                content += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_context}"
            messages.append({"role": "user", "content": content})
        
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=self.generation_config.get("max_tokens", 4096),
            temperature=self.generation_config.get("temperature", 0.1)
        )
        
        return response.choices[0].message.content
    
    def _generate_ollama_response(self, prompt: str, image_path: str = None, image_context: str = "", timeout: int = 30) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama API —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        import requests
        from .ollama_utils import is_vision_model
        
        # –ü–æ–ª—É—á–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏
        generation_params = get_model_generation_params(self.model_name)
        
        # –û–¢–õ–ê–î–ö–ê: –í—ã–≤–æ–¥–∏–º –ø—Ä–æ–º–ø—Ç
        print(f"\n[DEBUG] Ollama prompt (first 800 chars):\n{prompt[:800]}\n")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        data = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": generation_params
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∏—Å–ø–æ–ª—å–∑—É–µ–º is_vision_model –≤–º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ "vision" –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        if image_path and is_vision_model(self.model_name):
            try:
                base64_image = self._encode_image_base64(image_path)
                if base64_image:
                    data["images"] = [base64_image]
                    print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –∑–∞–ø—Ä–æ—Å –∫ Ollama (–º–æ–¥–µ–ª—å: {self.model_name})")
                else:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è {self.model_name}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ Ollama: {e}")
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–ª—è gemma3 –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º OCR —Ç–µ–∫—Å—Ç
        # –¥–∞–∂–µ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision, —Ç.–∫. –æ–Ω–∞ –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
        if image_context:
            # –î–æ–±–∞–≤–ª—è–µ–º OCR —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
            data["prompt"] += f"\n\nDocument OCR text:\n{image_context[:3000]}\n"
            print(f"‚ÑπÔ∏è OCR —Ç–µ–∫—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (–º–æ–¥–µ–ª—å: {self.model_name})")
        
        try:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è gemma3 —Å vision+OCR
            effective_timeout = 120 if "gemma" in self.model_name.lower() else max(timeout, 60)
            print(f"‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –¥–ª—è {self.model_name}: {effective_timeout} —Å–µ–∫—É–Ω–¥")
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data,
                timeout=effective_timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "")
                
                # –û–¢–õ–ê–î–ö–ê: –í—ã–≤–æ–¥–∏–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
                print(f"\n[DEBUG] Ollama raw response (first 1500 chars):\n{response_text[:1500]}\n")
                
                if response_text:
                    return response_text
                else:
                    raise ValueError("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama")
            else:
                error_msg = f"HTTP {response.status_code}"
                try:
                    error_data = response.json()
                    if "error" in error_data:
                        error_msg = error_data["error"]
                except:
                    pass
                raise ValueError(f"Ollama API –æ—à–∏–±–∫–∞: {error_msg}")
                
        except requests.exceptions.ConnectionError:
            raise ConnectionError("–ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama —Å–µ—Ä–≤–µ—Ä—É")
        except requests.exceptions.Timeout:
            raise TimeoutError(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama ({timeout}s)")
        except requests.exceptions.RequestException as e:
            raise ConnectionError(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: {e}")
    
    def process_image(self, image_path: str, ocr_lang=None, custom_prompt=None) -> Optional[Dict]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            ocr_lang: –Ø–∑—ã–∫ –¥–ª—è OCR (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω fallback)
            custom_prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            
        Returns:
            Optional[Dict]: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None
        """
        if not self.is_loaded:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
        
        if not os.path.exists(image_path):
            print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {image_path}")
            return None
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º OCR –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç vision
            image_context = ""
            model_has_vision = False
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É vision –¥–ª—è –º–æ–¥–µ–ª–∏
            if self.provider_name == "ollama":
                from .ollama_utils import is_vision_model
                model_has_vision = is_vision_model(self.model_name)
            else:
                model_has_vision = self.provider_config.supports_vision and not (
                    (self.provider_name == "deepseek") or
                    (self.provider_name == "mistral" and "pixtral" not in self.model_name.lower())
                )
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –î–ª—è gemma3 –í–°–ï–ì–î–ê –∏–∑–≤–ª–µ–∫–∞–µ–º OCR
            # —Ç.–∫. –æ–Ω–∞ –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å vision, –Ω–µ—Å–º–æ—Ç—Ä—è –Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫—É
            should_use_ocr = (
                not model_has_vision or 
                (self.provider_name == "ollama" and "gemma" in self.model_name.lower())
            )
            
            if should_use_ocr:
                print(f"üìù –ò–∑–≤–ª–µ–∫–∞–µ–º OCR —Ç–µ–∫—Å—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (–º–æ–¥–µ–ª—å: {self.model_name})")
                image_context = self.extract_text_from_image(image_path, ocr_lang or "rus+eng")
                if image_context:
                    print(f"‚úÖ OCR –∏–∑–≤–ª–µ—á–µ–Ω, –¥–ª–∏–Ω–∞: {len(image_context)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π
            prompt = custom_prompt or self.create_invoice_prompt(
                use_adaptive=True,
                ocr_text=image_context if image_context else None,
                image_available=model_has_vision
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.generate_response(prompt, image_path, image_context)
            
            # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = self.parse_llm_response(response)
            
            if result:
                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                result["_meta"] = {
                    "provider": self.provider_config.display_name,
                    "model": self.model_name,
                    "processed_at": str(datetime.now()),
                    "supports_vision": self.provider_config.supports_vision,
                    "used_ocr": bool(image_context)
                }
            
            return result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return None
    
    def cleanup_temp_files(self):
        """–û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤."""
        for temp_file in self.temp_files:
            try:
                if os.path.exists(temp_file):
                    os.unlink(temp_file)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ {temp_file}: {e}")
        self.temp_files.clear()
    
    def get_saved_prompt(self) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞.
        
        Returns:
            str: –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–ª–∏ –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        try:
            from ...settings_manager import settings_manager
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ (–æ–±–ª–∞—á–Ω–∞—è –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è)
            model_type = "cloud_llm" if self.provider_name in ["openai", "anthropic", "google", "mistral", "deepseek", "xai"] else "local_llm"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
            prompt_key = f"{model_type}_{self.provider_name}_prompt"
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            saved_prompt = settings_manager.get_setting(prompt_key, "")
            
            if saved_prompt:
                print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è {self.provider_name}")
                return saved_prompt
            else:
                print(f"‚ÑπÔ∏è –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é –±–∞–∑–æ–≤—ã–π –¥–ª—è {self.provider_name}")
                return self.create_invoice_prompt()
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞: {e}")
            return self.create_invoice_prompt()
    
    def get_full_prompt(self) -> str:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏.
        –°–æ–≤–º–µ—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å UI.
        
        Returns:
            str: –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –º–æ–¥–µ–ª–∏
        """
        return self.get_saved_prompt()
    
    def extract_invoice_data(self, image_path: str, prompt: str = None) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å—á–µ—Ç–∞-—Ñ–∞–∫—Ç—É—Ä—ã.
        –°–æ–≤–º–µ—Å—Ç–∏–º—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –æ—Å–Ω–æ–≤–Ω—ã–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é —Å—á–µ—Ç–∞
            prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            Dict[str, Any]: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å—á–µ—Ç–∞
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π
            if not prompt:
                prompt = self.get_saved_prompt()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ process_image
            result = self.process_image(image_path, custom_prompt=prompt)
            
            if result is None:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –∏–∑ {image_path}")
                return {}
            
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –∏–∑ {image_path}")
            return result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —Å—á–µ—Ç–∞: {e}")
            return {}
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        self.cleanup_temp_files() 