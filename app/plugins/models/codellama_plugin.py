"""
CodeLlamaPlugin –¥–ª—è InvoiceGemini
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–≥–∏–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—á–µ—Ç–æ–≤ —Å –∫–æ–¥–∞–º–∏ —Ç–æ–≤–∞—Ä–æ–≤
"""

import os
import json
from typing import Dict, Any, Optional, List
from ..base_llm_plugin import BaseLLMPlugin

class CodeLlamaPlugin(BaseLLMPlugin):
    """
    –ü–ª–∞–≥–∏–Ω –¥–ª—è Code Llama –º–æ–¥–µ–ª–µ–π
    –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    """
    
    def __init__(self, model_name: str = "codellama-7b-instruct", **kwargs):
        super().__init__(model_name, **kwargs)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Code Llama –º–æ–¥–µ–ª–µ–π
        self.supported_models = {
            "codellama-7b-instruct": {
                "path": "codellama/CodeLlama-7b-Instruct-hf",
                "memory_gb": 16,
                "context_length": 16384,
                "description": "Code Llama 7B Instruct - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
            },
            "codellama-13b-instruct": {
                "path": "codellama/CodeLlama-13b-Instruct-hf",
                "memory_gb": 28,
                "context_length": 16384,
                "description": "Code Llama 13B Instruct - –±–æ–ª–µ–µ –º–æ—â–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á"
            },
            "codellama-34b-instruct": {
                "path": "codellama/CodeLlama-34b-Instruct-hf",
                "memory_gb": 72,
                "context_length": 16384,
                "description": "Code Llama 34B Instruct - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"
            }
        }
        
        # –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å
        self.current_model_key = "codellama-7b-instruct"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
        self.generation_config = {
            "max_new_tokens": 2048,
            "temperature": 0.05,  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            "top_p": 0.9,
            "do_sample": True,
            "repetition_penalty": 1.05,
            "eos_token_id": None,
            "pad_token_id": None
        }
    
    def get_plugin_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–ª–∞–≥–∏–Ω–µ"""
        return {
            "name": "Code Llama Plugin",
            "version": "1.0.0",
            "description": "–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—á–µ—Ç–æ–≤ —Å –∫–æ–¥–∞–º–∏ —Ç–æ–≤–∞—Ä–æ–≤",
            "author": "InvoiceGemini Team",
            "supported_models": list(self.supported_models.keys()),
            "current_model": self.current_model_key,
            "is_loaded": self.is_loaded,
            "capabilities": [
                "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –∏ –∫–æ–¥–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤",
                "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π",
                "–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                "–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –Ω–æ–º–µ–Ω–∫–ª–∞—Ç—É—Ä",
                "–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"
            ]
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        model_config = self.supported_models[self.current_model_key]
        
        return {
            "name": f"Code Llama {self.current_model_key}",
            "model_family": "Code Llama",
            "model_path": model_config["path"],
            "device": str(self.device) if hasattr(self, 'device') else "Unknown",
            "is_loaded": self.is_loaded,
            "memory_requirements": f"~{model_config['memory_gb']} GB GPU/RAM",
            "context_length": model_config["context_length"],
            "torch_available": self._check_torch_available(),
            "description": model_config["description"],
            "specialization": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"
        }
    
    def get_training_config(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        model_config = self.supported_models[self.current_model_key]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Code Llama
        batch_size = 1 if "34b" in self.current_model_key else (2 if "13b" in self.current_model_key else 4)
        
        return {
            "supports_lora": True,
            "supports_qlora": True,
            "default_lora_rank": 32,  # –í—ã—à–µ –¥–ª—è –ª—É—á—à–µ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
            "max_sequence_length": model_config["context_length"],
            "training_args": {
                "batch_size": batch_size,
                "learning_rate": "1e-4",  # –ù–∏–∂–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                "num_epochs": 5,  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                "warmup_steps": 200,
                "logging_steps": 5,
                "save_steps": 250,
                "gradient_accumulation_steps": 16 // batch_size,
                "weight_decay": 0.01
            },
            "recommended_memory": model_config["memory_gb"],
            "chat_template": self._get_codellama_chat_template(),
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        }
    
    def load_model(self, model_key: Optional[str] = None) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—É—é Code Llama –º–æ–¥–µ–ª—å"""
        if not self._check_dependencies():
            return False
        
        if model_key and model_key in self.supported_models:
            self.current_model_key = model_key
        
        model_config = self.supported_models[self.current_model_key]
        model_path = model_config["path"]
        
        try:
            print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Code Llama –º–æ–¥–µ–ª–∏: {model_path}")
            
            import torch
            from transformers import (
                AutoTokenizer, 
                AutoModelForCausalLM, 
                BitsAndBytesConfig,
                CodeLlamaTokenizer
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
            quantization_config = None
            if torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("[WRENCH] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è Code Llama")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä (—Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –¥–ª—è Code Llama)
            print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ Code Llama —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            try:
                self.tokenizer = CodeLlamaTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    use_fast=False,  # Code Llama –ª—É—á—à–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –º–µ–¥–ª–µ–Ω–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
                    cache_dir=self.cache_dir
                )
            except (OSError, ValueError, ImportError, Exception) as e:
                # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É –ø—Ä–∏ –æ—à–∏–±–∫–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ
                print(f"‚ö†Ô∏è –ü–µ—Ä–µ—Ö–æ–¥ –∫ –±—ã—Å—Ç—Ä–æ–º—É —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É: {e}")
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    use_fast=True,
                    cache_dir=self.cache_dir
                )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                print("[WRENCH] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token = eos_token")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.generation_config["eos_token_id"] = self.tokenizer.eos_token_id
            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ Code Llama –º–æ–¥–µ–ª–∏...")
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "device_map": "auto" if torch.cuda.is_available() else None,
            }
            
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **model_kwargs,
                cache_dir=self.cache_dir
            )
            
            # –ï—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if not torch.cuda.is_available():
                self.model = self.model.to(self.device)
            
            self.is_loaded = True
            print(f"‚úÖ Code Llama –º–æ–¥–µ–ª—å {self.current_model_key} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return True
            
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫: {e}")
            print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch transformers accelerate bitsandbytes")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Code Llama –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def unload_model(self) -> bool:
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            
            if self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA
            if hasattr(self, 'device') and 'cuda' in str(self.device):
                import torch
                torch.cuda.empty_cache()
                print("üßπ CUDA –∫—ç—à –æ—á–∏—â–µ–Ω")
            
            self.is_loaded = False
            print("‚úÖ Code Llama –º–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ Code Llama –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def process_image(self, image_path: str, ocr_lang: str = "rus+eng", custom_prompt: Optional[str] = None) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OCR
            ocr_text = self.extract_text_from_image(image_path, ocr_lang)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—á–µ—Ç
            return self.process_invoice(ocr_text, image_path)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}"}

    def generate_response(self, prompt: str, image_context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é Code Llama –º–æ–¥–µ–ª–∏."""
        if not self.is_loaded:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            full_prompt = f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{image_context}" if image_context else prompt
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Code Llama
            formatted_prompt = f"[INST] {full_prompt} [/INST]"
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=4096).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **self.generation_config, pad_token_id=self.tokenizer.eos_token_id)
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            return response.strip()
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"
    
    def process_invoice(self, ocr_text: str, image_path: Optional[str] = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—á–µ—Ç —Å –ø–æ–º–æ—â—å—é Code Llama –º–æ–¥–µ–ª–∏"""
        if not self.is_loaded:
            print("‚ùå Code Llama –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}
        
        try:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            system_prompt = self._get_technical_system_prompt()
            user_prompt = self._create_technical_user_prompt(ocr_text)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Code Llama
            full_prompt = f"[INST] {system_prompt}\n\n{user_prompt} [/INST]"
            
            print(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å Code Llama ({self.current_model_key})")
            print(f"üìù –î–ª–∏–Ω–∞ OCR —Ç–µ–∫—Å—Ç–∞: {len(ocr_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                full_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=min(4096, self.supported_models[self.current_model_key]["context_length"])
            ).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞...")
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **self.generation_config,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            print(f"üì§ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–æ–π: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            parsed_result = self._parse_codellama_response(response)
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            parsed_result = self._enhance_technical_data(parsed_result)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            parsed_result['note_gemini'] = f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Å –ø–æ–º–æ—â—å—é Code Llama ({self.current_model_key})"
            parsed_result['raw_response_codellama'] = response
            parsed_result['model_used'] = self.current_model_key
            
            return parsed_result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å Code Llama: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "note_gemini": f"–û—à–∏–±–∫–∞ Code Llama ({self.current_model_key}): {str(e)}"
            }
    
    def _get_technical_system_prompt(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å—á–µ—Ç–æ–≤ –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π.
–¢–≤–æ—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è - —Ç–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—Ä—Ç–∏–∫—É–ª–æ–≤, –∫–æ–¥–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫.

–û–°–û–ë–´–ï –ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –∫–æ–¥—ã, –∞—Ä—Ç–∏–∫—É–ª—ã –∏ –Ω–æ–º–µ—Ä–∞ –¥–µ—Ç–∞–ª–µ–π
2. –°–æ—Ö—Ä–∞–Ω—è–π —Ç–æ—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤
3. –ò–∑–≤–ª–µ–∫–∞–π –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è (—à—Ç, –∫–≥, –º, –ª –∏ —Ç.–¥.)
4. –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤
5. –û–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Å–µ—Ä–∏–π–Ω—ã–µ –Ω–æ–º–µ—Ä–∞ –∏ –ø–∞—Ä—Ç–∏–∏
6. –í–æ–∑–≤—Ä–∞—â–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{
    "company": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏",
    "invoice_number": "–Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "date": "–¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY",
    "total_amount": "–æ–±—â–∞—è —Å—É–º–º–∞ —á–∏—Å–ª–æ–º",
    "currency": "–≤–∞–ª—é—Ç–∞",
    "items": [
        {
            "name": "—Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞",
            "article_code": "–∞—Ä—Ç–∏–∫—É–ª/–∫–æ–¥ —Ç–æ–≤–∞—Ä–∞",
            "quantity": "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "unit": "–µ–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è",
            "price": "—Ü–µ–Ω–∞ –∑–∞ –µ–¥–∏–Ω–∏—Ü—É",
            "amount": "—Å—É–º–º–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏",
            "specifications": "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏"
        }
    ],
    "amount_without_vat_gemini": "—Å—É–º–º–∞ –±–µ–∑ –ù–î–°",
    "vat_percent_gemini": "–ø—Ä–æ—Ü–µ–Ω—Ç –ù–î–°",
    "category_gemini": "–∫–∞—Ç–µ–≥–æ—Ä–∏—è (–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç/–†–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã/–ü—Ä–æ—á–µ–µ)",
    "description_gemini": "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤"
}"""
    
    def _create_technical_user_prompt(self, ocr_text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        return f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç/—Å—á–µ—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.
–û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–∏ –Ω–∞ –∞—Ä—Ç–∏–∫—É–ª—ã, –∫–æ–¥—ã —Ç–æ–≤–∞—Ä–æ–≤ –∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:

–¢–ï–ö–°–¢ –î–û–ö–£–ú–ï–ù–¢–ê:
{ocr_text}

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."""
    
    def _get_codellama_chat_template(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω –¥–ª—è Code Llama"""
        return """[INST] {{ system_prompt }}

{{ user_message }} [/INST]"""
    
    def _parse_codellama_response(self, response: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç Code Llama –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON"""
        try:
            import re
            import json
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç
            response = response.strip()
            
            # Code Llama –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥, –ø–æ—ç—Ç–æ–º—É –∏—â–µ–º JSON –±–æ–ª–µ–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
            json_patterns = [
                r'```json\s*(.*?)\s*```',  # JSON –≤ –∫–æ–¥ –±–ª–æ–∫–µ
                r'```\s*(.*?)\s*```',      # –õ—é–±–æ–π –∫–æ–¥ –±–ª–æ–∫
                r'\{.*\}',                 # –ü—Ä–æ—Å—Ç–æ–π JSON
            ]
            
            for pattern in json_patterns:
                json_match = re.search(pattern, response, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1) if len(json_match.groups()) > 0 else json_match.group(0)
                    try:
                        parsed = json.loads(json_str)
                        return parsed
                    except json.JSONDecodeError:
                        continue
            
            # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω
            print(f"‚ö†Ô∏è JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ Code Llama: {response[:200]}...")
            return {
                "error": "JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ",
                "raw_response": response
            }
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ Code Llama: {e}")
            return {
                "error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}",
                "raw_response": response
            }
    
    def _enhance_technical_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –£–ª—É—á—à–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—é —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–æ–≤–∞—Ä–æ–≤
            if 'items' in data and isinstance(data['items'], list):
                for item in data['items']:
                    if isinstance(item, dict) and 'name' in item:
                        item_name = item['name'].lower()
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
                        if any(keyword in item_name for keyword in ['—Ä–µ–∑–µ—Ü', '–ø–ª–∞—Å—Ç–∏–Ω–∞', '–¥–µ—Ä–∂–∞–≤–∫–∞']):
                            item['technical_category'] = '–†–µ–∂—É—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç'
                        elif any(keyword in item_name for keyword in ['—Å–≤–µ—Ä–ª', '—Ñ—Ä–µ–∑', '–º–µ—Ç—á–∏–∫']):
                            item['technical_category'] = '–ú–µ—Ç–∞–ª–ª–æ—Ä–µ–∂—É—â–∏–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç'
                        elif any(keyword in item_name for keyword in ['–¥–∏—Å–∫', '–∫—Ä—É–≥', '—â–µ—Ç–∫–∞']):
                            item['technical_category'] = '–ê–±—Ä–∞–∑–∏–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç'
                        else:
                            item['technical_category'] = '–ü—Ä–æ—á–µ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ'
            
            # –£–ª—É—á—à–∞–µ–º –æ–±—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—é
            if 'category_gemini' not in data or data['category_gemini'] == 'N/A':
                if 'items' in data and data['items']:
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –ø–µ—Ä–≤–æ–º—É —Ç–æ–≤–∞—Ä—É
                    first_item = data['items'][0]
                    if isinstance(first_item, dict) and 'technical_category' in first_item:
                        if '—Ä–µ–∂—É—â–∏–π' in first_item['technical_category'].lower():
                            data['category_gemini'] = '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ç–æ–∫–∞—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏'
                        elif '–º–µ—Ç–∞–ª–ª–æ—Ä–µ–∂—É—â–∏–π' in first_item['technical_category'].lower():
                            data['category_gemini'] = '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ñ—Ä–µ–∑–µ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏'
                        elif '–∞–±—Ä–∞–∑–∏–≤–Ω—ã–π' in first_item['technical_category'].lower():
                            data['category_gemini'] = '–†–∞—Å—Ö–æ–¥–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã'
                        else:
                            data['category_gemini'] = '–ü—Ä–æ—á–µ–µ'
            
            return data
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É–ª—É—á—à–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return data
    
    def switch_model(self, model_key: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å Code Llama"""
        if model_key not in self.supported_models:
            print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å: {model_key}")
            return False
        
        if self.is_loaded:
            print("üîÑ –í—ã–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏...")
            self.unload_model()
        
        print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª—å: {model_key}")
        return self.load_model(model_key)
    
    def get_supported_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return list(self.supported_models.keys())
    
    def _check_dependencies(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
        try:
            import torch
            import transformers
            return True
        except ImportError as e:
            print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
            print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch transformers accelerate")
            return False
    
    def _check_torch_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyTorch"""
        try:
            import torch
            return True
        except ImportError:
            return False 