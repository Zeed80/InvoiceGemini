"""
–ü–ª–∞–≥–∏–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Llama –º–æ–¥–µ–ª—è–º–∏ –≤ InvoiceGemini
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Llama 2, Llama 3 –∏ –∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–∏
"""
from typing import Dict, Any, Optional
import os

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. LlamaPlugin –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–º —Ä–µ–∂–∏–º–µ.")

from ..base_llm_plugin import BaseLLMPlugin

class LlamaPlugin(BaseLLMPlugin):
    """
    –ü–ª–∞–≥–∏–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Llama –º–æ–¥–µ–ª—è–º–∏.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Llama 2, Llama 3 –∏ –∏—Ö Chat –≤–∞—Ä–∏–∞—Ü–∏–∏.
    """
    
    def __init__(self, model_name: str = "llama-2-7b-chat", model_path: Optional[str] = None, **kwargs):
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è Llama
        default_path = model_path or self._get_default_model_path(model_name)
        super().__init__(model_name, default_path, **kwargs)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è Llama –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.chat_template = self._get_chat_template()
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è Llama
        self.generation_config.update({
            "max_new_tokens": 512,
            "temperature": 0.7,
            "do_sample": True,
            "top_p": 0.9,
            "repetition_penalty": 1.1,
            "pad_token_id": None  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        })
    
    def _get_default_model_path(self, model_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö Llama –º–æ–¥–µ–ª–µ–π."""
        model_paths = {
            "llama-2-7b": "meta-llama/Llama-2-7b-hf",
            "llama-2-7b-chat": "meta-llama/Llama-2-7b-chat-hf",
            "llama-2-13b": "meta-llama/Llama-2-13b-hf", 
            "llama-2-13b-chat": "meta-llama/Llama-2-13b-chat-hf",
            "llama-2-70b": "meta-llama/Llama-2-70b-hf",
            "llama-2-70b-chat": "meta-llama/Llama-2-70b-chat-hf",
            "llama-3-8b": "meta-llama/Meta-Llama-3-8B",
            "llama-3-8b-instruct": "meta-llama/Meta-Llama-3-8B-Instruct",
            "llama-3-70b": "meta-llama/Meta-Llama-3-70B",
            "llama-3-70b-instruct": "meta-llama/Meta-Llama-3-70B-Instruct"
        }
        
        return model_paths.get(model_name.lower(), "meta-llama/Llama-2-7b-chat-hf")
    
    def _get_chat_template(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω –¥–ª—è Chat –º–æ–¥–µ–ª–µ–π Llama."""
        if "llama-3" in self.model_name.lower():
            return """<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        else:
            # Llama 2 chat template
            return """<s>[INST] {prompt} [/INST]"""
    
    def load_model(self, model_path: Optional[str] = None) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å Llama.
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            
        Returns:
            bool: True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        if not TRANSFORMERS_AVAILABLE:
            print("‚ùå transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers torch")
            return False
        
        try:
            path = model_path or self.model_path
            print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Llama –º–æ–¥–µ–ª–∏: {path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                path,
                trust_remote_code=True,
                use_fast=True
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
            }
            
            # –ï—Å–ª–∏ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map
            if torch.cuda.is_available():
                model_kwargs["device_map"] = "auto"
                model_kwargs["low_cpu_mem_usage"] = True
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForCausalLM.from_pretrained(path, **model_kwargs)
            
            # –ï—Å–ª–∏ CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ CPU
            if not torch.cuda.is_available():
                self.model = self.model.to(self.device)
            
            self.is_loaded = True
            print(f"‚úÖ Llama –º–æ–¥–µ–ª—å {self.model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –Ω–∞ {self.device}")
            
            # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            if hasattr(self.model, 'config'):
                config = self.model.config
                print(f"   üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ~{getattr(config, 'num_parameters', 'Unknown')}")
                print(f"   üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {getattr(config, 'architectures', ['Unknown'])[0] if hasattr(config, 'architectures') else 'Unknown'}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Llama –º–æ–¥–µ–ª–∏ {self.model_name}: {e}")
            self.is_loaded = False
            return False
    
    def generate_response(self, prompt: str, image_context: str = "") -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é Llama –º–æ–¥–µ–ª–∏.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            image_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            str: –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏
        """
        if not self.is_loaded:
            return "‚ùå Llama –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            full_context = f"{prompt}\n\n–¢–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n{image_context}"
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template –µ—Å–ª–∏ —ç—Ç–æ chat –º–æ–¥–µ–ª—å
            if "chat" in self.model_name.lower() or "instruct" in self.model_name.lower():
                formatted_prompt = self.chat_template.format(prompt=full_context)
            else:
                formatted_prompt = full_context
            
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            inputs = self.tokenizer.encode(
                formatted_prompt, 
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º inputs –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if torch.cuda.is_available():
                inputs = inputs.to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=self.generation_config["max_new_tokens"],
                    temperature=self.generation_config["temperature"],
                    do_sample=self.generation_config["do_sample"],
                    top_p=self.generation_config["top_p"],
                    repetition_penalty=self.generation_config["repetition_penalty"],
                    pad_token_id=self.generation_config["pad_token_id"],
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
            response = full_response[len(formatted_prompt):].strip()
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            response = self._clean_response(response)
            
            return response
            
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Llama: {e}"
            print(error_msg)
            return error_msg
    
    def _clean_response(self, response: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."""
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('[INST]') and not line.startswith('<|'):
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines).strip()
    
    def process_image(self, image_path, ocr_lang=None, custom_prompt=None):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Llama.
        
        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
            ocr_lang: –Ø–∑—ã–∫ OCR
            custom_prompt: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            
        Returns:
            dict: –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é Llama: {image_path}")
        
        if not self.is_loaded:
            print("üîÑ –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–∞–≥—Ä—É–∂–∞–µ–º...")
            if not self.load_model():
                return {
                    "company": "",
                    "invoice_number": "",
                    "date": "",
                    "total_amount": 0,
                    "currency": "RUB", 
                    "items": [],
                    "note_gemini": f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {self.model_name}"
                }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é OCR
        image_context = self.extract_text_from_image(image_path, ocr_lang or "rus+eng")
        
        if not image_context or image_context.startswith("OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω") or image_context.startswith("–ù–µ —É–¥–∞–ª–æ—Å—å"):
            return {
                "company": "",
                "invoice_number": "",
                "date": "",
                "total_amount": 0,
                "currency": "RUB",
                "items": [],
                "note_gemini": f"‚ùå –û—à–∏–±–∫–∞ OCR: {image_context}"
            }
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
        prompt = self.create_invoice_prompt(custom_prompt)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        response = self.generate_response(prompt, image_context)
        
        # –ü–∞—Ä—Å–∏–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = self.parse_llm_response(response)
        result["note_gemini"] = f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {self.model_name} ({self.model_family})"
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: –Ω–∞–π–¥–µ–Ω–æ {len(result.get('items', []))} –ø–æ–∑–∏—Ü–∏–π")
        return result
    
    def get_training_config(self) -> Dict[str, Any]:
        """
        –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Llama –º–æ–¥–µ–ª–∏.
        
        Returns:
            dict: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        base_config = {
            "model_type": "llama",
            "supports_lora": True,
            "supports_qlora": True,
            "default_lora_rank": 16,
            "default_lora_alpha": 32,
            "max_sequence_length": 2048,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            "training_args": {
                "learning_rate": 2e-4,
                "batch_size": 4,
                "gradient_accumulation_steps": 4,
                "num_epochs": 3,
                "warmup_steps": 100,
                "save_steps": 500,
                "eval_steps": 500,
                "logging_steps": 10,
                "fp16": True,
                "dataloader_drop_last": True,
                "gradient_checkpointing": True
            }
        }
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        if "70b" in self.model_name.lower():
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π —É–º–µ–Ω—å—à–∞–µ–º batch size –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–µ gradient accumulation
            base_config["training_args"].update({
                "batch_size": 1,
                "gradient_accumulation_steps": 8,
                "learning_rate": 1e-4
            })
        elif "13b" in self.model_name.lower():
            # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π
            base_config["training_args"].update({
                "batch_size": 2,
                "gradient_accumulation_steps": 6
            })
        
        return base_config
    
    def get_model_info(self) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Llama –º–æ–¥–µ–ª–∏."""
        base_info = super().get_model_info()
        base_info.update({
            "chat_template": "chat" in self.model_name.lower() or "instruct" in self.model_name.lower(),
            "recommended_use": "–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä—É—Å—Å–∫–∏–º –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–º —è–∑—ã–∫–∞–º–∏",
            "memory_requirements": self._estimate_memory_requirements(),
            "transformers_available": TRANSFORMERS_AVAILABLE
        })
        return base_info
    
    def _estimate_memory_requirements(self) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø–∞–º—è—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏."""
        if "70b" in self.model_name.lower():
            return "~40GB VRAM (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è A100 –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ GPU)"
        elif "13b" in self.model_name.lower():
            return "~8-12GB VRAM (RTX 3080/4080 –∏–ª–∏ –ª—É—á—à–µ)"
        elif "7b" in self.model_name.lower() or "8b" in self.model_name.lower():
            return "~4-6GB VRAM (RTX 3060 Ti –∏–ª–∏ –ª—É—á—à–µ)"
        else:
            return "–ó–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏" 