"""
–£—Ç–∏–ª–∏—Ç–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –¥–ª—è Ollama.
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è Ollama —Å–µ—Ä–≤–µ—Ä–∞ –∏ –º–æ–¥–µ–ª–µ–π.
"""
import requests
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class OllamaModelInfo:
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏ Ollama"""
    name: str
    size: int
    digest: str
    modified_at: str
    details: Dict
    
    def get_size_mb(self) -> float:
        """–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ MB"""
        return self.size / (1024 * 1024)
    
    def get_size_gb(self) -> float:
        """–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ GB"""
        return self.size / (1024 * 1024 * 1024)


@dataclass
class OllamaDiagnosticResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ Ollama"""
    server_available: bool
    server_version: Optional[str]
    models_available: List[OllamaModelInfo]
    vision_models: List[str]
    recommended_models: List[str]
    base_url: str
    error_message: Optional[str]
    timestamp: str


class OllamaDiagnostic:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ Ollama"""
    
    # –ú–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π vision (–æ–±–Ω–æ–≤–ª–µ–Ω–æ 03.10.2025)
    VISION_MODELS = [
        # Llama Vision –º–æ–¥–µ–ª–∏
        "llama3.2-vision:11b",
        "llama3.2-vision:90b",
        "llama3.2-vision",
        
        # LLaVA –º–æ–¥–µ–ª–∏
        "llava:7b", 
        "llava:13b",
        "llava:34b",
        "bakllava:7b",
        
        # Gemma 3 - –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å visual –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
        "gemma3:4b",
        "gemma3:12b",
        "gemma3:27b",
        "gemma3",
        
        # Qwen Vision-Language –º–æ–¥–µ–ª–∏
        "qwen2.5vl:3b",
        "qwen2.5vl:7b",
        "qwen2.5vl:14b",
        "qwen2.5vl",
        "qwen2-vl",
        
        # –î—Ä—É–≥–∏–µ visual –º–æ–¥–µ–ª–∏
        "cogvlm",
        "minicpm-v",
        "moondream"
    ]
    
    # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–µ—Ç–æ–≤ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ 03.10.2025)
    RECOMMENDED_INVOICE_MODELS = [
        # Vision –º–æ–¥–µ–ª–∏ (–ª—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è —Å—á–µ—Ç–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏)
        "llama3.2-vision:11b",  # –õ—É—á—à–∏–π –≤—ã–±–æ—Ä - –ø–æ–ª–Ω–∞—è vision –ø–æ–¥–¥–µ—Ä–∂–∫–∞
        "qwen2.5vl:7b",          # –ë—ã—Å—Ç—Ä–∞—è vision –º–æ–¥–µ–ª—å
        "gemma3:12b",            # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –æ—Ç Google
        "gemma3:4b",             # –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è
        
        # Text-only –º–æ–¥–µ–ª–∏ (–¥–ª—è OCR-–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—á–µ—Ç–æ–≤)
        "llama3.1:8b",           # –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å
        "qwen2.5:7b",            # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
        "mistral:7b"             # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞
    ]
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.
        
        Args:
            base_url: URL Ollama —Å–µ—Ä–≤–µ—Ä–∞
        """
        self.base_url = base_url.rstrip('/')
        
    def run_full_diagnostic(self, timeout: int = 10) -> OllamaDiagnosticResult:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—É—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É Ollama.
        
        Args:
            timeout: –¢–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            OllamaDiagnosticResult: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–µ—Ä–∞
            server_available, version, error = self._check_server_availability(timeout)
            
            if not server_available:
                return OllamaDiagnosticResult(
                    server_available=False,
                    server_version=None,
                    models_available=[],
                    vision_models=[],
                    recommended_models=[],
                    base_url=self.base_url,
                    error_message=error,
                    timestamp=timestamp
                )
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            models = self._get_available_models(timeout)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª–∏ —Å vision
            vision_models = [m.name for m in models if any(vm in m.name for vm in self.VISION_MODELS)]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏
            recommended = [m.name for m in models if m.name in self.RECOMMENDED_INVOICE_MODELS]
            
            return OllamaDiagnosticResult(
                server_available=True,
                server_version=version,
                models_available=models,
                vision_models=vision_models,
                recommended_models=recommended,
                base_url=self.base_url,
                error_message=None,
                timestamp=timestamp
            )
            
        except Exception as e:
            return OllamaDiagnosticResult(
                server_available=False,
                server_version=None,
                models_available=[],
                vision_models=[],
                recommended_models=[],
                base_url=self.base_url,
                error_message=f"–û—à–∏–±–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏: {str(e)}",
                timestamp=timestamp
            )
    
    def _check_server_availability(self, timeout: int) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–µ—Ä–≤–µ—Ä–∞.
        
        Returns:
            Tuple[bool, Optional[str], Optional[str]]: 
                (–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–µ—Ä—Å–∏—è, —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ)
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π endpoint
            response = requests.get(f"{self.base_url}/api/version", timeout=timeout)
            
            if response.status_code == 200:
                version_data = response.json()
                version = version_data.get("version", "unknown")
                return True, version, None
            else:
                return False, None, f"–°–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª –∫–æ–¥ {response.status_code}"
                
        except requests.exceptions.ConnectionError:
            return False, None, f"–ù–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {self.base_url}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω (ollama serve)"
        except requests.exceptions.Timeout:
            return False, None, f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({timeout}—Å)"
        except Exception as e:
            return False, None, f"–û—à–∏–±–∫–∞: {str(e)}"
    
    def _get_available_models(self, timeout: int) -> List[OllamaModelInfo]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.
        
        Returns:
            List[OllamaModelInfo]: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=timeout)
            
            if response.status_code != 200:
                return []
            
            data = response.json()
            models = []
            
            for model_data in data.get("models", []):
                model_info = OllamaModelInfo(
                    name=model_data.get("name", ""),
                    size=model_data.get("size", 0),
                    digest=model_data.get("digest", ""),
                    modified_at=model_data.get("modified_at", ""),
                    details=model_data.get("details", {})
                )
                models.append(model_info)
            
            return models
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def test_model_response(self, model_name: str, timeout: int = 15) -> Tuple[bool, Optional[str]]:
        """
        –¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã.
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞
            
        Returns:
            Tuple[bool, Optional[str]]: (—É—Å–ø–µ—à–Ω–æ, —Å–æ–æ–±—â–µ–Ω–∏–µ)
        """
        try:
            test_data = {
                "model": model_name,
                "prompt": "Hello! Respond with: OK",
                "stream": False,
                "options": {"num_predict": 5}
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=test_data,
                timeout=timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "").strip()
                
                if response_text:
                    return True, f"‚úÖ –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—á–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ: '{response_text}'"
                else:
                    return False, "‚ùå –ú–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç"
            else:
                return False, f"‚ùå –û—à–∏–±–∫–∞: HTTP {response.status_code}"
                
        except requests.exceptions.Timeout:
            return False, f"‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è ({timeout}—Å)"
        except Exception as e:
            return False, f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"
    
    def get_model_recommendations(self, models: List[OllamaModelInfo]) -> Dict[str, str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –¥–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        
        Args:
            models: –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            
        Returns:
            Dict[str, str]: –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª—è–º
        """
        recommendations = {}
        model_names = [m.name for m in models]
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–µ—Ç–æ–≤ —Å vision
        vision_available = [m for m in model_names if any(vm in m for vm in self.VISION_MODELS)]
        if vision_available:
            recommendations['best_vision'] = vision_available[0]
        else:
            recommendations['install_vision'] = "llama3.2-vision:11b"
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        fast_models = [m for m in model_names if any(x in m for x in ["7b", "3b"])]
        if fast_models:
            recommendations['fastest'] = fast_models[0]
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_models = [m for m in model_names if any(x in m for x in ["70b", "34b", "13b"])]
        if quality_models:
            recommendations['best_quality'] = quality_models[0]
        
        return recommendations
    
    def format_diagnostic_report(self, result: OllamaDiagnosticResult) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π –æ—Ç—á–µ—Ç.
        
        Args:
            result: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
            
        Returns:
            str: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
        """
        lines = []
        lines.append("="*60)
        lines.append("üìã –û–¢–ß–ï–¢ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò OLLAMA")
        lines.append("="*60)
        lines.append(f"‚è∞ –í—Ä–µ–º—è: {result.timestamp}")
        lines.append(f"üåê URL: {result.base_url}")
        lines.append("")
        
        if not result.server_available:
            lines.append("‚ùå –°–ï–†–í–ï–† –ù–ï–î–û–°–¢–£–ü–ï–ù")
            lines.append(f"   –û—à–∏–±–∫–∞: {result.error_message}")
            lines.append("")
            lines.append("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            lines.append("   1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: https://ollama.com/download")
            lines.append("   2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä: ollama serve")
            lines.append("   3. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama3.2-vision:11b")
        else:
            lines.append("‚úÖ –°–ï–†–í–ï–† –î–û–°–¢–£–ü–ï–ù")
            if result.server_version:
                lines.append(f"   –í–µ—Ä—Å–∏—è: {result.server_version}")
            lines.append("")
            
            lines.append(f"üì¶ –£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ú–û–î–ï–õ–ò: {len(result.models_available)}")
            if result.models_available:
                for model in result.models_available:
                    size_str = f"{model.get_size_gb():.2f} GB" if model.get_size_gb() >= 1 else f"{model.get_size_mb():.0f} MB"
                    vision_mark = "üëÅÔ∏è" if model.name in result.vision_models else "üìù"
                    rec_mark = "‚≠ê" if model.name in result.recommended_models else ""
                    lines.append(f"   {vision_mark} {model.name} ({size_str}) {rec_mark}")
            else:
                lines.append("   ‚ö†Ô∏è –ú–æ–¥–µ–ª–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
            
            lines.append("")
            
            if result.vision_models:
                lines.append(f"üëÅÔ∏è –ú–û–î–ï–õ–ò –° VISION: {len(result.vision_models)}")
                for vm in result.vision_models:
                    lines.append(f"   ‚úÖ {vm}")
            else:
                lines.append("‚ö†Ô∏è –ú–û–î–ï–õ–ò –° VISION –ù–ï –ù–ê–ô–î–ï–ù–´")
                lines.append("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º: ollama pull llama3.2-vision:11b")
            
            lines.append("")
            
            if result.recommended_models:
                lines.append(f"‚≠ê –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –î–õ–Ø –°–ß–ï–¢–û–í: {len(result.recommended_models)}")
                for rm in result.recommended_models:
                    lines.append(f"   ‚úÖ {rm}")
            else:
                lines.append("üí° –†–ï–ö–û–ú–ï–ù–î–£–ï–ú –£–°–¢–ê–ù–û–í–ò–¢–¨:")
                for rm in self.RECOMMENDED_INVOICE_MODELS[:3]:
                    lines.append(f"   ollama pull {rm}")
        
        lines.append("="*60)
        return "\n".join(lines)


def quick_diagnostic(base_url: str = "http://localhost:11434") -> str:
    """
    –ë—ã—Å—Ç—Ä–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ Ollama —Å –≤—ã–≤–æ–¥–æ–º –æ—Ç—á–µ—Ç–∞.
    
    Args:
        base_url: URL Ollama —Å–µ—Ä–≤–µ—Ä–∞
        
    Returns:
        str: –û—Ç—á–µ—Ç –æ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–µ
    """
    diagnostic = OllamaDiagnostic(base_url)
    result = diagnostic.run_full_diagnostic()
    return diagnostic.format_diagnostic_report(result)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    print(quick_diagnostic())

