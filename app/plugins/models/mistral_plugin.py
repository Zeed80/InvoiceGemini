"""
MistralPlugin –¥–ª—è InvoiceGemini
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Mistral 7B –∏ 8x7B –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–µ—Ç–æ–≤
"""

import os
import json
from typing import Dict, Any, Optional, List
from ..base_llm_plugin import BaseLLMPlugin

class MistralPlugin(BaseLLMPlugin):
    """
    –ü–ª–∞–≥–∏–Ω –¥–ª—è Mistral –º–æ–¥–µ–ª–µ–π
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Mistral 7B Instruct v0.3 –∏ Mixtral 8x7B
    """
    
    def __init__(self, model_name: str = "mistral-7b-instruct", **kwargs):
        super().__init__(model_name, **kwargs)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Mistral –º–æ–¥–µ–ª–µ–π
        self.supported_models = {
            "mistral-7b-instruct": {
                "path": "mistralai/Mistral-7B-Instruct-v0.3",
                "memory_gb": 16,
                "context_length": 32768,
                "description": "Mistral 7B Instruct v0.3 - —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"
            },
            "mistral-8x7b-instruct": {
                "path": "mistralai/Mixtral-8x7B-Instruct-v0.1", 
                "memory_gb": 96,
                "context_length": 32768,
                "description": "Mixtral 8x7B - –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å —Å–º–µ—Å–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"
            },
            "mistral-7b-base": {
                "path": "mistralai/Mistral-7B-v0.3",
                "memory_gb": 14,
                "context_length": 32768,
                "description": "Mistral 7B Base - –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è fine-tuning"
            }
        }
        
        # –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å
        self.current_model_key = "mistral-7b-instruct"
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.generation_config = {
            "max_new_tokens": 2048,
            "temperature": 0.1,
            "top_p": 0.95,
            "do_sample": True,
            "repetition_penalty": 1.1,
            "eos_token_id": None,
            "pad_token_id": None
        }
    
    def get_plugin_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–ª–∞–≥–∏–Ω–µ"""
        return {
            "name": "Mistral Plugin",
            "version": "1.0.0",
            "description": "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ Mistral –∏ Mixtral –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—á–µ—Ç–æ–≤",
            "author": "InvoiceGemini Team",
            "supported_models": list(self.supported_models.keys()),
            "current_model": self.current_model_key,
            "is_loaded": self.is_loaded,
            "capabilities": [
                "–ú–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                "–í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö", 
                "–ü–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (32K —Ç–æ–∫–µ–Ω–æ–≤)",
                "–ë—ã—Å—Ç—Ä—ã–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –±–ª–∞–≥–æ–¥–∞—Ä—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                "–ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—á–µ—Ç–æ–≤"
            ]
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏"""
        model_config = self.supported_models[self.current_model_key]
        
        return {
            "name": f"Mistral {self.current_model_key}",
            "model_family": "Mistral",
            "model_path": model_config["path"],
            "device": str(self.device) if hasattr(self, 'device') else "Unknown",
            "is_loaded": self.is_loaded,
            "memory_requirements": f"~{model_config['memory_gb']} GB GPU/RAM",
            "context_length": model_config["context_length"],
            "torch_available": self._check_torch_available(),
            "description": model_config["description"]
        }
    
    def get_training_config(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        model_config = self.supported_models[self.current_model_key]
        
        return {
            "supports_lora": True,
            "supports_qlora": True,
            "default_lora_rank": 16,
            "max_sequence_length": model_config["context_length"],
            "training_args": {
                "batch_size": 2 if "8x7b" in self.current_model_key else 4,
                "learning_rate": "2e-4",
                "num_epochs": 3,
                "warmup_steps": 100,
                "logging_steps": 10,
                "save_steps": 500,
                "gradient_accumulation_steps": 8 if "8x7b" in self.current_model_key else 4
            },
            "recommended_memory": model_config["memory_gb"],
            "chat_template": self._get_mistral_chat_template()
        }
    
    def load_model(self, model_key: Optional[str] = None) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—É—é Mistral –º–æ–¥–µ–ª—å"""
        if not self._check_dependencies():
            return False
        
        if model_key and model_key in self.supported_models:
            self.current_model_key = model_key
        
        model_config = self.supported_models[self.current_model_key]
        model_path = model_config["path"]
        
        try:
            print(f"üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ Mistral –º–æ–¥–µ–ª–∏: {model_path}")
            
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            quantization_config = None
            if torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                print("[WRENCH] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            print("üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                use_fast=True
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                print("[WRENCH] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token = eos_token")
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.generation_config["eos_token_id"] = self.tokenizer.eos_token_id
            self.generation_config["pad_token_id"] = self.tokenizer.pad_token_id
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
            model_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if torch.cuda.is_available() else torch.float32,
                "device_map": "auto" if torch.cuda.is_available() else None,
            }
            
            if quantization_config:
                model_kwargs["quantization_config"] = quantization_config
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                **model_kwargs
            )
            
            # –ï—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if not torch.cuda.is_available():
                self.model = self.model.to(self.device)
            
            self.is_loaded = True
            print(f"‚úÖ Mistral –º–æ–¥–µ–ª—å {self.current_model_key} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
            return True
            
        except ImportError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫: {e}")
            print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch transformers accelerate bitsandbytes")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Mistral –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def unload_model(self) -> bool:
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏"""
        try:
            if self.model is not None:
                del self.model
                self.model = None
            
            if self.tokenizer is not None:
                del self.tokenizer
                self.tokenizer = None
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA
            if hasattr(self, 'device') and 'cuda' in str(self.device):
                import torch
                torch.cuda.empty_cache()
                print("üßπ CUDA –∫—ç—à –æ—á–∏—â–µ–Ω")
            
            self.is_loaded = False
            print("‚úÖ Mistral –º–æ–¥–µ–ª—å –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ Mistral –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def process_image(self, image_path: str, ocr_lang: str = "rus+eng", custom_prompt: Optional[str] = None) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é OCR
            ocr_text = self.extract_text_from_image(image_path, ocr_lang)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—á–µ—Ç
            return self.process_invoice(ocr_text, image_path)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return {"error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}"}

    def generate_response(self, prompt: str, image_context: str = "") -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é Mistral –º–æ–¥–µ–ª–∏."""
        if not self.is_loaded:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"
        
        try:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            full_prompt = f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞:\n{image_context}" if image_context else prompt
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ Mistral chat —Ñ–æ—Ä–º–∞—Ç
            chat_messages = [{"role": "user", "content": full_prompt}]
            formatted_prompt = self.tokenizer.apply_chat_template(
                chat_messages, tokenize=False, add_generation_prompt=True
            )
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=4096).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            with torch.no_grad():
                outputs = self.model.generate(**inputs, **self.generation_config, pad_token_id=self.tokenizer.eos_token_id)
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            return response.strip()
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}"

    def process_invoice(self, ocr_text: str, image_path: Optional[str] = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—á–µ—Ç —Å –ø–æ–º–æ—â—å—é Mistral –º–æ–¥–µ–ª–∏"""
        if not self.is_loaded:
            print("‚ùå Mistral –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return {"error": "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}
        
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Mistral
            system_prompt = self._get_system_prompt()
            user_prompt = self._create_user_prompt(ocr_text)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ Mistral chat —Ñ–æ—Ä–º–∞—Ç
            chat_messages = [
                {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
            ]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º chat template
            formatted_prompt = self.tokenizer.apply_chat_template(
                chat_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            print(f"üéØ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å Mistral ({self.current_model_key})")
            print(f"üìù –î–ª–∏–Ω–∞ OCR —Ç–µ–∫—Å—Ç–∞: {len(ocr_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=min(4096, self.supported_models[self.current_model_key]["context_length"])
            ).to(self.device)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    **self.generation_config,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            print(f"üì§ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–æ–π: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            parsed_result = self._parse_mistral_response(response)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            parsed_result['note_gemini'] = f"–ò–∑–≤–ª–µ—á–µ–Ω–æ —Å –ø–æ–º–æ—â—å—é Mistral ({self.current_model_key})"
            parsed_result['raw_response_mistral'] = response
            parsed_result['model_used'] = self.current_model_key
            
            return parsed_result
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å Mistral: {e}")
            import traceback
            traceback.print_exc()
            return {
                "error": str(e),
                "note_gemini": f"–û—à–∏–±–∫–∞ Mistral ({self.current_model_key}): {str(e)}"
            }
    
    def _get_system_prompt(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Mistral"""
        return """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–µ–ª–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å—á–µ—Ç–æ–≤. 
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—á–µ—Ç–∞.

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–µ—Å—å —Ç–µ–∫—Å—Ç –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ
2. –ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è
3. –í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON
4. –ï—Å–ª–∏ –ø–æ–ª–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–π "N/A"
5. –°—É–º–º—ã —É–∫–∞–∑—ã–≤–∞–π —á–∏—Å–ª–∞–º–∏ –±–µ–∑ —Å–∏–º–≤–æ–ª–æ–≤ –≤–∞–ª—é—Ç
6. –î–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{
    "company": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏",
    "invoice_number": "–Ω–æ–º–µ—Ä —Å—á–µ—Ç–∞", 
    "date": "–¥–∞—Ç–∞ —Å—á–µ—Ç–∞",
    "total_amount": "–æ–±—â–∞—è —Å—É–º–º–∞",
    "currency": "–≤–∞–ª—é—Ç–∞",
    "items": [
        {
            "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞/—É—Å–ª—É–≥–∏",
            "quantity": "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ",
            "price": "—Ü–µ–Ω–∞ –∑–∞ –µ–¥–∏–Ω–∏—Ü—É",
            "amount": "—Å—É–º–º–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏"
        }
    ],
    "amount_without_vat_gemini": "—Å—É–º–º–∞ –±–µ–∑ –ù–î–°",
    "vat_percent_gemini": "–ø—Ä–æ—Ü–µ–Ω—Ç –ù–î–°",
    "category_gemini": "–∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ä–∞—Å—Ö–æ–¥–æ–≤",
    "description_gemini": "–æ–ø–∏—Å–∞–Ω–∏–µ"
}"""
    
    def _create_user_prompt(self, ocr_text: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç"""
        return f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç —Å—á–µ—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ –¥–∞–Ω–Ω—ã–µ:

–¢–ï–ö–°–¢ –°–ß–ï–¢–ê:
{ocr_text}

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏."""
    
    def _get_mistral_chat_template(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —à–∞–±–ª–æ–Ω —á–∞—Ç–∞ –¥–ª—è Mistral"""
        return """<s>[INST] {{ system_prompt }}

{{ user_message }} [/INST]"""
    
    def _parse_mistral_response(self, response: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç Mistral –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç JSON"""
        try:
            import re
            import json
            
            # –û—á–∏—â–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            response = response.strip()
            
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ (–º–µ–∂–¥—É { –∏ })
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                parsed = json.loads(json_str)
                return parsed
            else:
                # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É
                print(f"‚ö†Ô∏è JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ Mistral: {response[:200]}...")
                return {
                    "error": "JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ",
                    "raw_response": response
                }
                
        except json.JSONDecodeError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –æ—Ç Mistral: {e}")
            return {
                "error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}",
                "raw_response": response
            }
        except Exception as e:
            print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ Mistral: {e}")
            return {
                "error": f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}",
                "raw_response": response
            }
    
    def switch_model(self, model_key: str) -> bool:
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç –Ω–∞ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å Mistral"""
        if model_key not in self.supported_models:
            print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å: {model_key}")
            return False
        
        if self.is_loaded:
            print("üîÑ –í—ã–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏...")
            self.unload_model()
        
        print(f"üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –º–æ–¥–µ–ª—å: {model_key}")
        return self.load_model(model_key)
    
    def get_supported_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return list(self.supported_models.keys())
    
    def _check_dependencies(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
        try:
            import torch
            import transformers
            return True
        except ImportError as e:
            print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {e}")
            print("üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install torch transformers accelerate")
            return False
    
    def _check_torch_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å PyTorch"""
        try:
            import torch
            return True
        except ImportError:
            return False 